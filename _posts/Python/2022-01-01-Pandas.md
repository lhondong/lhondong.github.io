---
title: "Pandas"
subtitle: ""
layout: post
author: "L Hondong"
header-img: "img/post-bg-12.jpg"
tags:
  - Pandas
  - Python
---

# Pandas

Numpy 在向量化的数值计算中表现优异，但是在处理更灵活、复杂的数据任务，如为数据添加标签、处理缺失值、分组和透视表等方面，Numpy显得力不从心。

**而基于Numpy构建的Pandas库，提供了使得数据分析变得更快更简单的高级数据结构和操作工具**。

## 特点

- 基于Numpy
- 提供使得数据分析变得更快更简单的高级数据结构和操作工具

## 对象创建

### 一维 Series

Series 是带标签数据的一维数组

通用结构: `pd.Series(data, index=index, dtype=dtype)`

- data：数据，可以是列表，字典或 Numpy 数组
- index：索引，为可选参数
- dtype: 数据类型，为可选参数（默认整数为 int64，浮点数为 float64）

#### 列表

- index 缺省，默认为整数序列

```python
import pandas as pd

data = pd.Series([1.5, 3, 4.5, 6])

Out:
0    1.5
1    3.0
2    4.5
3    6.0
dtype: float64

data = pd.Series([1.5, 3, 4.5, 6], index=["a", "b", "c", "d"])

Out:
a    1.5
b    3.0
c    4.5
d    6.0
dtype: float64
```

**注意：数据支持多种类型**

```python
data = pd.Series([1, 2, "3", 4], index=["a", "b", "c", "d"])

Out:
a    1
b    2
c    3
d    4
dtype: object
```

- **数据类型可被强制改变**

```python
data = pd.Series([1, 2, "3", 4], index=["a", "b", "c", "d"], dtype=float)

Out:
a    1.0
b    2.0
c    3.0
d    4.0
dtype: float64

data["c"]
>>> 3.0
```

如果其中的字符串不能被转变，则会报错 `ValueError: could not convert string to float: 'a'`

- data 为标量

```python
pd.Series(5, index=[100, 200, 300])

Out：
100    5
200    5
300    5
dtype: int64
```

#### 字典

- 默认以键为 index，值为 data

```python
population_dict = {"BeiJing": 2154,
                   "ShangHai": 2424,
                   "ShenZhen": 1303,
                   "HangZhou": 981 }
population = pd.Series(population_dict)

Out:
BeiJing     2154
ShangHai    2424
ShenZhen    1303
HangZhou     981
dtype: int64
```

- 字典创建，如果指定 index，则会到字典的键中筛选，找不到的，值设为 NaN

```python
population = pd.Series(population_dict, index=["BeiJing", "HangZhou", "c", "d"])

Out:
BeiJing     2154.0
HangZhou     981.0
c              NaN
d              NaN
dtype: float64
```

#### Numpy 数组 ndarray

```python
import numpy as np

x = np.arange(5)
pd.Series(x)

Out:
0    0
1    1
2    2
3    3
4    4
dtype: int32
```

### 多维 DataFrame

DataFrame 是带标签数据的多维数组

通用结构: `pd.DataFrame(data, index=index, columns=columns)`

- data：数据，可以是列表，字典或 Numpy 数组
- index：索引，为可选参数
- columns: 列标签，为可选参数

#### 1. 通过 Series 对象创建

```python
population_dict = {"BeiJing": 2154,
                   "ShangHai": 2424,
                   "ShenZhen": 1303,
                   "HangZhou": 981 }

population = pd.Series(population_dict)
pd.DataFrame(population, columns=["population"])

Out:
	population
BeiJing		2154
ShangHai	2424
ShenZhen	1303
HangZhou	981
```

#### 2. 通过 Series 对象字典创建

```python
GDP_dict = {"BeiJing": 30320,
            "ShangHai": 32680,
            "ShenZhen": 24222,
            "HangZhou": 13468 }

GDP = pd.Series(GDP_dict)

pd.DataFrame({"population": population,
              "GDP": GDP})

Out:
	population	GDP
BeiJing		2154	30320
ShangHai	2424	32680
ShenZhen	1303	24222
HangZhou	981	    13468
```

- **注意：数量不够的会自动补齐**

```python
pd.DataFrame({"population": population,
              "GDP": GDP,
              "country": "China"})

Out:
	population	GDP	country
BeiJing		2154	30320	China
ShangHai	2424	32680	China
ShenZhen	1303	24222	China
HangZhou	981	    13468	China
```

#### 字典列表

- 字典索引作为 index，字典键作为 columns

```python
import numpy as np
import pandas as pd

data = [{"a": i, "b": 2*i} for i in range(3)]
>>> [{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}]

data = pd.DataFrame(data)
Out:
	a	b
0	0	0
1	1	2
2	2	4

data1 = data["a"].copy()

Out:
0    0
1    1
2    2
Name: a, dtype: int64

data1[0] = 10

0    10
1     1
2     2
Name: a, dtype: int64

# data1 改变，但是 data 不会随之改变
```

- 不存在的键，会默认值为NaN

```python
data = [{"a": 1, "b":1},{"b": 3, "c":4}]

Out:
	a	b	c
0	1.0	1	NaN
1	NaN	3	4.0
```

#### Numpy 二维数组 narray

```python
data = np.random.randint(10, size=(3, 2))

array([[2, 9],
       [2, 9],
       [0, 8]])

pd.DataFrame(data, columns=["foo", "bar"], index=["a", "b", "c"])

Out:
	foo	bar
a	2	9
b	2	9
c	0	8
```

**注意：不同形状的 Numpy 生成的 DataFrame 形状区分**

```python
x = pd.DataFrame(np.arange(4))

x:
	0
0	0
1	1
2	2
3	3

x = pd.DataFrame(np.arange(4).reshape(1, 4)) # x 是一行 4 列的数组，生成的 DataFrame 也是一行 4 列

x:
	0	1	2	3
0	0	1	2	3
```

## DataFrame 性质

### 属性

```python
data = pd.DataFrame({"pop": population, "GDP": GDP})

            pop	    GDP
BeiJing	    2154	30320
ShangHai	2424	32680
ShenZhen	1303	24222
HangZhou	981	    13468
```

- `df.values`  返回 Numpy 数组表示的数据 `array([[ 2154, 30320], [ 2424, 32680], [ 1303, 24222], [  981, 13468]])`
- `df.index` 返回行索引，Pandas Index 数据类型，`Index(['BeiJing', 'ShangHai', 'ShenZhen', 'HangZhou'], dtype='object')`
- `df.columns` 返回列索引 `Index(['pop', 'GDP'], dtype='object')`
- `df.shape`  形状 `(4, 2)`
- `df.size` 大小 8
- `dp.dtypes` 返回每列数据类型 `pop    int64 GDP    int64 dtype: object`

### 索引

#### 获取列

- 字典式 `df["name"]`

```python
data["pop"]

Out:
BeiJing     2154
ShangHai    2424
ShenZhen    1303
HangZhou     981
Name: pop, dtype: int64

data[["GDP", "pop"]] 

Out:
	        GDP	    pop
BeiJing	    30320	2154
ShangHai	32680	2424
ShenZhen	24222	1303
HangZhou	13468	981
```

- 对象属性式 `d.name`

`data.GDP`

#### 获取行

- 绝对索引 `df.loc[]`

```python
data.loc["BeiJing"]

Out:
pop     2154
GDP    30320
Name: BeiJing, dtype: int64

data.loc[["BeiJing", "HangZhou"]]
Out:
	        pop	    GDP
BeiJing	    2154	30320
HangZhou	981	    13468
```

- 相对索引 `df.iloc[]`

```python
data.iloc[0]

Out:
pop     2154
GDP    30320
Name: BeiJing, dtype: int64

data.iloc[[1, 3]]

Out:
		pop	GDP
ShangHai	2424	32680
HangZhou	981	13468
```

#### 获取标量

```python
data.loc["BeiJing", "GDP"]
data.iloc[0, 1]
data.values[0][1]

>>> 30320
```

#### Series 对象的索引

```python
type(data.GDP)
>>> pandas.core.series.Series

GDP
Out:
BeiJing     30320
ShangHai    32680
ShenZhen    24222
HangZhou    13468
dtype: int64

GDP["BeiJing"]
>>> 30320
```

### 切片

```python
dates = pd.date_range(start='2019-01-01', periods=6)
>>> DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04',
               '2019-01-05', '2019-01-06'],
              dtype='datetime64[ns]', freq='D')

df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=["A", "B", "C", "D"])

Out:
	        A	        B	        C	        D
2019-01-01	-0.935378	-0.190742	0.925984	-0.818969
2019-01-02	-0.234414	-1.194674	1.080779	-2.294395
2019-01-03	-0.141572	0.058118	1.102248	1.207726
2019-01-04	0.305088	0.535920	-0.978434	0.177251
2019-01-05	0.313383	0.234041	0.163155	-0.296649
2019-01-06	0.250613	-0.904400	-0.858240	-1.573342
```

#### 行切片

- `df["a": "b"]`

```python
# 取前三行
df["2019-01-01": "2019-01-03"]
df.loc["2019-01-01": "2019-01-03"]
df.iloc[0: 3]
```

#### 列切片 loc, iloc

```python
# 取前三列
df.loc[:, "A": "C"]
df.iloc[:, 0: 3]
```

#### 行、列同时切片

```python
df.loc["2019-01-02": "2019-01-03", "C":"D"]
df.iloc[1: 3, 2:]

Out:
	        C	        D
2019-01-02	1.080779	-2.294395
2019-01-03	1.102248	1.207726
```

#### 行切片，列分散取值

```python
df.loc["2019-01-04": "2019-01-06", ["A", "C"]]
df.iloc[3:, [0, 2]]

Out:
	        A	        C
2019-01-04	0.305088	-0.978434
2019-01-05	0.313383	0.163155
2019-01-06	0.250613	-0.858240
```

- 行分散取值，列切片
- 行、列均分散取值

### 布尔索引

- 比较运算
- 掩码

```python
df[df > 0]

Out:

            A	        B	        C	        D
2019-01-01	NaN	        NaN	        0.925984	NaN
2019-01-02	NaN	        NaN	        1.080779	NaN
2019-01-03	NaN	        0.058118	1.102248	1.207726
2019-01-04	0.305088	0.535920	NaN	        0.177251
2019-01-05	0.313383	0.234041	0.163155	NaN
2019-01-06	0.250613	NaN	        NaN	        NaN

df.A > 0
Out:
2019-01-01    False
2019-01-02    False
2019-01-03    False
2019-01-04     True
2019-01-05     True
2019-01-06     True
Freq: D, Name: A, dtype: bool

df[df.A > 0] # 取出后三行

Out:
            A	        B	        C	        D
2019-01-04	0.305088	0.535920	-0.978434	0.177251
2019-01-05	0.313383	0.234041	0.163155	-0.296649
2019-01-06	0.250613	-0.904400	-0.858240	-1.573342
```

- `isin()` 方法

```python
df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']
ind = df2["E"].isin(["two", "four"])

2019-01-01    False
2019-01-02    False
2019-01-03     True
2019-01-04    False
2019-01-05     True
2019-01-06    False
Freq: D, Name: E, dtype: bool

df2[ind] # 取出第三行和第五行
Out:
	        A	        B	        C	        D	        E
2019-01-03	-0.141572	0.058118	1.102248	1.207726	two
2019-01-05	0.313383	0.234041	0.163155	-0.296649	four
```

### 赋值

- DataFrame 增加新列

```python
s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range('20190101', periods=6))
df["E"] = s1 # df 新增索引 'E'
```

- 索引赋值
- 切片赋值

```python
df.loc["2019-01-01", "A"] = 0
df.iloc[0, 1] = 0

df["D"] = np.array([5]*len(df))   # 可简化成df["D"] = 5

Out:
            A	        B	        C	        D	E
2019-01-01	0.000000	0.000000	0.925984	5	1
2019-01-02	-0.234414	-1.194674	1.080779	5	2
2019-01-03	-0.141572	0.058118	1.102248	5	3
2019-01-04	0.305088	0.535920	-0.978434	5	4
2019-01-05	0.313383	0.234041	0.163155	5	5
2019-01-06	0.250613	-0.904400	-0.858240	5	6
```

- 修改 index 和 columns

```python
df.index = [i for i in range(len(df))]
df.columns = [i for i in range(df.shape[1])]

Out:
    0	        1	        2	        3	4
0	0.000000	0.000000	0.925984	5	1
1	-0.234414	-1.194674	1.080779	5	2
2	-0.141572	0.058118	1.102248	5	3
3	0.305088	0.535920	-0.978434	5	4
4	0.313383	0.234041	0.163155	5	5
5	0.250613	-0.904400	-0.858240	5	6
```

## 数值运算及统计分析

```python
import pandas as pd
import numpy as np

dates = pd.date_range(start='2019-01-01', periods=6)
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=["A", "B", "C", "D"])

Out:
            A	        B	        C	        D
2019-01-01	-0.796846	-0.329379	-0.536845	0.816010
2019-01-02	0.223387	0.130896	1.202811	0.019009
2019-01-03	-0.792780	0.516628	-1.191318	-2.093418
2019-01-04	0.625127	0.219638	-0.951595	0.900715
2019-01-05	0.309149	-1.399961	-0.939279	0.250684
2019-01-06	-0.224059	1.446078	-0.152433	-2.343385
```

### 数据查看

- 查看前面的行 head
- 查看后面的行 tail

```python
df.head()    # 默认5行
df.head(2)   # 前 2 行
df.tail()    # 默认5行
df.tail(3)   # 后 3 行
```

- 查看总体信息 `df.info()`

### Numpy 通用函数同样适用于 Pandas

#### 向量化运算

```python
x = pd.DataFrame(np.arange(4).reshape(1, 4))

	0	1	2	3
0	0	1	2	3

x+5
np.exp(x)
y = pd.DataFrame(np.arange(4,8).reshape(1, 4))
x * y
```

#### 矩阵化运算

```python
np.random.seed(42)
x = pd.DataFrame(np.random.randint(10, size=(30, 30)))

z = x.T # 转置

y = pd.DataFrame(np.random.randint(10, size=(30, 30)))
x.dot(y)
np.dot(x, y)
```

- 执行相同运算，Numpy 比 Pandas 快
- 一般来说，纯粹的计算在 Numpy 里执行的更快，Numpy 更侧重于计算，Pandas 更侧重于数据处理

#### 广播运算

- 按行广播

```python
x = pd.DataFrame(np.random.randint(10, size=(3, 3)), columns=list("ABC"))
x.iloc[0]
x/x.iloc[0]

Out:
    A	        B	C
0	1.000000	1.0	1.000000
1	0.666667	2.0	1.285714
2	0.333333	2.0	1.000000
```

- 按列广播

```python
x.A
x.div(x.A, axis=0)

Out:
    A	B	C
0	1.0	0.5	1.166667
1	1.0	1.5	2.250000
2	1.0	3.0	3.500000

x.div(x.iloc[0], axis=1) # 按行

Out:
    A	        B	C
0	1.000000	1.0	1.000000
1	0.666667	2.0	1.285714
2	0.333333	2.0	1.000000
```

## Pandas 特有的用法

### 索引对齐

- Pandas 会自动对齐两个对象的索引，没有的值用 np.nan 表示

```python
data1 = pd.DataFrame(np.random.randint(0, 20, size=(2, 2)), columns=list("AB"))
	A	B
0	3	7
1	2	1
data2 = pd.DataFrame(np.random.randint(0, 10, size=(3, 3)), columns=list("ABC"))
    A	B	C
0	7	5	1
1	4	0	9
2	5	8	0

data1 + data2

Out:
    A	    B	    C
0	10.0	12.0	NaN
1	6.0	    1.0	    NaN
2	NaN	    NaN	    NaN
```

- 缺省值也可用fill_value来填充

```python
data1.add(data2, fill_value=0) # data1 用 0 补气

    A	    B	    C
0	10.0	12.0	1.0
1	6.0	    1.0	    9.0
2	5.0	    8.0	    0.0
```

### 统计相关

- 数据种类统计 `value_counts()`

```python
y = np.random.randint(3, size=20)
np.unique(y)
>>> array([0, 1, 2])

from collections import Counter
Counter(y)
>>> Counter({2: 11, 1: 5, 0: 4})

# 转换成 DataFrame
y1 = pd.DataFrame(y, columns=["A"])
np.unique(y1)
>>> array([0, 1, 2])
y1["A"].value_counts()

Out:
2    11
1     5
0     4
Name: A, dtype: int64
```

- 产生新的结果，并进行排序 `sort_values()`

```python
population_dict = {"BeiJing": 2154,
                   "ShangHai": 2424,
                   "ShenZhen": 1303,
                   "HangZhou": 981 }
population = pd.Series(population_dict) 

GDP_dict = {"BeiJing": 30320,
            "ShangHai": 32680,
            "ShenZhen": 24222,
            "HangZhou": 13468 }
GDP = pd.Series(GDP_dict)

city_info = pd.DataFrame({"population": population,"GDP": GDP})

city_info["per_GDP"] = city_info["GDP"]/city_info["population"]

Out:
	        population	GDP	per_GDP
BeiJing	    2154	30320	14.076137
ShangHai	2424	32680	13.481848
ShenZhen	1303	24222	18.589409
HangZhou	981	    13468	13.728848

# 递增排序
city_info.sort_values(by="per_GDP")

# 递减排序
city_info.sort_values(by="per_GDP", ascending=False)
```

- 按轴进行排序

```python
data = pd.DataFrame(np.random.randint(20, size=(3, 4)), index=[2, 1, 0], columns=list("CBAD"))

Out:
    C	B	A	D
2	3	13	17	8
1	1	19	14	6
0	11	7	14	2

# 行排序
data.sort_index()

Out:
    C	B	A	D
0	11	7	14	2
1	1	19	14	6
2	3	13	17	8

# 列排序
data.sort_index(axis=1)
data.sort_index(axis=1, ascending=False) # 降序
```

#### 统计方法

- `df.count()` 非空个数
- `df.sum()` 默认对列求和
- `df.sum(axis=1)` 对行求和
- `df.min()` 按列求最小值
- `df.max(axis=1)` 按行找最大值
- `df.idxmax()` 最大值索引
- `df.mean()` 均值
- `df.var()` 方差
- `df.std()` 标准差
- `df.median()` 中位数
- `data.mode()` 众数
- `df.quantile(0.75)` 75% 分位数
- `df.corr()` 相关性系数和协方差
- `df.corrwith(df["A"])`  计算与某一列的相关性系数
- `df.describe()` 一网打尽

```python
df.describe()

Out:
        A	        B	        C	        D
count	6.000000	6.000000	6.000000	6.000000
mean	2.378704	1.588916	0.655231	4.283124
std	    5.914449	4.371574	4.352947	2.593603
min	    -3.386712	-4.333177	-4.032613	-0.152567
25%	    -1.256706	-1.265251	-2.925910	3.158284
50%	    1.126767	1.531743	0.176172	5.273518
75%	    3.539202	5.052594	4.539740	6.157738
max	    13.113252	6.774559	5.577329	6.398588

data = pd.DataFrame([["a", "a", "c", "d"],
                     ["c", "a", "c", "b"],
                     ["a", "a", "d", "c"]], columns=list("ABCD"))

	A	B	C	D
0	a	a	c	d
1	c	a	c	b
2	a	a	d	c

# 字符类型 DataFrame 

data.describe()

	    A	B	C	D
count	3	3	3	3
unique	2	1	2	3
top	    a	a	c	d
freq	2	3	2	1
```

- count 非空元素的个数
- unique 非重复元素的个数
- top 出现频率最高的字符
- freq 最高出现的频率

#### 自定义输出

- `apply(method)` 的用法：使用 method 方法默认对每一列进行相应的操作
- `df.apply(np.cumsum)` 对列进行累加
- `df.apply(np.cumsum, axis=1)` 对行进行累加
- `df.apply(sum)` 计算各列之和
- `df.apply(lambda x: x.max()-x.min())` 计算每一列的最大值与最小值之差

```python
def my_describe(x):
    return pd.Series([x.count(), x.mean(), x.max(), x.idxmin(), x.std()], index=["Count", "mean", "max", "idxmin", "std"])
df.apply(my_describe)

        A	        B	        C	        D
Count	6.000000	6.000000	6.000000	6.000000
mean	2.378704	1.588916	0.655231	4.283124
max	    13.113252	6.774559	5.577329	6.398588
idxmin	3.000000	2.000000	5.000000	2.000000
std	    5.914449	4.371574	4.352947	2.593603
```

## 处理缺失值

```python
import pandas as pd
import numpy as np

data = pd.DataFrame(np.array([[1, np.nan, 2],
                              [np.nan, 3, 4],
                              [5, 6, None]]), columns=["A", "B", "C"])

# np.nan 是 Numpy 里的特殊浮点类型，None 导致数字类型变成 object

Out:
    A	B	C
0	1	NaN	2
1	NaN	3	4
2	5	6	None

data.dtypes

A    object
B    object
C    object
dtype: object
```

**注意：有 None、字符串等，数据类型全部变为 object，它比 int 和 float 更消耗资源**

### 发现缺失值

```python
data.isnull()

    A	    B	    C
0	False	True	False
1	True	False	False
2	False	False	True

data.notnull()

    A	    B	    C
0	True	False	True
1	False	True	True
2	True	True	False
```

### 删除缺失值

**注意：np.nan是一种特殊的浮点数**

- `data.dropna()` 删除所有含有 NaN 的行
- `data.dropna(axis="columns")` 删除整列
- `data.dropna(how="all")` 整行全部为 NaN 时才删除
- `data.dropna(axis="columns", how="all")` 整列全部为 NaN 时才删除
- `data.dropna(axis="columns", how="any")` 只要含有一个 NaN 就删除该列，与 `data.dropna(axis="columns")` 效果相同

### 填充缺失值

- `data.fillna(value=5)` 用 5 填充缺失值
- `data.fillna(value=data.mean())` 用每列均值进行填充
- `data.fillna(value=data.stack().mean())` 用所有数据的均值进行填充

## 合并数据

```python
import pandas as pd
import numpy as np

def make_df(cols, ind):
    "一个简单的DataFrame"
    data = {c: [str(c)+str(i) for i in ind]  for c in cols}
    return pd.DataFrame(data, ind)

make_df("ABC", range(3))

Out:
    A	B	C
0	A0	B0	C0
1	A1	B1	C1
2	A2	B2	C2
```

- 垂直合并

```python
df_1 = make_df("AB", [1, 2])
df_2 = make_df("AB", [3, 4]) # 相同的 columns

pd.concat([df_1, df_2])

Out:
    A	B
1	A1	B1
2	A2	B2
3	A3	B3
4	A4	B4
```

- 水平合并

```python
df_3 = make_df("AB", [0, 1])
df_4 = make_df("CD", [0, 1])

pd.concat([df_3, df_4], axis=1) # 相同的 index

Out:
    A	B	C	D
0	A0	B0	C0	D0
1	A1	B1	C1	D1
```

- 索引重叠

```python
df_5 = make_df("AB", [1, 2])
df_6 = make_df("AB", [1, 2]) # 行重叠，columns 相同，index 也相同

pd.concat([df_5, df_6])

Out:
    A	B
1	A1	B1
2	A2	B2
1	A1	B1
2	A2	B2
# 后两列的 index 和前两列的相同，可能会造成歧义

pd.concat([df_5, df_6], ignore_index=True) # 按 0,1,2...重新排列

Out:
    A	B
0	A1	B1
1	A2	B2
2	A1	B1
3	A2	B2

df_7 = make_df("ABC", [1, 2])
df_8 = make_df("BCD", [1, 2]) # 列重叠

pd.concat([df_7, df_8], axis=1)

Out:
    A	B	C	B	C	D
1	A1	B1	C1	B1	C1	D1
2	A2	B2	C2	B2	C2	D2

pd.concat([df_7, df_8],axis=1, ignore_index=True)

Out:
# 注意：会使原来的 index 或 column 信息丢失！
    0	1	2	3	4	5
1	A1	B1	C1	B1	C1	D1
2	A2	B2	C2	B2	C2	D2
```

- 对齐合并 merge()

```python
df_1 = make_df("AB", [1, 2])
df_2 = make_df("BC", [1, 2])

    A   B
1  A1  B1
2  A2  B2
    B   C
1  B1  C1
2  B2  C2

pd.merge(df_1, df_2)

Out:
    A	B	C
0	A1	B1	C1
1	A2	B2	C2
```

- 合并城市信息

```python
population_dict = {"city": ("BeiJing", "HangZhou", "ShenZhen"),
                   "pop": (2154, 981, 1303)}
population = pd.DataFrame(population_dict)

GDP_dict = {"city": ("BeiJing", "ShangHai", "HangZhou"),
            "GDP": (30320, 32680, 13468)}
GDP = pd.DataFrame(GDP_dict)

city_info = pd.merge(population, GDP)

Out: # 取二者的交集
    city	    pop	    GDP
0	BeiJing	    2154	30320
1	HangZhou	981	    13468

city_info = pd.merge(population, GDP, how="outer")

Out: # 取二者的并集
    city	    pop	    GDP
0	BeiJing	    2154.0	30320.0
1	HangZhou	981.0	13468.0
2	ShenZhen	1303.0	NaN
3	ShangHai	NaN	    32680.0
```

## 分组

```python
df = pd.DataFrame({"key":["A", "B", "C", "C", "B", "A"],
                  "data1": range(6),
                  "data2": np.random.randint(0, 10, size=6)})

    key	data1	data2
0	A	0	1
1	B	1	4
2	C	2	9
3	C	3	9
4	B	4	1
5	A	5	9

df.groupby("key") # 按 key 相同的进行分组，注意会延迟计算
>>> <pandas.core.groupby.generic.DataFrameGroupBy object at 0x000002276795A240>

for i in df.groupby("key"):
    print(str(i))

Out:
('A',   key  data1  data2
0   A      0      2
5   A      5      8)
('B',   key  data1  data2
1   B      1      2
4   B      4      4)
('C',   key  data1  data2
2   C      2      8
3   C      3      3)
```

- 求和

```python
df.groupby("key").sum()

    data1	data2
key
A	5	10
B	5	6
C	5	11
```

- 按列取值

```python
df.groupby("key")["data2"].sum()

Out:
key
A    10
B     6
C    11
Name: data2, dtype: int32
```

- 按组迭代

```python
for data, group in df.groupby("key"):
    print(data, group)

Out:
A   key  data1  data2
0   A      0      4
5   A      5      5
B   key  data1  data2
1   B      1      3
4   B      4      2
C   key  data1  data2
2   C      2      7
3   C      3      7
```

- 调用方法

```python
df.groupby("key")["data1"].describe()

Out:
	count	mean	std	min	25%	50%	75%	max
key						
A	2.0	2.5	3.535534	0.0	1.25	2.5	3.75	5.0
B	2.0	2.5	2.121320	1.0	1.75	2.5	3.25	4.0
C	2.0	2.5	0.707107	2.0	2.25	2.5	2.75	3.0
```

- 支持更复杂的操作

```python
df.groupby("key").aggregate(["min", "median", "max"])

Out:
	data1	data2
min	median	max	min	median	max
key				
A	0	2.5	5	2	5.0	8
B	1	2.5	4	2	3.0	4
C	2	2.5	3	3	5.5	8
```

- 过滤

```python
def filter_func(x):
    return x["data2"].std() > 3
df.groupby("key")["data2"].std()

Out:
key
A    4.242641
B    1.414214
C    3.535534
Name: data2, dtype: float64

df.groupby("key").filter(filter_func) # 筛选出 A 和 C

Out:
    key	data1	data2
0	A	0	2
2	C	2	8
3	C	3	3
5	A	5	8
```

- 转换

```python
df.groupby("key").transform(lambda x: x-x.mean())
df.groupby("key").apply(lambda x: x-x.mean())

Out:
	data1	data2
0	-2.5	-3.0
1	-1.5	-1.0
2	-0.5	2.5
3	0.5	    -2.5
4	1.5	    1.0
5	2.5	    3.0
```

- `apply()` 方法

```python
def norm_by_data2(x):
    x["data1"] /= x["data2"].sum()
    return x

df.groupby("key").apply(norm_by_data2)

Out:
    key	data1	data2
0	A	0.000000	4
1	B	0.200000	3
2	C	0.142857	7
3	C	0.214286	7
4	B	0.800000	2
5	A	0.555556	5
```

## 数据透视表

```python
def norm_by_data2(x):
    x["data1"] /= x["data2"].sum()
    return x

df.groupby("key").apply(norm_by_data2)

Out:

    key	data1	data2
0	A	0.000000	4
1	B	0.200000	3
2	C	0.142857	7
3	C	0.214286	7
4	B	0.800000	2
5	A	0.555556	5
```

- 将列表、数组设为分组键

```python
L = [0, 1, 0, 1, 2, 0]
df.groupby(L).sum()

Out:
    data1	data2
0	7	18
1	4	5
2	4	4
```

- 用字典将索引映射到分组

```python
df2 = df.set_index("key")

Out:
    data1	data2
key
A	0	2
B	1	2
C	2	8
C	3	3
B	4	4
A	5	8

mapping = {"A": "first", "B": "constant", "C": "constant"}
df2.groupby(mapping).sum()

Out:
            data1	data2
constant	10	    17
first	    5	    10
```

- 任意 Python 函数

```python
df2.groupby(str.lower).mean() # 变成小写

Out:
    data1	data2
a	2.5	5.0
b	2.5	3.0
c	2.5	5.5
```

- 多个有效值组成的列表

```python
df2.groupby([str.lower, mapping]).mean()

Out:
                data1	data2
a	first	    2.5	5.0
b	constant	2.5	3.0
c	constant	2.5	5.5
```

### 例：行星观测数据处理

```python
import seaborn as sns
planets = sns.load_dataset("planets")
planets.shape
>>> (1035, 6)

planets.head()

Out:
    method	number	orbital_period	mass	distance	year
0	Radial Velocity	1	269.300	7.10	77.40	2006
1	Radial Velocity	1	874.774	2.21	56.95	2008
2	Radial Velocity	1	763.000	2.60	19.84	2011
3	Radial Velocity	1	326.030	19.40	110.62	2007
4	Radial Velocity	1	516.220	10.50	119.47	2009

planets.describe()

Out:

        number	orbital_period	mass	distance	year
count	1035.000000	992.000000	513.000000	808.000000	1035.000000
mean	1.785507	2002.917596	2.638161	264.069282	2009.070531
std	    1.240976	26014.728304	3.818617	733.116493	3.972567
min	    1.000000	0.090706	0.003600	1.350000	1989.000000
25%	    1.000000	5.442540	0.229000	32.560000	2007.000000
50%	    1.000000	39.979500	1.260000	55.250000	2010.000000
75%	    2.000000	526.005000	3.040000	178.500000	2012.000000
max	    7.000000	730000.000000	25.000000	8500.000000	2014.000000

# 将 year 划分为十年
decade = 10 * (planets["year"] // 10)
decade = decade.astype(str) + "s"
decade.name = "decade"

decade.head()

Out:
0    2000s
1    2000s
2    2010s
3    2000s
4    2000s
Name: decade, dtype: object

# 按十年和 method 分组
planets.groupby(["method", decade]).sum()

Out:
	    number	orbital_period	mass	distance	year
method	decade				
Astrometry	2010s	2	1.262360e+03	0.00000	35.75	4023
Eclipse Timing Variations	2000s	5	1.930800e+04	6.05000	261.44	6025
2010s	10	2.345680e+04	4.20000	1000.00	12065
Imaging	2000s	29	1.350935e+06	0.00000	956.83	40139
2010s	21	6.803750e+04	0.00000	1210.08	36208
Microlensing	2000s	12	1.732500e+04	0.00000	0.00	20070
2010s	15	4.750000e+03	0.00000	41440.00	26155
Orbital Brightness Modulation	2010s	5	2.127920e+00	0.00000	2360.00	6035
Pulsar Timing	1990s	9	1.900153e+02	0.00000	0.00	5978
2000s	1	3.652500e+04	0.00000	0.00	2003
2010s	1	9.070629e-02	0.00000	1200.00	2011

# 对 number 进行求和，缺失的数据用 0 填充，unstack 将 decade 标签展开为列标签
planets.groupby(["method", decade])[["number"]].sum().unstack().fillna(0)

Out:
        number
decade	1980s	1990s	2000s	2010s
method			
Astrometry	0.0	0.0	0.0	2.0
Eclipse Timing Variations	0.0	0.0	5.0	10.0
Imaging	0.0	0.0	29.0	21.0
Microlensing	0.0	0.0	12.0	15.0
Orbital Brightness Modulation	0.0	0.0	0.0	5.0
Pulsar Timing	0.0	9.0	1.0	1.0
Pulsation Timing Variations	0.0	0.0	1.0	0.0
Radial Velocity	1.0	52.0	475.0	424.0
Transit	0.0	0.0	64.0	712.0
Transit Timing Variations	0.0	0.0	0.0	9.0
```

### 例：泰坦尼克号乘客数据分析

```python
import seaborn as sns

titanic = sns.load_dataset("titanic")

titanic.head()

Out:
	survived	pclass	sex	age	sibsp	parch	fare	embarked	class	who	adult_male	deck	embark_town	alive	alone
0	0	3	male	22.0	1	0	7.2500	S	Third	man	True	NaN	Southampton	no	False
1	1	1	female	38.0	1	0	71.2833	C	First	woman	False	C	Cherbourg	yes	False
2	1	3	female	26.0	0	0	7.9250	S	Third	woman	False	NaN	Southampton	yes	True
3	1	1	female	35.0	1	0	53.1000	S	First	woman	False	C	Southampton	yes	False
4	0	3	male	35.0	0	0	8.0500	S	Third	man	True	NaN	Southampton	no	True

titanic.groupby("sex")[["survived"]].mean()

Out:
        survived
sex
female	0.742038
male	0.188908

titanic.groupby(["sex", "class"])["survived"].aggregate("mean").unstack()

Out:
class	First	Second	Third
sex		
female	0.968085	0.921053	0.500000
male	0.368852	0.157407	0.135447

# 数据透视表
titanic.pivot_table("survived", index="sex", columns="class")

Out:
class	First	Second	Third
sex		
female	0.968085	0.921053	0.500000
male	0.368852	0.157407	0.135447

# 查看总的概率
titanic.pivot_table("survived", index="sex", columns="class", aggfunc="mean", margins=True)

Out:
class	First	Second	Third	All
sex			
female	0.968085	0.921053	0.500000	0.742038
male	0.368852	0.157407	0.135447	0.188908
All	    0.629630	0.472826	0.242363	0.383838

# 平均票价
titanic.pivot_table(index="sex", columns="class", aggfunc={"survived": "sum", "fare": "mean"})

Out:
	    fare	                                survived
class	First	Second	Third	            First	Second	Third
sex					
female	106.125798	21.970121	16.118810	91	70	72
male	67.226127	19.741782	12.661633	45	17	47
```

## 其他

- 向量化字符串操作
- 时间序列处理

### 多级索引

```python
base_data = np.array([[1771, 11115 ],
                      [2154, 30320],
                      [2141, 14070],
                      [2424, 32680],
                      [1077, 7806],
                      [1303, 24222],
                      [798, 4789],
                      [981, 13468]]) 
data = pd.DataFrame(base_data, index=[["BeiJing","BeiJing","ShangHai","ShangHai","ShenZhen","ShenZhen","HangZhou","HangZhou"]\
                                     , [2008, 2018]*4], columns=["population", "GDP"])

Out:
	                population	GDP
BeiJing	    2008	1771	11115
            2018	2154	30320
ShangHai	2008	2141	14070
            2018	2424	32680
ShenZhen	2008	1077	7806
            2018	1303	24222
HangZhou	2008	798	    4789
            2018	981	    13468

data.index.names = ["city", "year"]

Out:
	                population	GDP
city        year
BeiJing	    2008	1771	11115
            2018	2154	30320
ShangHai	2008	2141	14070
            2018	2424	32680
ShenZhen	2008	1077	7806
            2018	1303	24222
HangZhou	2008	798	    4789
            2018	981	    13468
```

- `data["GDP"]`
- `data.loc["ShangHai", "GDP"]`
- `data.loc["ShangHai", 2018]["GDP"]`

### 高性能的 eval

```python
df1, df2, df3, df4 = (pd.DataFrame(np.random.random((10000,100))) for i in range(4))
%timeit (df1+df2)/(df3+df4)
>>> 17.6 ms ± 120 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%timeit pd.eval("(df1+df2)/(df3+df4)")
10.5 ms ± 153 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

- 减少了复合代数式计算中间过程的内存分配

### 高性能的 Pandas：query

```python
df[(df.A < 0.5) & (df.B > 0.5)]
df.query("(A < 0.5)&(B > 0.5)")
```

- 小数组时，普通方法反而更快
