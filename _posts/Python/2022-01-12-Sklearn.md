---
title: "Scikit-learn"
subtitle: ""
layout: post
author: "L Hondong"
header-img: "img/post-bg-22.jpg"
tags:
  - Scikit-learn
  - Python
---

# Scikit-learn

Scikit-learn 库，可用来解决分类与回归问题。

## 数据集的预处理

- 鸢尾花数据集

### 数据集的下载

```python
import seaborn as sns
iris = sns.load_dataset("iris")
```

### 数据集的查看

```python
type(iris)
>>> pandas.core.frame.DataFrame

iris.shape
>>> (150, 5)

iris.head()

Out:
  
	sepal_length	sepal_width	petal_length	petal_width	species
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.7	3.2	1.3	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa

iris.info()

# 无缺失值
Out:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
sepal_length    150 non-null float64
sepal_width     150 non-null float64
petal_length    150 non-null float64
petal_width     150 non-null float64
species         150 non-null object
dtypes: float64(4), object(1)
memory usage: 5.9+ KB

iris.species.value_counts()
# 查看 specie 种类，共有 virginica,versicolor,setosa 三种，每种 50 个。
Out:
virginica     50
versicolor    50
setosa        50
Name: species, dtype: int64

# 绘制
sns.pairplot(data=iris, hue="species")
```

- 可以看到，花瓣的长度与花瓣的宽度有比较好的线性关系，用于分类的效果比较好
- 花萼的长度与花萼的宽度相关性比较差，分类效果不好
- 考虑删掉花萼的长度宽度，只保留花瓣的长度宽度

### 数据清洗

选择特征、处理缺失值等

```python
# 删除花萼的长度 sepal_length 与花萼的宽度 sepal_width
iris_simple = iris.drop(["sepal_length", "sepal_width"], axis=1)

Out:
  petal_length	petal_width	species
0	1.4	0.2	setosa
1	1.4	0.2	setosa
2	1.3	0.2	setosa
3	1.5	0.2	setosa
4	1.4	0.2	setosa
```

### 标签编码 

- LabelEncoder

```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()  # 对标签进行编码，setosa 编码为 0，versicolor 编码为 1，virginica 编码为 2
iris_simple["species"] = encoder.fit_transform(iris_simple["species"])

Out:
  petal_length	petal_width	species
0	1.4	0.2	0
1	1.4	0.2	0
2	1.3	0.2	0
3	1.5	0.2	0
4	1.4	0.2	0
5	1.7	0.4	0
...
```

### 数据标准化 

- StandardScaler
- 鸢尾花数据集特征比较接近，实际处理过程中未标准化

```python
from sklearn.preprocessing import StandardScaler
import pandas as pd

trans = StandardScaler()
_iris_simple = trans.fit_transform(iris_simple[["petal_length", "petal_width"]])
_iris_simple = pd.DataFrame(_iris_simple, columns = ["petal_length", "petal_width"])
_iris_simple.describe()

# 均值为 0，标准差为 1
Out:
  		petal_length	petal_width
count	1.500000e+02	1.500000e+02
mean	-8.652338e-16	-4.662937e-16
std		1.003350e+00	1.003350e+00
min		-1.567576e+00	-1.447076e+00
25%		-1.226552e+00	-1.183812e+00
50%		3.364776e-01	1.325097e-01
75%		7.627583e-01	7.906707e-01
max		1.785832e+00	1.712096e+00
```

### 构造训练集、验证集和测试集

- 暂不考虑验证集

```python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(iris_simple, test_size=0.2)
test_set.head()

Out:
  petal_length	petal_width	species
3		1.5	0.2	0
111	5.3	1.9	2
24	1.9	0.2	0
5		1.7	0.4	0
92	4.0	1.2	1

iris_x_train = train_set[["petal_length", "petal_width"]] # 选择出特征
iris_y_train = train_set["species"].copy()	# 选择出标签，用 copy 获取视图

iris_x_test = test_set[["petal_length", "petal_width"]]
iris_y_test = test_set["species"].copy()
```

### 预测结果的保存 

- `df.to_csv`

## 八大传统分类算法

### K 近邻

- 与待预测点最近的训练数据集中的 k 个邻居  
- 把 k 个近邻中最常见的类别预测为待预测点的类别

```python
from sklearn.neighbors import KNeighborsClassifier

clf = KNeighborsClassifier() # 构建分类器对象
>>> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights='uniform')
# 默认选 k=5

clf.fit(iris_x_train, iris_y_train) # 训练

res = clf.predict(iris_x_test) # 预测

print(res)
print(iris_y_test.values)
>>> [0 2 0 0 1 1 0 2 1 2 1 2 2 2 1 0 0 0 1 0 2 0 2 1 0 1 0 0 1 1]
>>> [0 2 0 0 1 1 0 2 2 2 1 2 2 2 1 0 0 0 1 0 2 0 2 1 0 1 0 0 1 1]

encoder.inverse_transform(res) # 翻转
>>> array(['setosa', 'virginica', 'setosa', 'setosa', 'versicolor',
       'versicolor', 'setosa', 'virginica', 'versicolor', 'virginica',
       'versicolor', 'virginica', 'virginica', 'virginica', 'versicolor',
       'setosa', 'setosa', 'setosa', 'versicolor', 'setosa', 'virginica',
       'setosa', 'virginica', 'versicolor', 'setosa', 'versicolor',
       'setosa', 'setosa', 'versicolor', 'versicolor'], dtype=object)

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy))

>>> 预测正确率:97%

out = iris_x_test.copy() # 存储数据
out["y"] = iris_y_test
out["pre"] = res

Out:
  	petal_length	petal_width	y	pre
3		1.5	0.2	0	0
111	5.3	1.9	2	2
24	1.9	0.2	0	0
5		1.7	0.4	0	0
92	4.0	1.2	1	1
57	3.3	1.0	1	1
...

out.to_csv("iris_predict.csv")
```

- 可视化

```python
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

def draw(clf):

    # 网格化
    M, N = 500, 500
    x1_min, x2_min = iris_simple[["petal_length", "petal_width"]].min(axis=0)
    x1_max, x2_max = iris_simple[["petal_length", "petal_width"]].max(axis=0)
    t1 = np.linspace(x1_min, x1_max, M)
    t2 = np.linspace(x2_min, x2_max, N)
    x1, x2 = np.meshgrid(t1, t2)
    
    # 预测
    x_show = np.stack((x1.flat, x2.flat), axis=1)
    y_predict = clf.predict(x_show)
    
    # 配色
    cm_light = mpl.colors.ListedColormap(["#A0FFA0", "#FFA0A0", "#A0A0FF"])
    cm_dark = mpl.colors.ListedColormap(["g", "r", "b"])
    
    # 绘制预测区域图
    plt.figure(figsize=(10, 6))
    plt.pcolormesh(t1, t2, y_predict.reshape(x1.shape), cmap=cm_light)
    
    # 绘制原始数据点
    plt.scatter(iris_simple["petal_length"], iris_simple["petal_width"], label=None,
                c=iris_simple["species"], cmap=cm_dark, marker='o', edgecolors='k')
    plt.xlabel("petal_length")
    plt.ylabel("petal_width")
    
    # 绘制图例
    color = ["g", "r", "b"]
    species = ["setosa", "virginica", "versicolor"]
    for i in range(3):
        plt.scatter([], [], c=color[i], s=40, label=species[i])    # 利用空点绘制图例
    plt.legend(loc="best")
    plt.title('iris_classfier')
    
draw(clf)
```

### 朴素贝叶斯

- 后验概率最大化
- 当 $X=(x_1, x_2)$发生的时候，哪一个类别 $y_k$ 发生的概率 $P(y_k|X)$ 最大

```python
from sklearn.naive_bayes import GaussianNB

clf = GaussianNB() # 构建分类器对象
clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### 决策树

- CART算法：每次通过一个特征，将数据尽可能的分为纯净的两类，递归的分下去

```python
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier() # 构建分类器对象
clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### 逻辑回归

- 特征映射成概率，全体概率之积最大化
- 训练：通过一个映射方式，将特征 $X=(x_1, x_2)$ 映射成 $P(y=c_k)$, 求使得所有概率之积最大化的映射方式里的参数
- 预测：计算 $p(y=c_k)$ 取概率最大的那个类别作为预测对象的分类

```python
from sklearn.tree import LogisticRegression

clf = LogisticRegression(solver='saga', max_iter=1000) # 构建分类器对象

>>> LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=None, solver='saga', tol=0.0001, verbose=0,
                   warm_start=False)

clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### 支持向量机

- 最小间隔的最大化
- 以二分类为例，假设数据可用完全分开：用一个超平面将两类数据完全分开，且最近点到平面的距离最大


```python
from sklearn.tree import SVC

clf = SVC(solver='saga', max_iter=1000) # 构建分类器对象

>>> SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
    kernel='rbf', max_iter=-1, probability=False, random_state=None,
    shrinking=True, tol=0.001, verbose=False)

clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### 集成方法：随机森林

- 训练集 m，有放回的随机抽取 m 个数据，构成一组，共抽取 n 组采样集
- n 组采样集分别训练得到 n 个弱分类器，弱分类器一般用决策树或神经网络
- 将 n 个弱分类器进行组合得到强分类器

```python
from sklearn.tree import RandomForestClassifier

clf = RandomForestClassifier(solver='saga', max_iter=1000) # 构建分类器对象

>>> RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                       max_depth=None, max_features='auto', max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators='warn',
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)

clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### 集成方法：Adaboost

- 根据上轮弱分类器效果，更新数据权重，弱分类器加权求和
- 训练集 m，用初始数据权重训练得到第一个弱分类器，根据误差率计算弱分类器系数，更新数据的权重
- 使用新的权重训练得到第二个弱分类器，以此类推
- 根据各自系数，将所有弱分类器加权求和获得强分类器

```python
from sklearn.tree import AdaBoostClassifier

clf = AdaBoostClassifier(solver='saga', max_iter=1000) # 构建分类器对象

>>> AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=None)

clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### 集成方法：GBDT 

- 不断地拟合残差
- 训练集 m，获得第一个弱分类器，获得残差，然后不断地拟合残差
- 所有弱分类器相加得到强分类器

```python
from sklearn.tree import GradientBoostingClassifier

clf = GradientBoostingClassifier(solver='saga', max_iter=1000) # 构建分类器对象

>>> GradientBoostingClassifier(criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_iter_no_change=None, presort='auto',
                           random_state=None, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)

clf.fit(iris_x_train, iris_y_train) # 训练
res = clf.predict(iris_x_test) # 预测

accuracy = clf.score(iris_x_test, iris_y_test) # 评估
print("预测正确率:{:.0%}".format(accuracy)) # 预测正确率:97%

draw(clf) # 可视化
```

### XGBoost

- GBDT 的损失函数只对误差部分做负梯度（一阶泰勒）展开
- XGBoost 损失函数对误差部分做二阶泰勒展开，更加准确，更快收敛

### lightgbm

- 微软：快速的，分布式的，高性能的基于决策树算法的梯度提升框架
- 速度更快

### Stacking

- 堆叠（或者叫模型融合）
- 先建立几个简单的模型进行训练，第二级学习器会基于前级模型的预测结果进行再训练