---
title: "FlowNet"
subtitle: ""
layout: post
author: "L Hondong"
header-img: "img/post-bg-5.jpg"
mathjax: true
tags:
  - 笔记
---

# FlowNet

FlowNet 主要有两个版本：

- FlowNet: Learning Optical Flow with Convolutional Networks
- FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks

## 摘要

光流是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。

如图所示即为光流的可视化，左边图片中每个像素都有一个 $x$ 方向和 $y$ 方向的位移，右图是通过计算得到的光流 flow，是个和原来图像大小相等的双通道图像。不同颜色表示不同的运动方向，深浅表示运动的速度。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-14-29-00.png" alt="FlowNet-2022-02-17-14-29-00" style="zoom:50%;" /></div>

## 一、简介

### 1.1 Motivation

### 1.2 Contributions

## 二、相关工作

### 2.1 

### 2.2 

## 三、Network Architectures

如图所示就是 FlowNet 神经网络的大体思路。

输入为两张图像，分别是第 $t$ 帧以及第 $t+1$ 帧的图像，首先通过一个由卷积层组成的收缩部分，用以提取各自的特征图。但是这样会使图片缩小，因此需要再通过一个扩大层，将其扩展到原图大小，进行光流预测。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-14-30-42.png" alt="FlowNet-2022-02-17-14-30-42" style="zoom:50%;" /></div>

网络整体上为编码模块接解码模块结构，编码模块均为 9 层卷积加 ReLU 激活函数层，解码模块均为 4 层反卷积加 ReLU 激活函数层，在文中解码模块又被称为细化模块。整个网络结构类似于 FCN（全卷积网络），由卷积和反卷积层构成，没有全连接层，因此理论上对输入图像的大小没有要求。

### 3.1 Contracting part

第一种方案：直接将输入的一对图片叠加在一起，让它们通过一系列只有卷积层的网络，称之为 FlowNetSimple。

第二种方案：将一对图片分开处理，分别进入卷积层提取各自的特征，然后再对它们的特征进行匹配，寻找它们之间的联系，称之为 FlowNetCorr。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-16-02-07.png" alt="FlowNet-2022-02-17-16-02-07" style="zoom:50%;" /></div>

图中黄色箭头所示即为比较两个特征图的操作。设两个特征图分别为 $f_1$ 和 $f_2$，它们的维度都是 $w\times h\times c$ ，即宽度为 $w$，高度为 $h$，通道数为 $c$，比如以 $x_1$ 为中心的一块和以 $x_2$ 为中心的一块，它们之间的联系用以下公式计算，假设块长为 $K:= 2k+1$，大小为 $K^2$

$$c(\mathbf{x}_{1}, \mathbf{x}_{2})=\sum_{\mathbf{o} \in[-k, k] \times[-k, k]}\left\langle\mathbf{f}_{1}\left(\mathbf{x}_{1}+\mathbf{o}\right), \mathbf{f}_{2}\left(\mathbf{x}_{2}+\mathbf{o}\right)\right\rangle$$

这一公式与神经网络的卷积操作是一样的，只不过普通的卷积是与 filter（卷积核）进行卷积，而这个是两个块进行卷积，所以它没有可以训练的权重。

计算这两块之间的联系计算复杂度是 $c \times K \times K$，而 $f_1$ 上每个块都要与 $f_2$ 上所有块计算联系，$f_1$ 上有 $w \times h$ 个块（每个像素点都可以做一个块的中心点），$f_2$ 上也有 $w \times h$ 个块，所有整个计算复杂度是 $c \times K \times K \times (w \times h) \times ( w \times h)$，这里是假设每个块在下一张图上时，可以移动任何位置。但是这样计算复杂度太高了，所以我们可以约束位移范围为 $d$（上下左右移动 $d$），就是每个块只和它附近 $D:= 2d+1$ 的位置块计算联系，而且还可以加上步长。

### 3.2 Expanding part

这里的放大部分主要由逆卷积层（upconvolutional layer）组成，而上卷积层又由逆池化层（unpooling，与 pooling 操作相反，用来扩大 feature map）和一个卷积层组成。

对 feature maps 执行逆卷积操作（绿色箭头 deconv5），并且把它和之前收缩部分对应的 feature map（灰色箭头 conv5_1）以及一个上采样的的光流预测（红色箭头 flow5）链接起来。每一步提升两倍的分辨率，重复四次，预测出来的光流的分辨率依然比输入图片的分辨率要小四倍。

论文中说在这个分辨率时再接着进行双线性上采样的 refinement 已经没有显著的提高，所以采用优化方式：the variational approach。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-16-14-56.png" alt="FlowNet-2022-02-17-16-14-56" style="zoom:50%;" /></div>

### 3.3 Variational refinement

## 四、数据集

FlowNet 主要使用了四种数据集，如表 1 所示：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-15-55-34.png" alt="FlowNet-2022-02-17-15-55-34" style="zoom:50%;" /></div>

1. Middlebury 数据集
   - 用于训练的图片对只有 8 对，从图片对中提取出的，用于训练光流的 ground truth 用四种不同的技术生成，位移很小，通常小于 10 个像素。
2. KITTI 数据集
   - 有 194 个用于训练的图片对，但只有一种特殊的动作类型，并且位移很大，视频使用一个摄像头和 ground truth 由 3D 激光扫描器得出，远距离的物体，如天空没法被捕捉，导致他的光流 ground truth 比较稀疏。
3. Mpi sintel 数据集
   - 是从人工生成的动画 sintel 中提取训练需要的光流 ground truth，是目前最大的数据集，每一个版本都包含 1041 个可一用来训练的图片对，提供的 gt 十分密集，大幅度，小幅度的运动都包含。
   - Sintel 数据集包括两种版本： 
     - Sintel final：包括运动模糊和一些环境氛围特效，如雾等。 
     - Sintel clean：没有上述 final 的特效。
4. Flying Chairs 数据集
   - 用于训练大规模的 cnns，sintel 的 dataset 依然不够大，所以作者他们自己弄出来一个 flying chairs 数据集。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-16-01-17.png" alt="FlowNet-2022-02-17-16-01-17" style="zoom:50%;" /></div>

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-17-16-17-54.png" alt="FlowNet-2022-02-17-16-17-54" style="zoom:50%;" /></div>

如图 5 所示，这一数据集背景是来自 flickr 的图片，剪切成四分之一，用 $512\times 384$ 作为背景，前景是生成的 3d 椅子模型，从这些模型中去掉一些相似的椅子模型，留下 809 种椅子，每一种有 62 个视角。

为了产生运动信息，产生第一张图片的时候会随机产生一个位移变量，与背景图片与椅子位移相关， 再通过这种位移变量产生第二个图片和光流。每一个图像对的这些变量，包括，椅子的类型，数量，大小，和产生的位置都是随机的，位移向量也是随机的产生的。

尽管 flying-chair 数据集已经很大，但是为了避免过拟合，而采用了 data Augmentation 的方法，让数据扩大，样式变多，不单调，防止分类变得严格。

# FlowNet 2.0

FlowNet 提出了第一个基于 CNN 的光流预测算法，虽然具有快速的计算速度，但是精度依然不及目前最好的传统方法。这在很大程度上限制了 FlowNet 的应用。

FlowNet2.0 是 FlowNet 的增强版，在 FlowNet 的基础上进行提升，在速度上只付出了很小的代价，使性能大幅度提升，追平了目前领先的传统方法。

主要有以下改进：

- 增加了训练数据
- 改进了训练策略

深度学习算法的一大优势即是具有学习能力，算法的性能会随着不断学习而提升。FlowNet2.0 在 FlowNet 的基础上，额外增加了具有 3 维运动的数据库 FlyingThings3D 和更加复杂的训练策略。

#### Flying Things3D 数据集

在 FlowNet2.0 中，又生成并引入了另外一个数据集 Flying Things3D，由一堆会飞的三维图形组成，更加接近现实，有 22000 对图像。

相比于 FlyingChair 中的图像只具有平面变换，FlyingThings3D 中的图像具有真实的 3D 运动和亮度变化，按理说应该包含着更丰富的运动变换信息，利用它训练出的网络模型应该更具鲁棒性。然而实验发现，训练结果不仅与数据种类有关，还与数据训练的顺序有关。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-18-11-37-07.png" alt="FlowNet-2022-02-18-11-37-07" style="zoom:50%;" /></div>

表中是 FlowNet 使用不同的训练策略以及不同的数据集顺序得到的实验结果，其中 $S_{short}$ 为 FlowNet 中的训练策略，FlowNet2 中增加 $S_{long}$ 策略和 $S_{fine}$ 策略，数字表示的是 EPE（End Point Error），这种评估方式是将所有像素点的 gound truth 和预测出来的光流之间的差别距离（欧氏距离）的平均值，EPE 越低越好。

从表中可以看出，先使用 Chairs 这样简单的数据集然后再使用 Things3D 这样复杂的数据集，可以得到比两个数据集一起 mixed 训练更好的结果。

## 训练策略

相对于 FlowNet，FlowNet 2.0 主要改变的是训练策略。

实验表明，先在 FlyingChair 上按 $S_{long}$ 策略，再在 FlyingThings3D 上按 $S_{fine}$ 策略后，所得结果最好。单独在 FlyingThing3D 上训练的结果反而下降。文中给出了解释是尽管 FlyingThings3D 比较真实，包含更丰富的运动信息，但是过早的学习复杂的运动信息反而不易收敛到更好的结果，而先从简单的运动信息学起，由易到难反得到了更好的结果。

同时，结果发现 FlowNetC 的性能要高于 FlowNetS。

下图为神经网络训练过程中的学习率（Learning Rate）随着迭代次数（Iteration）的衰减策略

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-18-11-36-41.png" alt="FlowNet-2022-02-18-11-36-41" style="zoom:50%;" /></div>

## 网络的堆叠

论文发现所有最好的光流预测算法都利用了循环优化的方法。而基于 CNN 的像素级预测算法结果往往都含有很多噪声和模糊。通常的做法都是利用一些后处理方法对结果进行优化，如 FlowNet 中，利用传统变分优化方法对 FlowNet 输出结果进行再优化。那么是否也能够利用 CNN 来代替后处理方法对结果进行再优化呢？文中对这一问题进行了探究。

实验结果证明在 FlowNetC 的基础上堆叠 FlowNetS，当以每个 FlowNet 为单位逐个进行训练时，得到的结果最优。也就是说在训练当前 FlowNet 模块时，前面的 FlowNet 模块参数均为固定状态。

此外，后续的堆叠 FlowNet 模块，除了输入 $I_1$、$I_2$ 外，再输入前一模块的预测光流 $w_i = (ui, vi)^T$，选择性地通过光流 $w_i$ 或者双线性插值扭曲图像 $I_2(x,y)$，得到变换图像 $\tilde I_{2,i}(x, y) = I_2 (x+u_i , y+v_i )$，计算误差图像 $\Vert \tilde I_{2,i}-I_1 \Vert$ 后，可以使新堆叠的 FlowNet 模块专注去学习 $I_1$ 与 $\tilde I_{2,i}-I_1$ 之间剩下的运动变换，从而有效的防止堆叠后的网络过拟合。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-18-11-41-50.png" alt="FlowNet-2022-02-18-11-41-50" style="zoom:50%;" /></div>

除了引入 FlowNetS，FlowNetC，还引入了每层只有 3/8 个通道的 FlowNetc 和 FlowNets，以小写的 $s$ 和 $c$ 表示，3/8 是因为当通道数为 3/8 时，EPE 和通过网络所需时间达成一个较好的平衡。这里的训练都是用 Chairs->Things3D 的方法，且网络按照 one-by-one 的设置训练。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-18-11-59-45.png" alt="FlowNet-2022-02-18-11-59-45" style="zoom:50%;" /></div>

实验设置及结果见表 3，CSS 表示 1 个 FlowNetC + 2 个 FlowNetS，其他以此类推，由此可见，两个小网络在一起，虽然他们的参数更少，但是效果好于一个大网络。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-18-12-00-01.png" alt="FlowNet-2022-02-18-12-00-01" style="zoom:50%;" /></div>

## Small Displacement

上面使用的 Chairs 及 Things3D 数据集相对真实场景的变动较大，所以训练的效果在实际应用时并不太好，所以作者就重新做了一个数据集，叫作 ChairsSDHom， 使用 FlowNet2-CSS 在 ChairsSDHom 及 Sintel 上 finetune 得到 FlowNet-Css-ft-sd , finetune 时对 small displacement 的 loss 权重加强，效果在微变物体上表现变好，变化大的物体的效果也还不错，但是仍然还有噪声。于是作者就设计了图 2 中右下角的 FlowNet-SD。

主要改进： 

- 将 FlowNetS 的第一层的 stride=2 变为 1 
- 将 7x7 和 5x5 变为几个 3x3 
- 在上采样前加了卷积层用了平滑噪声

## Fusion

最后的 Fusion 融合了前面的 FlowNetCSS-ft-sd 及 FlowNet-SD 的结果，输出预测的光流图 Flow。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/FlowNet-2022-02-18-18-48-17.png" alt="FlowNet-2022-02-18-18-48-17" style="zoom:50%;" /></div>

从实验结果来看，FlowNet2 在各个公共数据库上，在精度方面已经追平了目前最好的一些传统算法。同时，在速度上依然保持着高效快速的优势。下面看一下各种情况下 FlowNet2 的输出结果：

- 小位移情况下，FlowNet2-CSS 的光流预测噪声非常大，而 FlowNet2-SD 的输出非常光滑，最后融合结果充分利用了 FlowNet2-SD 的结果。
- 大位移情况下，FlowNet2-CSS 预测出了大部分运动，而 FlowNet2-SD 则丢失了大部分运动信息，最后融合结果又很好的利用了 FlowNet2-CSS 的结果。

综上，FlowNet2-CSS 与 FlowNet2-SD 做到了很好地互补，共同保证了 FlowNet2 在各种情况下的预测质量。

文中还通过将 FlowNet2 的预测结果直接用于运动分割和动作识别的任务中，证明 FlowNet2 的精度已完全可以和其他传统算法媲美的程度，已达到可以实际应用的阶段。