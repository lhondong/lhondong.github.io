---
title: "网络之 TCP 拥塞控制算法 BBR"
subtitle: "TCP 中的 BBR 整理"
layout: post
author: "L Hondong"
header-img: "img/post-bg-5.jpg"
tags:
  - TCP
  - BBR
  - 拥塞控制
---

# 网络之 TCP 拥塞控制算法 BBR

## 核心理解：
整个传输线路有三种状态：空闲（包来了马上得到处理）、排队（处理不过来了，先放到队列里面等着）、丢包（队列都塞满了，只能丢弃）；

Reno 和 CUBIC 实际上是一类（AIMD-Additive Increase Multiplicative Decrease）：他们最大的特点（或者说是缺陷）是把丢包当做拥塞。Reno 使得传输线路在上面三种状态间来回切换；CUBIC 也类似，但是相比 Reno，CUBIC 让传输线路较少处于空闲状态，也因为如此，CUBIC 会导致延迟的增加，因为充分利用了排队功能。

Vegas 和 BBR 是另外一种尝试，它们努力让传输线路一直维持在即将开始排队但是还没排队的状态。它们重点依赖对 RTT 的计算，而不像 Reno 和 CUBIC 主要依赖对 ack 包的追踪。

BBR 有一个最大的特点，就是不直接把丢包当成是拥塞。在 5%以下的丢包，BBR 几乎不做出反应；在 5%-20%只做很小的反应；只有在丢包达到 20%以上时，BBR 才会把传输速度降下来。

Vegas 传输速度也是线性恢复的，这一点和 Reno 类似。另外一个问题在于，Vegas 在开始排队时就开始回撤，如果同一个线路上有类似 Reno 的 session 也在跑，因为 Reno 需要等到丢包了才会回撤，所以整个线路都会被 Reno 挤占掉。

BBR 与 Reno 和 CUBIC 共处，从报告上来讲，容易挤占 CUBIC 的空间。具体与 queue 设置、Bandwidth Delay Product、RTT 等相关。

## 部分重点内容摘要：
1. P 层只管发送网络包，但是把管理数据流（包括检测和修复丢包、管理数据流量）交给传输层的协议（也即 TCP 协议）。
2. TCP 并不是一个固定的协议，TCP 怎么管理数据流，怎么检测丢包并做出反应，是有很多的实现的变种的。
3. Reno 流控制算法，基于 ACK 反馈，它的特点在于：
    * 对 out-of-order 的 ACK 反馈包的反应比较敏感（会把发送速率降低到原来的一半）。
    * 在带宽比较大的情况下，在降速之后再恢复回原来的带宽利用率耗时较长（10G 带宽+30msRTT，需要 3.5 小时才能让带宽从 5G 提升到 10G）。
    * Congestion Avoidance 理论上会一直增加（每个 RTT 增加一个 segment），最终会导致 out-of-order ACK 反馈的发生，所以最终会导致类似锯齿状（sawtooth）的带宽使用情况。
    * 如果发生丢包导致的 timeout 的情况，Reno 就丢失了整个流控制的 ACK 反馈信号，只能重新开始整个 session 的管理（slow-start）。
4. BIC 及 CUBIC 流控制算法：
    * BIC 基于二分查找算法（binary chop）来从 out-of-order 的 ACK 反馈中恢复发送速率。
    * BIC 有一个最大的常量限制，而且一般比 Reno 算法固定的一个 segment 要大一些。
    * BIC 在低 RTT 网络环境下，表现得非常的激进（很容易在短时间内又超过上次碰到的最大值），BIC 使用指数函数（exponential function）来管理这个流的发送数据大小。
    * CUBIC 是 BIC 的一种改进，它使用三阶多项式函数（third-order polynomial function）来管理流数据大小。
    * CUBIC 对 out-of-order 的 ACK 反馈，回撤程度比 Reno 小，而且能更快地恢复到原来的 flow rate

## 核心摘要：
BBR 降低了网络的 RTT，全球平均降低 53%，在发展中国家降低 80%。（主要是因为它尽量避免使用 queue）

传统的 loss-based congestion control（比如 CUBIC）的问题主要在于：把 packet loss 和 congestion 等同起来了；而现在这个两个事情并不能简单的等同起来。

对 TCP 的性能来讲，重要的几个概念：
* round-trip propagation time (RTprop)（在没有 queue 的情况下的 RTT）
* bottleneck bandwidth, BtlBw (the bandwidth at the slowest link in each direction)
* Bandwidth Delay Product (BDP) _is the maximum possible amount of data in transit in a network, and is obtained by multiplying the bottleneck bandwidth and round-trip propagation time.

网络的最佳运行点（optimal operating point）在于：刚好在 BDP 的阈值；而 loss-based congestion control 算法则运行在 BDP+bottleneck buffer
BBR 追踪两个基本的数据：bottleneck bandwidth and round-trip propagation time
BBR 每 8 个 RTT 周期，会控制传输速度为当前已探测的 bottleneck bandwidth 的 1.25，0.75，1，1，1，1，1（倍数），主要是用来探测是否有更高的传输速率的可能性（比如线路情况变化了）。

BBR 相对 CUBIC，对于 2G、3G 等网络降低用户的延迟非常有帮助（延迟本身主要是因为手机-SGSN (serving GPRS support node) 直接的 buffer 导致，150KB-10MB）。

## 核心理解：
当前 TCP 网络慢的主要问题在于：把 packet loss 理解成了 congestion，这在 1980s 那个年代是对的，因为技术条件的限制。

目前 loss-based congestion control 算法（例如 CUBIC），在 buffer 很大的情况下，会引发 [bufferbloat](https://en.wikipedia.org/wiki/Bufferbloat) 问题（delay 比较大，delay 的 jitter 也比较大）；在 buffer 很小的情况下，会误把丢包当做 congestion，导致较低的吞吐量。

TCP 传输三大限制常量：RTprop（管子的长度），BltBw(bottleneck bandwidth)（管子中间的最小直径），bottleneck buffer。

因以上三个限制，引发传输的三个阶段：app-limited（应用不够快），bandwidth-limited（处理不过来了，但是 buffer 还有空间），buffer-limited（不但处理不过来，而且 buffer 也被占满了）。

大的 buffer 可以是 BDP（BDP =BtlBw × RTprop）的几个数量级大小，可能导致增加数秒的 RTT，从而增加整个延迟。

通过测量这两个参数（bottleneck bandwidth and round-trip propagation time, or BBR）：从不确定性中（ambiguities）寻找到 Kleinrock's optimal operating point。

当管道 100%利用的时候，并不意味着没有 queue（比如第一次发送 10 个包，然后稳定地发送 5 个包，刚好管道的速率也是 5，那么就会保持一个永远不能消失的 5 个包的队列），进而导致 delay。

由于对 buffer 的使用情况不一样，短的连接和请求（比如 request/response 的类型连接）适合探测真实的 RTprop，但是却无法获知 BtlBw；大数据量、持续时间较长的连接适合探测 BtlBw，但是却无法获知真实的 RTprop（除了开始几个 RTT 数据之外）。

pacing_gain 是 BBR 的最重要的调节参数；会以 BtlBw x pacing_gain 的速率来发送。

BBR 分为四个阶段：STARTUP，DRAIN，ProbeBW，ProbeRTT；大部分的时间会花在 ProbeBW 阶段；

1. STARTUP：当 BBR 发现在三次尝试 double 传输速率时，但是实际上增加的量小于 25%，则认为管道满了，退出 STARTUP 阶段，进入 DRAIN 阶段。详细参考：[Startup](https://tools.ietf.org/id/draft-cardwell-iccrg-bbr-congestion-control-00.html#estimating-when-startup-has-filled-the-pipe)
    * 这里和 CUBIC 的 slow-start 的核心区别在于：slow-start 是基于丢包才停止的，而 BBR 是基于对带宽增长的停滞而退出这个阶段。详细可参考这个 [PDF 文档](https://www.ietf.org/proceedings/97/slides/slides-97-iccrg-bbr-congestion-control-02.pdf) 的 33 页。
2. DRAIN：主要是释放掉在 STARTUP 阶段造成的 queue，以降低后续的延迟；它的 pacing_gain 是 STARTUP 时期的倒数。
3. ProbeBW：每 8 个 RTT 周期，会控制传输速度为当前已探测的 bottleneck bandwidth 的 1.25，0.75，1，1，1，1，1（所谓的 pacing_gain，即倍数），主要是用来探测是否有更高的传输速率的可能性（比如线路情况变化了）。
4. ProbeRTT：每隔几秒会进入一次 ProbeRTT 阶段，它在至少一个 RTT 时间长度期间，把 inflight 包降低到 4 个，然后恢复来的状态；这一阶段主要是释放出 queue，让其他的 flow 能探测到真实的 RTprop 数值，而当其他 flow 发现新的 RTprop 值时，也会触发进入 ProbeRTT 阶段。目的主要是保证公平和稳定。

CUBIC 只能一直增加，直到丢包发生或者接受者的 inflight limit（receiver window size），才停止增加传输速率。
