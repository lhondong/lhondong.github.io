---
title: "Deep Video Super-Resolution Using HR Optical Flow Estimation"
subtitle: ""
layout: post
author: "L Hondong"
header-img: "img/post-bg-30.jpg"
mathjax: true
tags:
  - 视频超分
  - 光流估计
---

# Deep Video Super-Resolution Using HR Optical Flow Estimation

## 摘要

现有的 VSR 方法都是在估计 LR 帧之间的光流，然而，LR 帧的光流不能为 HR 恢复提供精确的细节。提出了一种端到端的视频 SR 网络来超分光流和图像，LR 帧的光流 SR 可以提供精确的时序相关性，从而提高视频 SR 性能。提出 optical flow reconstruction network (OFRnet) 从粗到细地推理 HR 帧，然后使用 HR 光流进行运动补偿，

## 一、简介

### 1.1 Motivation

相比于图像超分任务，Adjacent frames in a video clip provide additional information for Video SR。

提出 SOF-VSR，通过光流恢复时间细节，从而提高 VSR 的准确性和时间一致性。视频超分旨在生成具有与 LR 时序一致性且视觉效果更好的高分辨率图像。视频超分的关键挑战在于：如何更有效的利用连续帧间的时序信息。现有的深度学习方法通常采用光流方法从 LR 图像上估计时序信息，低分辨率的光流会影响 HR 图像的细节复原效果。

该文提出了一种端到端的视频超分方法，它同时对光流与图像进行超分，光流超分开源提供更精确的时序信息进而提升视频超分的性能。作者首先提出一种光流重建网络 (OFRNet) 以“自粗而精”的方式预测 HR 光流，然后采用 HR 光流进行运动补偿编码时序信息，最后采用补偿后的 LR 图像送入超分网络 (SRNet) 生成超分结果。

作者通过充分的实验验证了 HR 光流对于超分性能提升的有效性，所提方法在 Vid4 与 DAVIS10 数据集上取得了 SOTA 性能。

### 1.2 Contributions

- 将光流与图像的超分集成到统一的 SOF-VSR 框架中，光流超分有助于提升图像超分性能；
- 提出一种 OFRNet 采用“自粗而精”的方式从 LR 图像中预测 HR 光流，它有助于重建更精确的时序信息；
- 在公开基准数据集 (Vid4，DAVIS10) 上，所提 SOF-VSR 取得了 SOTA 性能。

## 二、相关工作

### 2.1 

### 2.2 

## 三、方法

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-25-01.png" alt="SOF-VSR-2022-02-22-22-25-01" style="zoom:50%;" /></div>

上图给出了所提方法的流程图，它包含光流估计模块、运动补偿模块以及图像超分模块。接下来从四个方面进行展开介绍与分析 (OFRNet、运动补偿、SRNet 以及损失函数）。

先给出关于视频超分的定义，给定帧连续 LR 序列，视频超分的目标是对中间帧进行超分重建。在该文中，作者首先将输入由 RGB 空间转为 YCbCr，仅针对 Y 通道进行超分。LR 图像序列首先送入 OFRNet 中预测 HR 光流，它采用中间帧以及近邻帧作为输入，预测 HR 光流，然后采用采用 space-to-depth 将 HR 光流变换到低分辨率，得到 LR 光流体；下一步采用所得光流进行运动补偿；最后补偿后的 LR 图像将送入 SRNet 进行 HR 图像重建。

### 3.1 Optical Flow Reconstruction Net (OFRnet)

作者设计了一种 CNN 网络用于从 LR 图像中直接预测 HR 光流，它以成对图像（中间帧以及近邻帧）作为输入，输出 HR 光流，该过程可以描述为：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-28-43.png" alt="SOF-VSR-2022-02-22-22-28-43" style="zoom:50%;" /></div>

为降低模型大小以及训练难度，作者采用了尺度递归架构，见上图。在前两级作者采用递归模块估计不同尺度的光流信息，在第三级作者首先采用递归架构生成深度表达，然后引入 SR 模块重建 HR 光流。这种尺度递归架构有助于 OFRNet 处理复杂的运动，同时使得模型更轻量。

### 3.2 Motion Compensation Module

通过 OFRNet 得到 HR 光流后，作者通过 space-to-depth 变换对 HR 光流与 LR 图像进行桥接，见下图。注：光流的幅值需要除以 s 以匹配 LR 图像的空间分辨率。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-29-27.png" alt="SOF-VSR-2022-02-22-22-29-27" style="zoom:50%;" /></div>

基于上述变换后的 LR 光流对 LR 图像进行运动补偿，补偿过程可以描述为：

$$
\left[F_{i \rightarrow j}^{H}\right]^{s H \times s W \times 2} \rightarrow\left[F_{i \rightarrow j}^{H}\right]^{H \times W \times 2 s^{2}}
$$

$$
C_{i \rightarrow j}^{L}=W(I_i^L,\left[F_{i \rightarrow j}^{H}\right]^{H \times W \times 2 s^{2}})
$$

其中 $W(\cdot)$ 表示双线性插值仿射变换操作，表示 concat 后的结果。可以看到：尽管是在 LR 层面进行的运动补偿，但 HR 光流的时序信息均被编码到了补偿帧。

通过 OFRNet 得到 HR 光流后，作者通过 `space-to-depth` 变换对 HR 光流与 LR 图像进行桥接，见下图。注：光流的幅值需要除以 s 以匹配 LR 图像的空间分辨率。

### 3.3 SRNet

SRNet 采用帧 LR 图像作为输入，并对中间帧进行超分。完成运动补偿后，将每个近邻帧的补偿帧通过 concat 进行组合并送入到 SRNet 中进行图像重建，该过程可以描述为：

$$
I^{SR}_0 = \text{Net}_{SR}(C^L; \Theta_{SR})
$$

其中 $I^{SR}_0$ 表示超分结果，表示运动补偿后的 LR 输入以及中间帧输入。下图给出了 SRNet 的网络结构示意图。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-35-59.png" alt="SOF-VSR-2022-02-22-22-35-59" style="zoom:50%;" /></div>

### 3.4 Loss Function

作者分别针对 SRNet 和 OFRNet 设计了损失函数，它们分别是：

$$
L_{SR}=\left\|I_{0}^{S R}-I_{0}^{H}\right\|_{2}^{2}
$$

$$
L_{OFR}=\sum_{i \in[-N, N], i \neq 0} \frac{L_{level3, i}+\lambda_{2} L_{level2, i}+\lambda_{1} L_{level1, i}}{2 N}
$$

$$
\begin{aligned}
L_{level3, i} &=\left\|W\left(I_{i}^{H}, F_{i \rightarrow 0}^{H}\right)-I_{0}^{H}\right\|_{1}+\lambda_{3}\left\|\Delta F_{i \rightarrow 0}^{H}\right\|_{1} \\
L_{level2, i} &=\left\|W\left(I_{i}^{L}, F_{i \rightarrow 0}^{L}\right)-I_{0}^{L}\right\|_{1}+\lambda_{3}\left\|\Delta F_{i \rightarrow 0}^{L}\right\|_{1} \\
L_{level1, i} &=\left\|W\left(I_{i}^{L R}, F_{i \rightarrow 0}^{L D}\right)-I_{0}^{H}\right\|_{1}+\lambda_{3}\left\|\Delta F_{i \rightarrow 0}^{L D}\right\|_{1}
\end{aligned}
$$

其中用于约束光流的平滑性，作者设置超参为。联合训练的损失函数定义如下：

其中。

## 四、实验

### 4.1 Datasets

训练数据：作者选用了 CDVL，测试数据为 Derf4、Vid4 以及 DAVIS。评价准则：PSNR、SSIM 以及 MOVIE。下图给出了所提方法在 Vid4 上的参数量与 FLOPS 以及指标。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-37-32.png" alt="SOF-VSR-2022-02-22-22-37-32" style="zoom:50%;" /></div>

### 4.2 

下表给出了所提方法与其他视频超分方法的在 Vid4 数据集上性能对比：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-38-02.png" alt="SOF-VSR-2022-02-22-22-38-02" style="zoom:50%;" /></div>

下表给出了所提方法与其他视频超分方法的在 DAVIS10 数据集上性能对比：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SOF-VSR-2022-02-22-22-39-04.png" alt="SOF-VSR-2022-02-22-22-39-04" style="zoom:50%;" /></div>

## 五、总结
