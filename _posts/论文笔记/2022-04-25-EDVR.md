---
title: "EDVR"
subtitle: "Video Restoration with Enhanced Deformable Convolutional Networks"
layout: post
author: "L Hondong"
header-img: "img/post-bg-40.jpg"
mathjax: true
tags:
  - 视频超分
  - 视频传输
---

# EDVR: Video Restoration with Enhanced Deformable Convolutional Networks

提出 VSR 新型模块，性能提升显著，获得 NTIRE2019 四个赛道冠军，且遥遥领先！

虽然 TDAN 针对视频的帧间对齐和融合做了一定的工作，但仍然没有很好解决问题：

- 提升**帧间对齐、帧间相关性**的性能；
- 高效的进行**帧间融合**。

EDVR (Video Restoration framework with Enhanced Deformable convolutions) 针对 帧间对齐 (frames alignment)、帧间融合 (frames fusion)，提出了对应的模块：

- 对齐模块，Pyramid, Cascading and Deformable (PCD)，由粗到细的进行帧间对齐；
- 融合模块，Temporal and Spatial Attention (TSA)，通过注意力机制区分特征的重要性。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/EDVR-2022-04-25-10-23-40.png" alt="EDVR-2022-04-25-10-23-40" style="zoom:50%;" /></div>

整体结构分为 4 部分：

1. Feature extraction，也就是 PreDeblur Module；
2. Alignment，也就是 PCD Align Module；
3. Fusion，也就是 TSA Fusion Module；
4. Reconstruction，也就是 Reconstruction Module。

特征抽取和重构由一般的卷积层组成，论文主要创新点在 PCD 对齐模块 和 TSA 融合模块。

### Pyramid, Cascading and Deformable (PCD)

序列图像经过特征抽取模块后，得到每帧图像的特征 $F_{t+i}, i \in[-N, +N]$ 作为 PCD 对齐模块的输入。

PCD 模块对每个相邻帧都进行了与参考帧的对齐操作，考虑到对齐中出现的复杂动作和大视差问题，PCD 参考了光流估计中的金字塔流程、级联精化的思想，提出了如下图结构。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/EDVR-2022-04-25-10-25-48.png" alt="EDVR-2022-04-25-10-25-48" style="zoom:50%;" /></div>

PCD 对齐模块使用金字塔模块抽取特征，只不过每层金字塔使用的是可变形卷积。下面取参考帧 $i$ 与相邻帧 $t+i$ 的对齐进行说明。

金字塔特征使用 stride 为 2 的卷积操作，获取下采样特征（虚黑线）：

$$
F_{t+i}^{l+1}=f_{\text{stride2-conv}}\left(F_{t+i}^{l}\right), F_{t+1}^{1}=F_{t+i}
$$

1. 与图像特征同级的 offset $\Delta P_{t+i}^{l}$ （因为是可变形卷积 $\mathrm{v}$，所以还有 weights，和 offset 类似） 则由同级的图像特征 $\left[F_{t+i}^{l}, F_{t}^{l}\right]$ （黄色实线）和下一级的 offset $\Delta P_{t+i}^{l+1}$ 双线性揷值上采样（左边紫色虚线）后结合运算得到；
2. 对同级相邻帧特征 $F_{t+i}^{l}$ 做可变形卷积（蓝色实线），最后与下一级的对齐特征 $\left(F_{t+i}^{a}\right)^{l+1}$ 双线性插值上采样（右边紫色虚线）后进行结合运算，得到相邻帧的对齐特征 $\left(F_{t+i}^{a}\right)^{l}$ 。

$$
\Delta P_{t+i}^{l}=f\left(\left[\left[F_{t+i}^{l}, F_{t}^{l}\right],\left(\Delta P_{t+1}^{l+1}\right)^{\uparrow 2} * 2\right]\right) 
$$

$$
\left(F_{t+i}^{a}\right)^{l}=g\left(\left[\operatorname{DConv}\left(F_{t+i}^{l}, \Delta P_{t+i}^{l}\right),\left(\left(F_{t+i}^{a}\right)^{l+1}\right)^{\uparrow 2}\right]\right)
$$

其中，$f, g$ 是一般的卷积层，$[\ldots, \ldots]$ 是 concat 操作，$(\ldots)^{\uparrow 2}$ 是双线性上采样操作，下一级的 offset 上采样后有 $*2$ 操作，因为 feature map 变大了 2 倍（论文中没显式表示，代码里有）。

最后，融合相邻帧的最上层对齐特征 $\left(F_{t+i}^{a}\right)^{1}$ 和参考帧特征 $F_{t}^{1}$ 进行可变形卷积操作（紫色背景区域）：

$$
\Delta P_{t+i}^{0}=f\left(\left[\left(F_{t+i}^{a}\right)^{1}, F_{t}^{1}\right]\right)
$$

$$
F_{t+i}^{a}=\operatorname{DConv}\left(\left(F_{t+i}^{a}\right)^{1}, \Delta P_{t+i}^{0}\right)
$$

### Temporal and Spatial Attention (TSA)

本文提到 inter-frame temporal relation 和 intra-frame spatial relation 很重要：

1. 不同的相邻帧由于时序位置、模糊程度、视差问题，为参考帧提供的信息量不同；
2. 对齐效果不好的帧对接下来的重构阶段不利。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/EDVR-2022-04-25-10-38-54.png" alt="EDVR-2022-04-25-10-38-54" style="zoom:50%;" /></div>

对每帧的对齐特征 $F_{t+i}^{a}$ 都用参考帧特征 $F_{t}$ 和对应的相邻帧对齐特征 $\left(F_{t+i}^{a}\right)^{0}$ 计算一个注意力热图，来表示帧间相似距离，亦即 temporal attention:

$$
h\left(F_{t+i}^{a}, F_{t}^{a}\right)=\operatorname{sigmoid}\left(\sum_{\text{channel}}\left(\theta\left(F_{t+i}^{a}\right)^{T} \phi\left(F_{t}^{a}\right)\right)\right)
$$
然后作用到对齐特征上获得注意力加成特征 $\widetilde{F}_{t+i}^{a}$，最后用 $1 \times 1$ 卷积做融合得到：

$$
\widetilde{F}_{t+i}^{a}=F_{t+i}^{a} \odot h\left(F_{t+i}^{a}, F_{t}^{a}\right)
$$

$$
F_{\text {fusion }}=\operatorname{Conv}\left(\left[\widetilde{F}_{t-N}^{a}, \ldots, \widetilde{F}_{t}^{a}, \ldots, \widetilde{F}_{t+N}^{a}\right]\right)
$$

其中，$\theta, \phi$ 是卷积嵌入操作，嵌入后的通道数和原来一样（默认 64 通道），然后按通道相乘、累加得到单通道特征图，最后经过 sigmoid 得到 temporal attention map; $\odot$ 是矩阵元素乘，$[\ldots, \ldots]$ 是通道 concat 操作。

代码做了两个 fusion（输出都是 64 通道），一个作为特征（也就是垂直方向），一个作为金字塔输入，做金字塔卷积后输出一个 spatial attention map 与垂直方向的 fusion 特征相乘，一个再做卷积后加上去。最后得到最下面的 temporal & spatial attention fusion feature，作为最后的重构模块输入。

## 实验设置及参数

NTIRE19 比赛数据集 REDS：分辨率 720p，240 train clips，30 validation/test clip，每个 clip 100 帧。实际用 266 clips 训练，4 clips 验证，30 clips 测试不可见。

网络的通道数为 128，输入分辨率为 64 或 256（SR 或 deblurring 任务），输入帧为 5 帧（N=2），batch-size 为 32，损失函数使用 Carbonnier Loss。

Adam 优化器，$\beta_1=0.9,\beta_2=0.999$，学习率为 $4\times 10^{-4}$。

实际模型很难训，作者给出训练指导：

1. 使用更小的学习率；
2. 每次训练都保存 checkpoints，如果模型训炸了，就用上一次的 checkpoint 继续训；
3. 使用简单数据训，然后不断加困难数据（curriculum learning）

难训/bad cases 一大原因和可变形卷积相关。

## 实验结果及分析

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/EDVR-2022-04-25-13-44-00.png" alt="EDVR-2022-04-25-13-44-00" style="zoom:50%;" /></div>

PCD 模块和 TSA 的消融实验：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/EDVR-2022-04-25-13-44-19.png" alt="EDVR-2022-04-25-13-44-19" style="zoom:50%;" /></div>