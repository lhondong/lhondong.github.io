---
title: "MAE: Masked Autoencoders Are Scalable Vision Learners"
subtitle: "MAE"
layout: post
author: "L Hondong"
header-img: "img/post-bg-3.jpg"
mathjax: true
tags:
  - Transformer
  - 笔记
---

# MAE: Masked Autoencoders Are Scalable Vision Learners

GPT2：Language Models **are** Unsupervised Multitask **Learners**

GPT3：Language Models **are** Few-Shot **Learners**

FAIR, He Kaiming

## 摘要

证明了 MAE 是一种可扩展的自监督学习器。随机地对 patches 掩码，然后重建这些 patches。整个思想来自于 BERT 的带掩码的语言模型，但不一样的是 BERT 里 token 是词，而 MAE 里是块，预测的是这个块里的所有的像素。

两个核心设计：

1. 设计了一个非对称编码器-解码器体系结构，其中的编码器仅在可见的 patches 子集上运行（没有被 mask 的 tokens）。解码器为轻量级解码器，从 latent representation 和 mask tokens 中重建原始图像。（BERT 预测的东西相对来说比较简单，因此解码器就是一个简单的全连接层。）
2. 发现 mask 输入图像比例很高的时候（例如 75%），生成了一个 nontrivial and meaningful 自监督学习任务。（简单来说，如果只是预测几块的话，只需要简单的插值就可以，模型无法学习到高层特征，但是把一大半的像素给遮住的话，迫使模型学一些更好的表征）

这两种设计的结合使我们能够高效地训练大型模型：加快了训练速度 3 倍甚至更多（训练量是之前的 1/4），同时提高了准确性。

Scalable approach 可以使得 **high-capacity** 模型泛化性很好：在仅使用 ImageNet-1K 数据的方法中，vanilla ViT-Huge 模型实现了最佳准确率 (87.8%)，超过了所有在 ImageNet-21k pre-training 的 ViT 变体模型。

迁移到下游任务中，MAE 性能优于有监督的预训练，并有着很好的可扩展性。

## 一、简介

### 1.1 Motivation

有监督学习需要数亿的有标注图像，然而，通常这些有标注图像都不可以公开访问。

NLP 通过自监督的预训练解决了这种对数据的需求，解决方案主要基于 GPT[40,41,4] 中的自回归语言建模和 BERT 中的掩蔽自编码器。概念上很简单：删除一部分数据，并学习预测删除的内容。这些方法现在可以训练包含超过 1000 亿个参数的可泛化 NLP 模型 [4]。

Masked autoencoders 在图像中已有应用，比如在 denoising autoencoders 中，在一个图片中加入很多噪声，然后通过去噪学习对这个图片的理解。

Masked autoencoders，一种更通用的去噪自编码器，在视觉上相关的研究 [49,39] 先于 BERT。BERT 训练词向量的方式，就是将一个句子中的这个单词 mask 掉，然后根据这个词周围的其他词汇来预测这个 mask 掉的单词的语义；这种特征学习的方式在 NLP 中已经证明是 work 的了，但是在图像上一直没人去深入想一下为什么 auto-encoder 中间层的 feature 迁移来做分类效果很差。

#### 是什么使 masked 自编码在视觉和语言之间有不同？

本文试图从（模型结构，信息特点，解码器）三个方面来回答这个问题。

1. **模型结构不同**：目前为止，architectures 都是不同的。在视觉上，卷积网络在过去十年中占主导地位。卷积通常在规则网格上运行，并不是直接集成 “指标”（如 mask tokens [14] 或 position embedding [47]）到卷积网络。然而，这种架构上的差距已经通过引入 Vision transformer (ViT) [16] 得到解决，应该不再是一个障碍。
2. **图像和文本的信息密度不一样**：语言是人类产生的信号，具有高度的语义和信息密集性。当训练一个模型来预测每句话中只有几个遗漏的单词时，这项任务似乎诱导了复杂的语言理解。相反，图像是具有大量空间冗余的自然信号。一个丢失的 patch 可以从邻近的 patch 中恢复，而不需要对部件、对象和场景有高水平的理解。为了克服这种差异并鼓励学习有用的特性，本文展示了一个简单的策略，可以在计算机视觉中工作得很好：用随机 mask 掩盖很大一部分图像。这种策略在很大程度上减少了冗余，并创建了一个具有挑战性的自监督任务，需要超越低级图像统计的整体理解。想对本文的重建任务有一个定性的认识，请参见图 2-4）。
3. **解码器重建是像素级别的任务**：自编码器的解码器将潜在的表示映射回输入，在重构文本和图像之间扮演着不同的角色。在视觉中，解码器对像素进行重构，因此其输出的语义水平低于一般的识别任务。这与语言相反，在语言中，解码器预测包含丰富语义信息的缺失单词。虽然在 BERT 中解码器可以是微不足道的 （一个 MLP)，但本文发现，对于图像，解码器的设计在确定学到的潜在表示的语义级别方面起着关键作用。

在此分析的启发下，本文提出了一种简单、有效、可扩展的用于视觉表示学习的 masked 自编码器 (MAE)。本文的 MAE masks 来自输入图像的随机 patch，并在像素空间中重建缺失的 patch。它具有非对称的编解码设计。编码器 patch 的操作只在可见的子集 （没有被 mask 的 tokens)， 解码器是轻量级的，并根据 mask tockens 的潜在表示重构输入图像 （图 1)。将 mask tokens 的小解码器（非对称 encoder-decoder ）计算，其计算量得到很大的下降。在这种设计下，一个非常高的 mask 比 （如 75%) 可以达到双赢的情况：它优化精度，同时允许编码器只处理一小部分 （如 25%) 的 patch，这可以减少 3 倍或更多的预训练时间，同样也可以减少内存消耗，使能够轻松地将 MAE 扩展到大型模型。

<div align=center><img src="/Assets/Images/MAE-2022-03-02-18-56-33.png" alt="MAE-2022-03-02-18-56-33" style="zoom:50%;" /></div>

解码器的输入 vector 中，空白的（灰色的 token）相当于是 mask embedding，重建出所有 patch，组合成目标。

重点：不对称的 Autoencoder，编码器非常大，解码器非常小，因为不需要解码器重建出图像（本来就知道图像）。pre-training 不是重点，不需要训练的非常好，重点是学习编码器的参数而不是解码器的参数。

问题：

1. 预训练时费时间
2. 引入 pre-training 和 fine-tuning 之间的 gap。

### 1.2 Contributions

MAE 学习了非常高容量的模型，因此可以很好地泛化。通过 MAE 预训练，可以在 ImageNet-1K 上训练像 ViT-Large/-Huge 这样的数据匮乏模型，提高泛化性能。使用普通的 ViT - Huge 模型，在 ImageNet-1K 上微调时，达到了 87.8% 的精度。这比以前所有只使用 ImageNet-1K 数据的结果都要好。

本文还评估了迁移学习在目标检测、实例分割和语义分割方面的作用。在这些任务中，本文的预训练比有监督的预训练取得更好的结果，更重要的是，观察到通过 scaling up models 获得显著的增益。这些观察结果与在 NLP 自监督预训练中所看到的情况一致，希望它们将使 CV 领域能够探索类似的轨迹。

## 二、相关工作

### 2.1 Masked 语言建模

Masked 语言建模及其自回归对应方法，如 BERT [14] 和 GPT [40,42,4]，是非常成功的 NLP 预训练方法。这些方法提供了输入序列的一部分，训练模型来预测缺失的内容。这些方法已经被证明可以很好地扩展，大量的实验证据表明这些预先训练的表示可以很好地推广到各种下游任务。

### 2.2 自编码

自编码是学习表征的一种经典方法。它有一个将输入映射到潜在表示的编码器和一个重建输入的解码器。例如，PCA 和 k-means 是自动编码器 [25]。去噪自动编码器 (DAE)[48] 是一类损坏输入信号并学习重建原始的、未损坏的信号的自动编码器。一系列的方法可以被认为是在不同的破坏下的通用 DAE（如屏蔽像素 [49,39,6] 或删除颜色通道 [59]）。我们的 MAE 是去噪自编码的一种形式，但在许多方面不同于经典的 DAE。

### 2.3 Masked 图像编码

从被 masking 损坏的图像中学习表示。[49] 的开创性工作提出 mask 作为一种噪声类型的 DAE。编码器 [39] 使用卷积网络填补大的缺失区域。在 NLP 的激励下，相关的最近的方法 [6,16,2] 是基于 transformer [47]。iGPT [6] 作用于像素序列，预测未知像素。ViT 论文 [16] 研究了用于自监督学习的 masked patch 预测。最近，BEiT [2] 提出预测离散 tokens [37,43]。

BEiT 对每个 patch 给一个离散的标号，更像 BERT 一点，但是在 MAE 中，直接还原的是原始像素信息。

### 2.4 自监督学习

自监督学习方法已经在计算机视觉中展现出了巨大的潜力，通常集中在训练前的不同的网络前置（pretext）任务上 [15,50,35,59,38,17]。最近，对比学习 [3,21] 很流行（如 [51,36,22,7]），它模拟两个或多个视图之间的图像相似度和不相似度 （或仅相似度 [20,8])。对比学习和相关方法强烈依赖于数据扩充 [7,20,8]。自编码在概念上追求不同的方向，正如本文将要展示的那样，它表现出不同的行为。

## 三、方法

本文的 masked 自编码器 (MAE) 是一种简单的自编码方法，它可以在给定部分观测值的情况下重建原始信号。与所有自动编码器一样，本文的方法有一个编码器，它将观察到的信号映射到潜在表示；以及一个解码器，它从潜在表示重建原始信号。与传统的自编码器不同，本文采用非对称设计，允许编码器只对观察到的部分信号 （没有掩码标记） 进行操作，并采用轻量级解码器从潜在表示和 mask tokens 中重建完整的信号。

### 3.1 Masking

和 ViT 一样，本文将图像划分为常规的不重叠的 patches。然后对一个子集的 patch 采样，并 mask 其余 patch。采样策略很简单：随机采样，不进行替换，遵循均匀分布，简单地称之为 “随机采样”。

随机采样高 masking 率 （即删除 patch 的比例） 在很大程度上消除冗余，从而创建一个不容易由可见邻近 patch 插值解决的任务 （见图 2-4)。均匀分布防止潜在的中心偏差 （例如， mask 图像中心附近）。这样，高度稀疏的输入为设计高效的编码器创造了机会。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-09-35-50.png" alt="MAE-2022-03-04-09-35-50" style="zoom:50%;" /></div>

<div align=center><img src="/Assets/Images/MAE-2022-03-04-09-36-14.png" alt="MAE-2022-03-04-09-36-14" style="zoom:50%;" /></div>

<div align=center><img src="/Assets/Images/MAE-2022-03-04-09-36-31.png" alt="MAE-2022-03-04-09-36-31" style="zoom:50%;" /></div>

### 3.2 MAE 编码器

MAE 编码器是一个 ViT[16]，但只作用于可见的、未 mask 的 patch。

就像在标准 ViT 中一样，MAE 编码器通过线性映射，嵌入 patch 和 位置 embedding，然后通过一系列 Transformer block 处理结果集。但是，MAE 编码器只在整个集合的一个小子集上运行 （即没有被 masked 的部分，本文通常为 25%)。被 masked 的 patch 直接删除；mask tokens 都不使用。这使得网络只用一小部分的计算和内存就能够训练非常大的编码器，完整的集合由轻量级解码器处理。

### 3.3 MAE 解码器

MAE 解码器的输入是由 (i) 编码的未被 masked 的 patches 和 (ii) mask tokens 组成的全套 tokens。参见图 1。

每个 mask tokens 都是一个共享的、学习到的向量，它指示需要预测的缺失 patches（**每一个被盖住的块都表示成一个相同的向量，这个向量的值可以通过学习得到**）。

向这个完整集合中的所有标记添加位置信息 embeddings；如果没有 embedding，mask tokens 就无法得到关于其在图像中的位置的信息。解码器由另一系列的 Transformer blocks 组成。

MAE 解码器只在预训练期间用于执行图像重建任务 （只有编码器生成图像表示进行识别），因此可以独立于编码器灵活地设计解码器架构。使用非常小的解码器进行实验，比编码器更窄、更浅。例如，默认解码器与编码器相比，每个 token 的计算量 <10%。使用这种非对称设计，整套 token 只由轻量级解码器处理，这大大减少了预训练的时间。

### 3.4 重建目标

MAE 通过预测每个 mask 块的像素值来重建输入。解码器输出中的每个元素都是一个像素值向量，代表一个 patch。解码器的最后一层是一个线性投影，其输出通道的数量等于一个 patch 中的像素值的数量。解码器的输出经过 reshape 构成重建图像，本文的损失函数计算重建图像和原始图像在像素空间中的均方误差 (MSE)。和 BERT 一样，本文只在 mask token 上计算损失。

本文还设计了一个变体，其重建目标是每个 mask patch 的归一化像素值。在本文的实验中，使用归一化像素作为重建目标可以提高再现质量。

### 3.5 简单实现

MAE 预训练可以有效地进行，而且重要的是，不需要任何专门的稀疏操作。

首先，为每个输入 patch 生成一个 token （通过线性映射并添加 position embedding )。然后，随机打乱 token 列表，并根据 mask 比率删除列表的后面部分。这个过程，编码器只产生一个小的 token 子集，大小为没有被 masked 的采样 patch。在编码之后，将 mask token 的列表添加到已编码 patch 列表中，并对整个列表进行 unshuffle （与随机打乱操作相反），以使所有 token 与其目标对齐。然后使用解码器解码这个完整的列表 （添加了 position embedding)。

如前所述，不需要进行稀疏操作。因为 shuffle 和 reshuffle 操作非常快，因此引入的开销是可以忽略不计的。

### 总结

1. **Masking**，要用 random masking mask 掉很大一部分比例的 patches。降低冗余，避免 center bias；
2. **MAE encoder**，是一堆 Transoformer Blocks，只 encode 了 unmasked patches 和 position embeddings；
3. **MAE decoder**，也是一堆 Transoformer Blocks，其输入包含了上一步 encoded 的 visible patches 和所有的 mask 掉的 tokens 两部分，另外，再加上 position embeddings 来让每一个 tokens 知道他们的位置应该在哪里；每一个 mask token 是一个共享的，可学习的 vector；
4. **Reconstruction target**，只是用 MSE Loss 重建了 mask 掉的 patches；这是为了证明方法的有效性，要是想提高未 mask 掉区域的重建效果的话，也可以归一化像素作为重建目标。

## 四、ImageNet Experiments

在 ImageNet-1K (IN1K) 训练集上进行自监督的预训练，然后进行有监督训练，评估表示通过两种方式实现：

1. 端到端微调，允许改整个模型所有可学习的参数；
2. linear probing 线性探测，只允许改最后一层的线性输出层。

实验记录 224×224 中心剪裁的最高验证精度。详情见附录 A.1。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-15-50-56.png" alt="MAE-2022-03-04-15-50-56" style="zoom:50%;" /></div>

#### baseline: ViT-Large

使用 ViT-large (ViT-L/16)[16] 作为消融研究的 backbone。ViT-L 非常大 （比 ResNet-50 大一个数量级），并且倾向于过拟合。下表为从头开始训练与 MAE 微调的性能对比。

| , original [16]| scratch, our impl.| baseline MAE |
|---|---|---|
|76.5 | 82.5 | 84.9| 

注意到，从头开始训练有监督的 ViT-L 是很重要的，并且需要一个具有强正则化的好方法 (82.5%，参见附录 A.2 （请参考原文）)。即便如此，MAE 预训练还是有很大的进步。这里的微调只适用于 50 个 epoch （vs. 200 from scratch），这意味着微调的准确性很大程度上依赖于预训练。

### 4.1 Main Properties

使用表 1 中的默认设置来对 MAE 进行消融实验。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-16-07-06.png" alt="MAE-2022-03-04-16-07-06" style="zoom:50%;" /></div>

#### Masking ratio

图 5 展示了 Masking 比率的影响，最佳比率惊人得高，比率为 75% 时对于线性 probing 和微调效果都很好。这一行为与 BERT 形成对比，其典型 masking ratio 为 1 5%。本文的 masking ratio 也远高于计算机视觉相关研究 (20%-50%)。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-16-08-36.png" alt="MAE-2022-03-04-16-08-36" style="zoom:50%;" /></div>

模型推理缺失的 patch，以产生不同的、但貌似合理的输出 （图 4)。它感知物体和场景的格式塔 （gestalt），这不能简单地通过扩展线条或纹理来完成。我们假设这种类似推理的行为与有用表征的学习有关。

图 5 还显示了线性 probing 和微调结果遵循不同的趋势。对于线性 probing，精度随着 masking ratio 的增加而稳步提高，直到最高点（sweet point）: 精度差距高达 ~20% (54.6% vs. 73.5%)。对于微调，结果对比率的敏感性较低，而且较大范围的 masking ratio (40-80%) 效果都很好。图 5 中的所有微调结果都比从头开始训练要好 (82.5%)。

#### 解码器的设计

MAE 解码可以灵活设计，如表 1a 和 1b 所示，从深度和宽度上进行消融。

表 1a 变化解码器的深度（Transformer 的数量），足够深的解码器对于线性 probing 很重要。这可以用像素重建任务和识别任务之间的差距来解释：自编码器的最后几层更专门用于重建，但与识别不太相关。合理深度的解码器可以解释重建的特殊性（reconstruction specialization），将潜在表示留在更抽象的层次。这种设计可以在线性 probing 方面提高 8% （表 1a，'lin')。然而，如果使用微调 （表 1a，'ft')，编码器的最后一层可以调整以适应识别任务。解码器深度对改善微调的影响较小。

有趣的是，单块解码器的 MAE 可以通过微调实现强大的性能 (84.8%)。单个 Transformer 块是将信息从可见 token 传播到 mask token 的最低要求。这种小型解码器可以进一步加快训练速度。

表 1b 研究了解码器的宽度 （通道数）。默认使用 512-d，它在微调和线性 probing 下表现良好。更窄的解码器也可以很好地进行微调。

总的来说，默认的 MAE 解码器是轻量级的。它有 8 个块，512-d 的宽度 （表 1 中的灰色部分）。相对于 ViT-L (24 个区块，1024-d)，每个 token 只有 9% 的 FLOPs。因此，虽然解码器处理所有 token，但它仍然是整个计算的一小部分。

#### Mask token

MAE 的一个重要设计是在编码器中跳过 mask token [M]，然后仅在轻量级解码器中解码，如表 1c 所示。

如果编码器使用 mask token，它的性能会变差：在线性 probing 中它的精度会下降 14%。这样，预训练和部署之间会存在一个间隙：该编码器在预训练输入中有很大一部分 mask token，这在未处理的图像中是不存在的。这种差距可能会降低部署的准确性。通过从编码器中去除 mask，限制编码器总是看到真实的 patch，从而提高精度。

此外，通过在编码器中跳过 mask token，大大减少了训练计算量。在表 1c 中，将整体训练 FLOPs 减少 3.3 倍。这在 MAE 的实现中会有 2.8 倍的 wall-clock 加速 （见表 2)。采用较小的解码器 (1 块），或较大的编码器 (ViT- H)，或两者联合，wall-clock 的加速会更大 (3.5-4.1 倍）。

注意，当 masking ratio 为 75% 时，加速可以为 >4 倍，部分原因是自注意复杂度是二次方的。此外，内存大大减少，可以训练更大的模型，或通过大批量的训练提高速度。时间和内存效率使 MAE 有利于训练非常大的模型。

#### 重建目标

在表 1d 中比较了不同的重建目标。到目前为止，本文的结果是基于没有 （每个 patch) 归一化的像素。使用带有归一化的像素可以提高精度，这种每个 patch 的归一化增强了局部对比度。

在另一种变体中，我们在 patch 空间中执行主成分分析，并使用最大的主成分系数 （这里是 96) 作为目标。这样做会降低准确性。

两个实验都表明高频分量在本文的方法中是有用的。

本文还比较了预测 token 的 MAE 变体，即 BEiT 中使用的 target。对于这个变体，本文使用 DALLE 预先训练的 dVAE 作为 tokenizer 标注器。在这里，MAE 解码器利用交叉熵损失预测 token 指数。与非归一化像素相比，这种标记化提高了 0.4%的微调精度，但与归一化像素相比没有优势。它还降低了线性 probing 精度。在 §5 中，进一步证明了 token 化（tokenization）在迁移学习中是不必要的。

本文采用基于像素的 MAE 比 token 化简单得多。dVAE tokenizer 还需要一个预训练阶段，可能需要额外的数据 (250M 图像 [43])。dVAE 编码器是一个大型的卷积网络 (40% 的 ViT-L FLOPs)，增加了不小的开销。使用像素的方法不会增加 FLOPs。

#### 数据增强

表 1e 研究了数据增强对 MAE 预训练的影响。

MAE 只使用裁剪的数据增强方法时表现很好，无论是固定大小或随机大小 （同时也使用了随机水平翻转）。添加颜色抖动会降低结果，所以不在其他实验中使用它。

令人惊讶的是，MAE 在没有使用数据增强 （只有中心裁剪，没有翻转） 时表现也很好。这一特性与对比学习及其相关方法有很大的不同，后者严重依赖于数据增强。[20] 观察到，仅使用裁剪数据增强法在 BYOL [20] 和 SimCLR [7] 的准确率分别降低了 13% 和 28%。此外，没有证据表明对比学习可以在没有增强的情况下工作：图像的两个视图是相同的，很容易满足于一个简单的解决方案。

在 MAE 中，数据增强的作用主要是通过随机 mask （然后消融） 来实现的。每个迭代的 mask 是不同的，因此无论数据增强与否，它们都会生成新的训练样本。masking 使 pretext 任务变得困难，因此只需要较少的增强来规范训练。

#### mask 采样策略

在表 1f 中比较了不同的 mask 采样策略，如图 6 所示。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-19-55-15.png" alt="MAE-2022-03-04-19-55-15" style="zoom:50%;" /></div>

1. [2] 中提出的 block-wise 分块掩蔽策略倾向于去除较大的块 （图 6 中间）。在 50% 的比例下，MAE 使用 block-wise mask 效果不错，但在 75% 的比例下就会退化。这种策略比随机采样策略更难，其训练损失更高，图像重建效果也更加模糊。
2. 网格采样的策略，即如图 6 右所示。这是一个更简单的任务，训练损失更低，图像重建的锐化较好。但是，表示 (representation) 质量较低。
3. 简单的随机采样对 MAE 来说是最好的。这种策略可以使用更高的 mask ratio，这样会有更大的加速效益，同时也享有良好的准确性。

#### 训练方法

到目前为止，本文的消融是基于 800 个 epoch 的预训练。图 7 展示了训练时间长度的影响，准确率随着训练时间而稳步提高。事实上，即使在 1600 epoch，也没有观察到线性 probing 精度的饱和。这不同于对比学习方法，在 ViT-L 上，用 MoCo v3 方法在第 300 epoch 时出现饱和。值得注意，MAE 编码器每个 epoch 只看到 25% 的 patch，而在对比学习中，编码器每个 epoch 看到 200% (two-crop) 甚至更多 (multi-crop) patch。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-20-00-13.png" alt="MAE-2022-03-04-20-00-13" style="zoom:50%;" /></div>

### 4.2 Comparisons with Previous Results

#### 自监督方法对比

表 3 比较了自监督 ViT 模型的微调结果。对于 ViT-B，所有方法的效果都很好。对于 ViT-L，方法之间的差距比较大，这表明更大的模型面临的挑战是减少过拟合。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-20-28-52.png" alt="MAE-2022-03-04-20-28-52" style="zoom:50%;" /></div>

MAE 可以很容易地扩大规模，并在更大的模型上，显示出稳定的改善。在 ViT-H(224 size) 上，MAE 获得 86.9% 的准确率。通过微调 448 size，MAE 达到 87.8% 的精度，仅使用 IN1K 数据。

在所有仅使用 IN1K 数据的方法中，最佳精度为 87.1% (512 size) [56]。在极具竞争力的基准 IN1K（无外部数据）中，MAE 比最先进的技术进步了不小的幅度。本文的结果是基于普通的 ViT，预计在更先进的网络上，将表现得更好。

与 BEiT[2] 相比，MAE 在更简单、更快的同时也更准确。本文的方法是重建像素，与 BEiT 预测 token 相比：当使用 ViT-B 重建像素时，BEiT 降低 1.8%。本文的方法不需要 dVAE 预训练。

此外，MAE 比 BEiT 要快得多 (3.5×每 epoch)，其原因如表 1c 所示。

为了提高精度，表 3 中的 MAE 模型进行了 1600 个 epoch 的预训练 （图 7)。即便如此，如果在相同的硬件上训练，本文的总预训练时间比所有其他方法都要少。例如，使用相同的 128 个 TPU-v3 内核，使用 ViT-L，MAE 的训练时间为 1600 个 epoch ，31 小时，而 MoCo v3 的训练时间为 300 个 epoch，36 小时。

#### 自监督预训练对比

在原始的 ViT 论文中，当使用 IN1K 训练时，ViT-L 开始退化，如图 8 所示。本文改进的监督方法可以更好地从零开始进行训练 （图 8，“our impl”; 见 A.2)，但准确性是饱和的。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-20-34-33.png" alt="MAE-2022-03-04-20-34-33" style="zoom:50%;" /></div>

MAE 预训练，仅使用 IN1K，可以更好地泛化：对于更高容量的模型，从零开始训练的增益更大。它遵循的趋势类似于 JFT-300M 监督预训 [16]。这一比较表明， MAE 可以帮助扩大模型尺寸。

### 4.3 Partial Fine-tuning

线性 probing 在过去几年已经成为一种流行的方法；然而，它错过了追求强大但非线性特征的机会，而这正是深度学习的优势。

本文提出了一个部分微调方法：微调最后几个层，同时冻结其他层。在早期的工作中也使用了该方案，如 [54,59,35]。

图 9 展示了结果。值得注意的是，只需微调一个 Transformer 块，就能将精度从 73.5% 显著提高到 81.0%。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-20-37-25.png" alt="MAE-2022-03-04-20-37-25" style="zoom:50%;" /></div>

此外，如果只微调最后一个块的 “一半” （即 MLP 子块），可以得到 79.1%，比线性 probing 好得多。这个变体本质上是对一个 MLP 头进行微调。微调几个块 （例如，4 或 6 个） 可以获得不错的精度。

在图 9 中，与 MoCo v3 [9] 进行了比较，这是一种与可用的 ViT-L 结果进行对比的方法。它具有比 MAE 更高的线性 probing 精度。然而，它所有的部分微调结果都比 MAE 的差。在调优 4 个块时，差距为 2.6%。

这些结果表明，这些结果表明，MAE 表示的线性可分离性较差，但它们具有较强的非线性特征，并在非线性头调谐时表现得比较好。

这些观察结果表明，线性可分性并不是评价表现质量的唯一度量标准。另外，线性 probing 与迁移学习性能没有很好的关联，如在目标检测中。据我们所知，线性评价在 NLP 中并不常用来进行 token 预训练。

## 五、Transfer Learning Experiments

本文在 COCO[32] 和 ADE20K[60] 上评估 MAE 迁移学习在目标检测和分割中的作用。使用表 3 中的预训练模型。

#### 目标检测和分割

本文在 COCO 数据集上对 mask R-CNN[23] 进行端到端微调。ViT backbone 网络适用于 FPN[31] （见附录 A.3)。将此目标检测系统应用于表 4 中的所有条目。box AP 用于目标检测，mask AP 用于实例分割。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-20-42-52.png" alt="MAE-2022-03-04-20-42-52" style="zoom:50%;" /></div>

与预训练有监督相比，MAE 在所有配置下表现更好 （表 4)。ViT-B 较小时，MAE 比预训练有监督高 2.4 点 (50.3 vs. 47.9, APbox)。更重要的是，随着 ViT-L 的增大，MAE 训练前的表现比预训练有监督高出 4.0 点 (53.3 vs 49.3)。

基于像素的 MAE 优于与基于 token 的 BEiT，而 MAE 更简单、更快。MAE 和 BEiT 都优于 MoCo v3, MoCo v3 与有监督的预训练相当。

#### 语义分割

在 ADE20K 上使用了 [2] 中 UperNet [52] 进行实验。详情见 A.4。从表 5 可以看出，MAE 显著提高了 ViT-L 的迁移结果，比有监督的预训练对照提高了 3.7 分 (53.6 vs 49.9)。基于像素的 MAE 优于基于 token 的 BEiT。这些观察结果与 COCO 的一致。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-21-10-07.png" alt="MAE-2022-03-04-21-10-07" style="zoom:50%;" /></div>

#### 像素 vs. token

表 6 给出了作为 MAE 重建目标的像素和 token 的全面比较。虽然使用 dVAE tokens 比使用非归一化像素要好，但它在统计上类似于本研究的所有任务和模型中只使用归一化像素。它再次表明 tokenization 对 MAE 来说是不必要的。

<div align=center><img src="/Assets/Images/MAE-2022-03-04-21-11-22.png" alt="MAE-2022-03-04-21-11-22" style="zoom:50%;" /></div>

## 六、总结

具有良好扩展性的简单算法是深度学习的核心。在 NLP 中，简单的自监督学习方法已经从指数尺度模型中获得了很好的效果。在计算机视觉中，尽管自我监督学习方面取得了进展，但实际上预训练仍然是有监督的（例如 [28,44,24,16]），还是主要靠有标号的数据作为训练数据。

在本文中，我们在 ImageNet 和迁移学习中观察到，自动编码器作为一种简单的自我监督方法，可以媲美有标号的有监督学习效果。视觉中的自监督学习现在可能正走上与 NLP 类似的轨道。

另一方面，我们也注意到，图像和语言是不同性质的信号，存在着很多区别。MAE 可以看作是 BERT 在计算机视觉上的一个扩展，对于 NLP 来说，一个词是一个语义的单元，含有的语义信息比较多；在图片中，虽然一个 patch 中含有一定的语义信息，但一个 patch 并不含有一个特定的物体，可能是含有多个物体的一小块，或是一个物体重叠的一块，即使是这样，MAE 也能学习到一个隐藏的比较好的语义表达。

**本文不是试图删除对象，而是重新删除最有可能不会形成语义片段的随机 patch**。MAE 重建像素，而像素不是语义实体。然而，MAE 依然能够推断出复杂的整体重建，这表明它已经学习了许多视觉信息，即语义。

**假设这种行为是通过 MAE 中丰富的隐藏表现方式发生的。**
