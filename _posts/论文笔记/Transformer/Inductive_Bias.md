# Inductive Bias

归纳偏置

在机器学习中，很多学习算法经常会对学习的问题做一些关于目标函数的必要假设，称为 归纳偏置 (Inductive Bias)。

归纳 (Induction) 是自然科学中常用的两大方法之一 （归纳与演绎，Induction & Deduction)，指从一些例子中寻找共性、泛化，形成一个较通用的规则的过程。偏置 (Bias) 则是指对模型的偏好。

- 通俗理解：归纳偏置可以理解为，从现实生活中观察到的现象中归纳出一定的规则 (heuristics)，然后对模型做一定的约束，从而可以起到 “模型选择” 的作用，类似贝叶斯学习中的 “先验”。
- 西瓜书解释：机器学习算法在学习过程中对某种类型假设的偏好，称为归纳偏好，简称偏好。归纳偏好可以看作学习算法自身在一个庞大的假设空间中对假设进行选择的启发式或 “价值观”。
- 维基百科解释：如果学习器需要去预测 “其未遇到过的输入” 的结果时，则需要一些假设来帮助它做出选择。
- 广义解释：归纳偏置会促使学习算法优先考虑具有某些属性的解。

例如，深度神经网络就偏好性地认为，**层次化处理信息有更好效果**；卷积神经网络认为**信息具有空间局部性 (Locality)，可用滑动卷积共享权重的方式降低参数空间**；循环神经网络则**将时序信息考虑进来，强调顺序重要性**；图网络则认为**中心节点与邻居节点的相似性会更好引导信息流动**。

事实上，将 Inductive Bias 翻译成归纳性偏好可能更符合我们的理解和认知。

- “奥卡姆剃刀” 原理：希望相同性能下，学习到的模型复杂度更低
- KNN 中假设特征空间中相邻的样本倾向于属于同一类
- SVM 中假设好的分类器应该最大化类别边界距离

## 机器学习中常见的归纳偏置

|算法|归纳偏置|
|---|---|
|最近邻|假设在特征空间 (feature space）中一小区域内大部分的样本是同属一类。给一个末知关别的样本，猜测它与它最紧接的大部分邻居是同属类。这就是用于最近邻法的偏置。这个假设是相近的样本应倾向同属于一类别。KNN 就是基于这种思想
|最少特征数|除非有充分的证据显示一个特征是有效用的，否则它应当被删除。这是特征选择算法背后所使用的假设。|
|最大边界|当要在两个类别间画一道分界线时，试图去最大化边界的宽度。这是用于支持向量机的偏置。|
|最小描述长度|当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。奥卡姆剃刀的理论基础|
|最大条件独立性|如果假说能转成贝叶斯模型架构，则试着使用最大化条件独立性。这是用于朴素贝叶斯分类器的偏置。|
|最小交叉验证误差|当试图在假说中做选择时，挑选那个具有最低交叉验证误差的假说。|

## CNN Inductive Bias

CNN 的 Inductive Bias 是**局部性 (Locality)** 和**空间不变性 (Spatial Invariance)/平移等效性 (Translation Equivariance)**，即空间位置上的元素 (Grid Elements) 的联系/相关性近大远小，以及空间平移的不变性 (Kernel 权重共享）。

- Locality: CNN用滑动窗口在图片上做卷积。假设是图片相邻的区域有相似的特征。i.e., 桌椅在一起的概率大，距离近的物品 相关性越强。
- Translation Equaivariance：$f(g(x)) = g(f(x))$  ，卷积 $f$ 和平移 $g$ 函数的顺序不影响结果。

CNN 的卷积核像一个 template 模板，同样的物体无论移动到哪里，遇到了相同的卷积核，它的输出一致。

CNN 有 locality 和 translation equivariance 归纳偏置，即 CNN 有很多先验信息，所以只需要较少的数据就可以学好一个模型。

## RNN Inductive Bias

RNN 的 Inductive Bias 是**序列性 (Sequentiality)** 和**时间不变性 (Time Invariance)**，即序列顺序上的时间步 (Timesteps) 有联系，以及时间变换的不变性 (RNN 权重共享）。

## Attention Inductive Bias

注意力机制，也是基于从人的直觉、生活经验归纳得到的规则。

归纳偏置的意义或作用是使得学习器具有了泛化的能力。加了一定正则的偏置的模型更为简单而通用 (模型复杂度受到惩罚而更低，恰当拟合数据点，泛化性能更好)。