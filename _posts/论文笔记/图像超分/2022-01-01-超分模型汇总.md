---
title: "图像超分模型汇总"
subtitle: "整理汇总图像超分模型"
layout: post
author: "L Hondong"
header-img: "img/post-bg-4.jpg"
tags:
  - 图像超分
---

# 超分模型汇总

## 超分辨率（Super-resolution, SR）
超分辨率是一项底层图像处理任务，将低分辨率的图像映射至高分辨率，以期达到增强图像细节的效果。图像模糊不清的原因有很多，比如各式噪声、有损压缩、降采样等等。

超分辨率是计算机视觉的一个经典应用。SR是指通过软件或硬件的方法，从观测到的低分辨率图像重建出相应的高分辨率图像（说白了就是提高分辨率），在监控设备、卫星图像遥感、数字高清、显微成像、视频编码通信、视频复原和医学影像等领域都有重要的应用价值。

超分分为以下两种：  
**Image SR** 只参考当前低分辨率图像，不依赖其他相关图像的超分辨率技术，称为单幅图像的超分辨率（single image super resolution，SISR）。

**Video SR** 参考多幅图像或多个视频帧的超分辨率技术，称之为多帧视频/多图的超分辨率（multi-frame super resolution）。对于Video SR，其核心思想就是用时间带宽换取空间分辨率。简单来讲，就是在无法得到一张超高分辨率的图像时，可以多取相邻几帧，然后将这一系列低分辨率的图像组成一张高分辨的图像。

一般来讲 VSR 相比于 SISR 具有更多的可参考信息，并具有更好的高分辨率视频图像的重建质量，但是其更高的计算复杂度也限制了其应用。

本文主要介绍 SISR 。SISR 是一个逆问题，对于一个低分辨率图像，可能存在许多不同的高分辨率图像与之对应，因此通常在求解高分辨率图像时会加一个先验信息进行规范化约束。在传统的方法中，这个先验信息可以通过若干成对出现的低-高分辨率图像的实例中学到。而基于深度学习的 SR 通过神经网络直接学习低分辨率图像到高分辨率图像的端到端的映射函数。

最早将深度学习用到 SISR 的是 Kaiming 大神等人在 ECCV2014 年提出的 SRCNN，那时并没有像 NTIRE 这样相关的比赛，只有少量开源数据集可以玩玩。至此，对 Kaiming 大神的敬仰又多了一分。

ECCV2018 有一篇关于小物体目标检测的论文让人印象深刻，迅速引起了大家的关注，这篇文章叫 SOD-MTGAN，文中巧妙利用了 SR 对小物体进行放大，然后得到更高的小物体检测准确率，因此 SR 被证明是有利于目标检测和跟踪的。

超分辨率是一个典型的非适定性(ill-posed)问题，LR到HR的映射具有高度不确定性，每一个LR输入都有大量的合理的HR与之对应，但是神经网络往往无法从这些合理HR集合中挑选出单一样本，而只能输出整个集合的统计期望（优化L2目标函数近似得到平均值），因此导致输出图像锐度消失。采用生成对抗网络(GAN)训练框架，有助于解决纹理缺失的问题。GAN是一个非常庞杂的问题，在超分辨率领域，主要依靠增加的对抗损失函数来指导生成网络训练，在经过仔细调参后，使生成网络输出接近训练集HR空间分布的图像，填充缺失的细节纹理。

| Model | Published | Code | Keywords | Pretrained |
| --- | --- | --- | --- | --- |
|SRCNN|[ECCV14](https://arxiv.org/abs/1501.00092)|[Keras](https://github.com/qobilidop/srcnn) |Kaiming|[Model](https://github.com/LoSealL/Model/releases)|
|RAISR|[arXiv](https://arxiv.org/abs/1606.01299)|-|Google, Pixel 3|
|**ESPCN**|[CVPR16](https://arxiv.org/abs/1609.05158)|[Keras](https://github.com/qobilidop/srcnn)|Real time/SISR/**VideoSR**|[Model](https://github.com/LoSealL/Model/releases)|
|**VDSR**|[CVPR16](https://arxiv.org/abs/1511.04587)|-|Deep, Residual|[Model](https://drive.google.com/open?id=1hW5YDxXpmjO2IfAy8f29O7yf1M3fPIg1)|
|DRCN|[CVPR16](https://arxiv.org/abs/1511.04491)|-|Recurrent|
|DRRN|[CVPR17](http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf)|[Caffe](https://github.com/tyshiwo/DRRN_CVPR17), [PyTorch](https://github.com/jt827859032/DRRN-pytorch)|Recurrent|
|**LapSRN**|[CVPR17](http://vllab.ucmerced.edu/wlai24/LapSRN/)|[Matlab](https://github.com/phoenix104104/LapSRN)|Huber loss|
|IRCNN|[CVPR17](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Learning_Deep_CNN_CVPR_2017_paper.pdf)|[Matlab](https://github.com/cszn/IRCNN)|
|**EDSR**|[CVPR17](https://arxiv.org/abs/1707.02921)|[PyTorch](https://github.com/thstkdgus35/EDSR-PyTorch)|NTIRE17 Champion|[Model](https://github.com/LoSealL/Model/releases)|
|BTSRN|[CVPR17](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Fan_Balanced_Two-Stage_Residual_CVPR_2017_paper.pdf)|-|NTIRE17|
|SelNet|[CVPR17](https://ieeexplore.ieee.org/document/8014887)|-|NTIRE17|
|TLSR|[CVPR17](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Xu_Fast_and_Accurate_CVPR_2017_paper.pdf)|-|NTIRE17|
|**SRGAN**|[CVPR17](https://arxiv.org/abs/1609.04802)|[Tensorflow](https://github.com/tensorlayer/srgan)|1st proposed GAN|
|**VESPCN**|[CVPR17](https://arxiv.org/abs/1611.05250)|-|**VideoSR**| [Model](https://drive.google.com/open?id=19u4YpsyThxW5dv4fhpMj7c5gZeEDKthm)|
|MemNet|[ICCV17](https://arxiv.org/abs/1708.02209)|[Caffe](https://github.com/tyshiwo/MemNet)|
|SRDenseNet|[ICCV17](http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf)|[PyTorch](https://github.com/wxywhu/SRDenseNet-pytorch)|Dense|[Model](https://drive.google.com/open?id=1aXAfRqZieY6mTfZUnErG84-9NfkQSeDw)|
|SPMC|[ICCV17](https://arxiv.org/abs/1704.02738)|[Tensorflow](https://github.com/jiangsutx/SPMC_VideoSR)|**VideoSR**|
|EnhanceNet|[ICCV17](https://arxiv.org/abs/1612.07919)|[TensorFlow](https://github.com/msmsajjadi/EnhanceNet-Code)|Perceptual Loss|
|PRSR|[ICCV17](http://openaccess.thecvf.com/content_ICCV_2017/papers/Dahl_Pixel_Recursive_Super_ICCV_2017_paper.pdf)|[TensorFlow](https://github.com/nilboy/pixel-recursive-super-resolution)|an extension of PixelCNN|
|AffGAN|[ICLR17](https://arxiv.org/pdf/1610.04490.pdf)|-|
|MS-LapSRN|[TPAMI18](https://ieeexplore.ieee.org/document/8434354)|[Matlab](https://github.com/phoenix104104/LapSRN)|Fast LapSRN|
|DnCNN|[TIP17](http://ieeexplore.ieee.org/document/7839189/)|[Matlab](https://github.com/cszn/DnCNN)|Denoise|[Model](https://github.com/LoSealL/Model/releases)|
|DCSCN|[arXiv](https://arxiv.org/abs/1707.05425)|[Tensorflow](https://github.com/jiny2001/dcscn-super-resolution)|
|IDN|[CVPR18](https://arxiv.org/abs/1803.09454)|[Caffe](https://github.com/Zheng222/IDN-Caffe)|Fast|[Model](https://drive.google.com/open?id=1Fh3rtvrKKLAK27r518T1M_JET_LWZAFQ)|
|DSRN|[CVPR18](http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Image_Super-Resolution_via_CVPR_2018_paper.pdf)|[TensorFlow](https://github.com/WeiHan3/dsrn/tree/db21d57dfab57de3608f0372e749c6488b6b305d)|Dual state，Recurrent|
|RDN|[CVPR18](https://arxiv.org/abs/1802.08797)|[Torch](https://github.com/yulunzhang/RDN)|Deep, BI-BD-DN|
|SRMD|[CVPR18](https://arxiv.org/abs/1712.06116)|[Matlab](https://github.com/cszn/SRMD)|Denoise/Deblur/SR|[Model](https://drive.google.com/open?id=1ORKH05-aLSbQaWB4qQulIm2INoRufuD_)|
|xUnit|[CVPR18](http://openaccess.thecvf.com/content_cvpr_2018/papers/Kligvasser_xUnit_Learning_a_CVPR_2018_paper.pdf)|[PyTorch](https://github.com/kligvasser/xUnit)|Spatial Activation Function|
|DBPN|[CVPR18](https://arxiv.org/abs/1803.02735)|[PyTorch](https://github.com/alterzero/DBPN-Pytorch)|NTIRE18 Champion|[Model](https://drive.google.com/open?id=1ymtlOjhkGmad-od0zw7yTf17nWD4KMVi)|
|WDSR|[CVPR18](https://arxiv.org/abs/1808.08718)|[PyTorch](https://github.com/JiahuiYu/wdsr_ntire2018), [TensorFlow](https://github.com/ychfan/tf_estimator_barebone/blob/master/docs/super_resolution.md)|**NTIRE18 Champion**|
|ProSRN|[CVPR18](https://arxiv.org/abs/1804.02900)|[PyTorch](https://github.com/fperazzi/proSR)|
|ZSSR|[CVPR18](http://www.wisdom.weizmann.ac.il/~vision/zssr/)|[Tensorflow](https://github.com/assafshocher/ZSSR)|Zero-shot|
|FRVSR|[CVPR18](https://arxiv.org/abs/1801.04590)|[TXT](https://github.com/msmsajjadi/FRVSR)|**VideoSR**|[Model](https://github.com/LoSealL/Model/releases)|
|**DUF**|[CVPR18](http://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf)|[Tensorflow](https://github.com/yhjo09/VSR-DUF)|**VideoSR**|
|**TDAN**|[CVPR20](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf)|[PyTorch](https://github.com/YapengTian/TDAN-VSR-CVPR-2020)|**VideoSR**，Deformable Align|
|SFTGAN|[CVPR18](https://arxiv.org/abs/1804.02815)|[PyTorch](https://github.com/xinntao/SFTGAN)|
|**CARN**|[ECCV18](https://arxiv.org/abs/1803.08664)|[PyTorch](https://github.com/nmhkahn/CARN-pytorch)|Lightweight|[Model](https://github.com/LoSealL/Model/releases/carn)|
|RCAN|[ECCV18](https://arxiv.org/abs/1807.02758)|[PyTorch](https://github.com/yulunzhang/RCAN)|Deep, BI-BD-DN|
|MSRN|[ECCV18](http://openaccess.thecvf.com/content_ECCV_2018/papers/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.pdf)|[PyTorch](https://github.com/MIVRC/MSRN-PyTorch)|[Model](https://drive.google.com/open?id=1A0LoY3oB_VnArP3GzI1ILUNJbLAEjdtJ)|
|SRFeat|[ECCV18](http://openaccess.thecvf.com/content_ECCV_2018/papers/Seong-Jin_Park_SRFeat_Single_Image_ECCV_2018_paper.pdf)|[Tensorflow](https://github.com/HyeongseokSon1/SRFeat)|GAN|
|TSRN|[ECCV18](https://arxiv.org/pdf/1808.00043.pdf)|[PyTorch](https://github.com/waleedgondal/Texture-based-Super-Resolution-Network)|
|**ESRGAN**|[ECCV18](https://arxiv.org/abs/1809.00219)|[PyTorch](https://github.com/xinntao/ESRGAN)|**PRIM18 region 3 Champion**|[Model](https://github.com/LoSealL/Model/releases/download/esrgan/esrgan.zip)|
|**EPSR**|[ECCV18](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vasu_Analyzing_Perception-Distortion_Tradeoff_using_Enhanced_Perceptual_Super-resolution_Network_ECCVW_2018_paper.pdf)|[PyTorch](https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw)|**PRIM18 region 1 Champion**|
|PESR|[ECCV18](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Perception-Enhanced_Image_Super-Resolution_via_Relativistic_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf)|[PyTorch](https://github.com/thangvubk/PESR)|ECCV18 workshop|
|FEQE|[ECCV18](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.pdf)|[Tensorflow](https://github.com/thangvubk/FEQE)|Fast|
|NLRN|[NIPS18](https://papers.nips.cc/paper/7439-non-local-recurrent-network-for-image-restoration.pdf)|[Tensorflow](https://github.com/Ding-Liu/NLRN)|Non-local, Recurrent|
|SRCliqueNet|[NIPS18](https://arxiv.org/abs/1809.04508)|-|Wavelet|
|FFDNet|[TIP18](https://ieeexplore.ieee.org/document/8365806/)|[Matlab](https://github.com/cszn/FFDNet)|Conditional denoise|
|CBDNet|[CVPR19](https://arxiv.org/abs/1807.04686)|[Matlab](https://github.com/GuoShi28/CBDNet)|Blind-denoise|
|SOFVSR|[ACCV18](http://arxiv.org/abs/1809.08573)|[PyTorch](https://github.com/LongguangWang/SOF-VSR)|**VideoSR**|[Model](https://github.com/LoSealL/Model/releases/download/sofvsr/SOFVSR_x4.zip)|
|TecoGAN|[arXiv](http://arxiv.org/abs/1811.09393)|[Tensorflow](https://github.com/thunil/TecoGAN)|**VideoSR**，GAN|[Model](https://github.com/LoSealL/Model/releases/download/tecogan/tecogan.zip)|
|MoreMNAS|[arXiv](https://arxiv.org/pdf/1901.01074.pdf)|-|Lightweight，NAS|
|**FALSR**|[arXiv](https://arxiv.org/pdf/1901.07261.pdf)|[TensorFlow](https://github.com/xiaomi-automl/FALSR)|Lightweight，NAS|
|RBPN|[CVPR19](https://arxiv.org/abs/1903.10128)|[PyTorch](https://github.com/alterzero/RBPN-PyTorch)|**VideoSR**|[Model](https://drive.google.com/open?id=1Ozp5j-DBWJSpXY5GvxiEPKdfCaAbOXqu)|
|DPSR|[CVPR19](https://arxiv.org/abs/1903.12529)|[PyTorch](https://github.com/cszn/DPSR)|
|DNI|[CVPR19](https://arxiv.org/pdf/1811.10515.pdf)|[PyTorch](https://github.com/xinntao/DNI)|
|OISR|[CVPR19](https://openaccess.thecvf.com/content_CVPR_2019/papers/He_ODE-Inspired_Network_Design_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf)|[PyTorch](https://github.com/HolmesShuan/OISR-PyTorch)|ODE-inspired Network|
|MAANet|[arXiv](https://arxiv.org/abs/1904.06252)|-|Multi-view Aware Attention|
|SRFBN|[CVPR19](https://arxiv.org/abs/1903.09814)|[PyTorch](https://github.com/Paper99/SRFBN_CVPR19)|Feedback|
|RNAN|[ICLR19](https://openreview.net/pdf?id=HkeGhoA5FX)|[PyTorch](https://github.com/yulunzhang/RNAN)|Residual Non-local Attention|
|FSTRN|[CVPR19](https://arxiv.org/pdf/1904.02870.pdf)|-|**VideoSR**，fast spatio-temporal residual block|
|SRNTT|[CVPR19](https://arxiv.org/abs/1903.00834)|[Tensorflow](https://github.com/ZZUTK/SRNTT)|Adobe|
|SAN|[CVPR19](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf)|[Matlab](https://github.com/daitao/SAN)|Second-order Attention, CVPR19 oral|
|**EDVR**|[CVPR19](https://arxiv.org/pdf/1905.02716.pdf)|[Pytorch](https://github.com/xinntao/EDVR)|**VideoSR, NTIRE19 video restoration and enhancement champions**|
|AdaFM|[CVPR19](https://arxiv.org/abs/1904.08118)|[Pytorch](https://github.com/hejingwenhejingwen/AdaFM)|SenseTime Oral|
|Ensemble for VSR|[CVPR19](https://arxiv.org/pdf/1905.02462.pdf)|-|**VideoSR**, NTIRE19 video SR 2nd place|
|TENet|[arXiv](https://arxiv.org/pdf/1905.02538.pdf)|[Pytorch](https://github.com/guochengqian/TENet)|A Joint Solution for Demosaicking, Denoising and Super-Resolution|
|MCAN|[arXiv](https://arxiv.org/pdf/1903.07949.pdf)|[Pytorch](https://github.com/macn3388/MCAN)|Matrix-in-matrix CAN, Lightweight|
|IKC&SFTMD|[CVPR19](https://arxiv.org/pdf/1904.03377.pdf)|-|Blind Super-Resolution|
