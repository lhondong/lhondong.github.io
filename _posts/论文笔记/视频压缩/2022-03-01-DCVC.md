---
title: "Deep Contextual Video Compression"
subtitle: "DCVC"
layout: post
author: "L Hondong"
header-img: "img/post-bg-46.jpg"
mathjax: true
tags:
  - 笔记
---

# DCVC

Deep Contextual Video Compression

[NIPS 2021]

## 摘要

过去的神经网络视频编码方法还是用的传统的思路，先生成预测帧，然后编码当前帧的残差。然而预测编码可能只是一个次优解，因为只使用了简单的减法去除冗余。

从预测编码到条件编码。

提出一个问题：

> How to define, use, and learn condition under a deep video compression framework.

设计了一种高效的条件编码框架，将时域（特征域 feature domain）上下文特征作为条件输入去帮助编解码器编码当前帧，从而充分挖掘条件编码的潜力。而这种设计也便于充分利用高维特征来帮助视频高频细节获得更好的重建质量。

与此同时，DCVC 是一个拓展性非常强的框架，其里面的上下文特征可以灵活设计。实验表明，在标准 1080p 视频上，所提出的 DCVC 相比 x265 (veryslow) 获得了 26.0% 的码率节省（PSNR 为指标）。在 DCVC 下，最新的方法相比 H.265-HM 有 14.4% 的码率节省（PSNR 为质量评价指标）。如果以 MS-SSIM 为质量评价指标，相比 H.266-VTM 则有 21.1% 的码率节省。

## 一、简介

### 1.1 Motivation

#### 残差编码

从 1988 年的 H.261 到 2020 年发布的 H.266，近 30 年来所有传统的视频编码标准都是基于残差编码的框架。在残差编码中，预测帧先会从之前已经解码的帧中生成出来，然后再计算当前帧与预测帧的残差。该残差会被编码变成码流，解码器将码流解码并获得重建后的残差，最后和预测帧相加获得解码帧。残差编码是一种简单高效的方式，但它的熵大于或等于条件编码的熵，并不是最优的方式。

因为给定预测帧 $\tilde{x}_t$，编码当前帧 $x_t$，使用手工相减的熵为 $H(x_t-\tilde{x}_t)$，而使用条件编码的熵为 $H(x_t \vert \tilde{x}_t)$，一般有：

$$H(x_t - \tilde{x}_t) \geq H(x_t \vert \tilde{x}_t)$$

另外从理论上讲，当前帧 $x_t$ 待编码的像素与之前的重建帧和当前帧 $x_t$ 已经重建的像素都可能有相关性。对于传统编码器，由于搜索空间巨大，使用人为制定的规则去显示地挖掘这些像素之间的相关性是一件非常困难的事情。因此残差编码假设当前像素只和预测帧对应位置的像素具有强相关性，这是条件编码的一个特例。

考虑到残差编码的简单性，最近的基于深度学习的视频压缩方法也大多采用残差编码，使用神经网络去替换传统编码器中的各个模块。

而本文认为，与其将视野局限在残差编码，更应该充分利用深度学习的优势去挖掘这些像素之间的相关性来设计条件编码，所以设计了一种基于条件编码的视频压缩方法，方法对比如图所示。

<div align=center><img src="/images/DCVC-2022-02-10-08-30-51.png" alt="DCVC-2022-02-10-08-30-51" style="zoom:50%;" /></div>

#### 条件编码

通常，在设计一个基于深度学习的条件编码框架时会遇到以下问题：“什么是条件？如何利用条件？如何学习条件？”

准确来讲，条件可以是任何能帮助当前帧编码的信息。一种简单直接的方法是把预测帧作为条件，虽然这样可行，但预测帧只包含 RGB 三个通道的像素信息，这会限制条件编码的潜力。既然已经采纳条件编码，为什么不可以让网络自动学习它所需要的条件？在 DCVC 里，网络在时域上学习生成上下文特征，该下文特征作为条件输入去帮助当前帧的编码和解码。

DCVC 实现框架如下图所示。至于如何学习上下文特征，首先设计了一个特征提取器将之前解码帧从像素域转换到特征域，同时利用运动估计去学习运动向量。该运动向量在经过编码和解码之后会指导网络从哪里提取特征。考虑到运动补偿引发的空间不连续性，本文又设计了一个上下文改进模块去生成最终的上下文特征。该上下文特征通过并以并联的方式作为编码器和解码器的条件输入。

### 1.2 Contributions

1. We design a deep contextual video compression framework based on conditional coding. The definition, usage, and learning manner of condition are all innovative. Our method can achieve higher compression ratio than previous residue coding-based methods.
2. We propose a simple yet efficient approach using context to help the encoding, decoding, as well as the entropy modeling. For entropy modeling, we design a model which utilizes spatial-temporal correlation for higher compression ratio or only utilizes temporal correlation for fast speed.
3. We define the conditionas the context in feature domain.The context with higher dimensions can provide richer information to help reconstruct the high frequency contents.
4. Our framework is extensible. There exists great potential in boosting compression ratio by better defining, using, and learning the condition

## 二、相关工作

### 2.1 Deep image compression 

### 2.2 Deep video compression 

## 三、方法

### 3.1 The framework of DCVC

传统的视频编码框架：

$$ \hat{x}_{t}=f_{d e c}\left(\left\lfloor f_{e n c}\left(x_{t}-\tilde{x}_{t}\right)\right\rceil\right)+\tilde{x}_{t} \text { with } \tilde{x}_{t}=f_{p r e d i c t}\left(\hat{x}_{t-1}\right) $$

其中 $\lfloor\cdot\rceil$ 表示量化操作，$f_{predict}$ 表示生成预测帧 $\tilde{x}_t$ 的函数，在传统编码框架里，一般是 MEMC 操作。大多数现有的基于 DL 的视频编解码器，$f_{predict}(\cdot)$ 是完全由神经网络组成的 MEMC。

DCVC 中，一开始最简单的想法是直接使用预测帧 $\tilde{x}_t$ 作为条件：

$$ \hat{x}_t = f_{dec}(\lfloor f_{enc}(x_t\vert \tilde{x}_t) \rceil \vert \tilde{x}_t) \text{ with } \tilde{x}_t = f_{predict}(\hat{x}_{t−1})$$

然而，这种情况仍然局限于低通道维度的像素域，限制了模型容量。既然使用了条件编码，为什么不让模型自己学习条件呢？因此，本文提出了一种上下文视频压缩框架，使用神经网络来生成上下文，而不是预测帧。框架表示为：

$$ \hat{x}_t = f_{dec}(\lfloor f_{enc}(x_t\vert \bar{x}_t) \rceil \vert \bar{x}_t) \text{ with } \bar{x}_t = f_{context}(\hat{x}_{t−1})$$

$f_{context}$ 表示生成上下文 $\bar{x}_t$ 的函数，$f_{enc}, f_{dec}$ 分别表示条件编码上下文的编码器和解码器，与传统的残差编码器解码器不同。

<div align=center><img src="/images/DCVC-2022-02-10-08-35-56.png" alt="DCVC-2022-02-10-08-35-56" style="zoom:50%;" /></div>

在 DCVC 框架中，时域高维上下文特征为编码条件。相比传统的像素域的预测帧，高维特征可以提供更丰富的时域信息，不同的通道也可以有很大的自由度去提取不同类型的信息，从而帮助当前帧高频细节获得更好的重建。下图展示了在像素域的残差编码和在特征域的条件编码的误差对比，在背景和前景中的高频内容可以获得明显的重建误差减小，这主要得益于使用更丰富的高维特征。

为了为编码 $x_t$ 提供更丰富和更相关的条件，上下文位于更高维度的特征域中。相比传统的像素域的预测帧，高维特征可以提供更丰富的时域信息，不同的通道也可以有很大的自由度去提取不同类型的信息，从而帮助当前帧高频细节获得更好的重建。

下图给出了一个分析示例，图中右上角显示了上下文中的四个通道示例。纵观这四个通道，会发现不同的通道有不同的侧重点。例如，与 $x_t$ 中的高频可视化相比，第三通道似乎更注重高频内容。相比之下，第二个和第四个通道更注重提取颜色信息，其中第二个通道侧重于绿色，而第四个通道强调红色。得益于这些不同的上下文特征，DCVC 可以获得更好的重建质量，特别是对于高频较多的复杂纹理。右下角图像展示了在像素域的残差编码和在特征域的条件编码的误差对比，DCVC 在背景和前景中的高频内容可以获得明显的重建误差减小，这主要得益于使用更丰富的高维特征。

<div align=center><img src="/images/DCVC-2022-02-10-08-51-47.png" alt="DCVC-2022-02-10-08-51-47" style="zoom:50%;" /></div>

当前帧的编码和解码都以上下文 $\bar{x}_t$ 为条件。通过上下文编码器，$x_t$ 被编码为 latent codes $y_t$，然后将 $y_t$ 量化为 $\hat{y}_t$。通过上下文解码器，最终获得重构帧 $\hat{x}_t$。

在 DCVC 中，使用网络自动挖掘当前帧 $x_t$ 和上下文特征 $\bar{x}_t$ 之间的相关性，然后消除冗余，而不是像残差编码中使用固定减法运算。

该设计方法还具有自适应地使用上下文的能力。由于视频中的运动，新的内容经常出现在对象边界区域。这些新内容可能无法在之前的解码帧中找到好的参考。在这种情况下，基于 DL 的残差编码的视频编解码器仍然强行对残差进行编码。对于视频中运动较大的区域或者新内容出现的区域，帧间相关性通常较低，残差可能非常大，并且通过减法运算进行的帧间编码可能比帧内编码更差。相比之下，条件编码框架能够自适应地利用条件。对于新的内容，这种设计使框架可以在帧内编码和帧间编码获得自适应转换，自适应地去挖掘空间相关性来编码。如图所示，条件编码方法对新内容的区域可以显著减小新内容的重建误差。

### 3.2 Entropy model

<div align=center><img src="/images/DCVC-2022-02-10-08-52-15.png" alt="DCVC-2022-02-10-08-52-15" style="zoom:50%;" /></div>

> The cross-entropy between the estimated probability distribution and the actual latent code distribution is a tight lower bound of the actual bitrate.

估计概率分布和实际 latent codes 分布之间的交叉熵是实际比特率的严格下限：

$$R(\hat{y}_t) \geq \mathbb E_{\hat{y}_t∼q_{\hat{y}_t}} [−log_2 p_{\hat{y}_t} (\hat{y}_t)]$$

$p_{\hat{y}_t} (\hat{y}_t)$ and $q_{\hat{y}_t} (\hat{y}_t)$ are estimated and true probability mass functions of quantized latent codes $\hat{y}_t$。

实际上，算术编码几乎可以以交叉熵的比特率对 latent codes 进行编码。实际比特率 $R(\hat{y}_t)$ 与交叉熵比特率之间的差距可以忽略不计。

因此，我们的目标就是设计一个熵模型，该模型能够准确地估计 latent codes $p_{\hat{y}_t} (\hat{y}_t)$ 的概率分布。熵模型的框架如图 4 所示。首先，我们使用超先验模型 [13] 学习分层先验，并使用自回归网络 [14] 学习空间先验。这两种先验（层次先验和空间先验）常用于深度图像压缩。然而，对于视频，latent codes 也具有时间相关性。因此，我们建议使用上下文 x̄t 来生成时间先验。如图 4 所示，我们设计了一个时间优先编码器来探索时间相关性。

在条件编码框架 DCVC 中，上下文特征除了作为编码器和解码器的输入，也同时被用作熵模型的输入。如图 5 的左图所示，除了超先验，还同时利用了空域和时域先验。空域先验由自回归模型学习，而时域先验则由时域先验编码器从上下文特征中学习，从而保证熵模型可以获得更准确的概率估计，减小算术编码器所花费的比特代价。需要指出的是，自回归模型是一种比较“昂贵”的模型，虽然它可以进一步去除空间上的冗余，但是它的概率估计和编码是交织在一起的，从而引入了空间依赖并难以并行化。因此，虽然自回归模型可以提升压缩效率，但也会显著降低编解码速度。相比之下，时域先验的相关操作没有这种依赖性，完全可以并行化去处理。在图 5 的右图中也提供了只利用超先验和时域先验的熵模型。得益于高维上下文特征作为输入，这种熵模型可以用较小的压缩效率代价去获得显著的编解码加速。

## 四、实验

### 4.1 

实验表明，提出的 DCVC 相比传统编码器 x265 (veryslow) 在三个 1080p 视频数据集分别获得 23.9%, 25.3%, 26.0%的码率节省。另外，通过和最近的基于深度学习的压缩方法 DVC 和 DVCPro 相比。DVC 和 DVCPro 采用和传统编码器一样的残差编码框架。如图 5 所示，相比 DVC 和 DVCPro，DCVC 有着非常显著的码率节省。

<div align=center><img src="/images/DCVC-2022-02-10-08-52-37.png" alt="DCVC-2022-02-10-08-52-37" style="zoom:50%;" /></div>

如果以 MS-SSIM 为质量评价指标，DCVC 方法甚至比 H.266-VTM 有高达21.1%的码率节省。与此同时，基于时域先验的熵模型也使该方法的编码速度在所有方法中是最快的，有着数量级的提升，解码速度和 H.265-HM/H.266-VTM 接近。

<div align=center><img src="/images/DCVC-2022-02-10-08-52-59.png" alt="DCVC-2022-02-10-08-52-59" style="zoom:50%;" /></div>

DCVC 具有很强的可拓展性，其高维上下文特征可以灵活设计，例如设计更好的学习或者利用方式。基于 DCVC 的条件编码框架，最新的方法在性能上有了进一步的提升，相关论文也将在之后公开。图 7 展示了最新方法的性能对比。值得注意的是，H.265-HM 和 H.266-VTM 都是使用性能最强的低延迟编码配置，并且 intra period 不是目前主流的 10 或者 12，而是更为实用的 32。如表 1 所示，如果以 PSNR 为质量评价指标，基于 DCVC 提升的方法相比 H.265-HM 可以获得高达 14.4%的码率节省。相比之下，其它基于深度学习的视频压缩方法离 H.265-HM 的压缩性能还相差甚远。

<div align=center><img src="/images/DCVC-2022-02-10-08-53-46.png" alt="DCVC-2022-02-10-08-53-46" style="zoom:50%;" /></div>

### 4.2 

## 五、总结

本文设计了一种基于条件编码的视频压缩框架 DCVC，该框架比常用的残差编码框架具有更低的信息熵下界。另外，对于运动复杂区域或者新内容区域而言，残差编码所假设的帧间预测总是最有效的并不正确。相比之下，提出的 DCVC 可以自适应地学习帧内编码和帧间编码。在 DCVC 里，其条件被定义为上下文特征。相比传统 RGB 三通道像素，具有更高维度的上下文特征，并可以携带更丰富的时域信息来帮助编码，特别是对高频细节的恢复。在未来，高分辨率视频将会更受欢迎和普及，而针对高分辨率视频中的更多高频细节内容，DCVC 的优势也将更为明显。

DCVC 是一种可拓展性非常强的框架，其潜力巨大，值得进一步研究，可以通过更好地学习和使用上下文特征去设计更有效的解决方案，并获得更好的压缩性能。
