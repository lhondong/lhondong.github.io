---
title: "FVC"
subtitle: "FVC: A New Framework towards Deep Video Compression in Feature Space"
layout: post
author: "L Hondong"
header-img: "img/post-bg-5.jpg"
mathjax: true
tags:
  - 笔记
---

# FVC

FVC: A New Framework towards Deep Video Compression in Feature Space

Zhihao Hu, Guo Lu, Dong Xu CVPR 2021 (Oral)

CVPR 2021 的一篇 Oral，提出了特征空间视频编码（FVC）网络，可以通过在特征空间中执行所有的主要操作（即运动估计、运动压缩、运动补偿和残差压缩）。

## 摘要

基于学习的视频压缩在过去的几年里吸引了越来越多的关注。以往的混合编码方法依赖于像素空间操作来减少时空冗余，这可能存在不准确的运动估计或效率较低的运动补偿。本文提出了一个特征空间视频编码网络 (FVC)，在特征空间中执行所有主要操作（即运动估计、运动压缩、运动补偿和残余压缩）。

在所提出的可变形补偿模块中，首先在特征空间中通过运动估计来产生运动信息（即偏移图），并使用自编码器网络进行压缩。然后利用可变形卷积进行运动补偿，并生成预测特征。然后压缩当前帧的特征和可变形补偿模块的预测特征之间的残差特征，为了更好地进行帧重建，还在多帧特征融合模块中使用非局部注意机制，融合了之前多个重构帧的参考特征。综合的实验结果表明，该框架在 HEVC、UVG、VTL 和 MCL-JCV 等四个基准数据集上达到了最先进的性能。

## 一、简介

### 1.1 Motivation

人们对开发下一代视频压缩技术的研究兴趣越来越大。虽然传统的视频压缩方法 [37，27] 通过使用手工设计的模块（如基于块的运动估计、离散余弦变换 (DCT)) 来减少空间和时间冗余而取得了良好的性能，但这些模块不能基于大规模视频数据以端到端的方式进行优化。

最近的深度视频压缩工作 [22，38，2，10，14，21，19] 通过在混合视频压缩框架中应用深度神经网络，已经取得了令人印象深刻的结果。目前，大多数的工作原理只依赖于像素级的操作（例如，运动估计或运动补偿）来减少冗余。例如，在 [22，21，14，23] 中使用像素级光流估计和运动补偿来减少时间冗余，并利用自编码器式网络进一步压缩像素级残差。然而，这种像素级的范例存在以下三个限制。
1. 首先，要产生精确的像素级光流信息，特别是对于具有复杂非刚性运动模式的视频。
2. 其次，即使可以提取足够精确的运动信息，像素级运动补偿过程也可能引入额外的伪影。
3. 第三，压缩像素级的残差信息也是一项非常重要的任务。

给定各种应用的鲁棒表示能力，希望在特征空间中进行运动补偿或残差压缩，更有效地减少视频中的空间或时间冗余。此外，可变形卷积（deformable convolution）[9，35] 的最新进展表明，在特征空间中对齐两个连续的帧是可行的。基于学习到的卷积核的偏移图（即所谓的动态核），可变形的卷积可以决定从输入特征中采样哪些位置。通过使用基于动态核的可变形卷积，可以更好地处理连续两帧之间更复杂的非刚性运动模式，从而提高运动补偿结果，并减轻后续残差压缩模块的负担。不幸的是，将特征空间操作和可变形卷积无缝地集成到现有的混合深度视频压缩框架中，并以端到端的方式训练整个网络仍然是一项非常重要的任务。

本文没有按照现有的工作 [22，14，3，19] 像传统的视频编解码器那样采用像素级操作，而是通过减少特征空间的时空冗余，提出了一种新的特征空间视频编码网络 (FVC)。

首先基于从两个连续帧中提取的表示来估计运动信息（即可变形卷积中卷积核的偏移映射）。然后利用自动编码器式网络对偏移图进行压缩，并在随后的可变形卷积操作中使用重构的偏移图（offset maps），生成预测特征，以获得更精确的运动补偿。然后采用另一种自动编码器式的网络来压缩原始输入特征与预测特征之间的残差特征。最后，提出了一种多帧特征融合模块，结合以往多帧的参考特征，实现更好的帧重构。

与最先进的基于学习的视频压缩方法 [21，14] 相比，在特征空间中执行所有操作，以获得更精确的运动估计和运动补偿，其中可以无缝地合并可变形卷积到混合深度视频压缩框架中。因此可以减轻运动估计/补偿等不准确的像素级操作所带来的误差，并获得更好的视频压缩性能。

### 1.2 Contributions

- 提出了一种新的基于学习的视频压缩框架，可以在特征空间中执行运动估计、运动补偿和残余压缩等所有操作。
- 为了在特征空间中进行有效的运动补偿，使用可变形卷积来“扭曲”先前重构帧的参考特征，并更准确地预测当前帧的特征，其中利用自动编码器样式网络压缩相应的偏移图。
- 提出了一种基于非局部注意机制的多帧特征融合模块，它结合了以往多个帧的特征，可以更好地重构当前帧。
- FVC 在包括 HEVC、UVG、VTL 和 MCL-JCV 等四个基准数据集上实现了最先进的性能，这证明其有效性。

## 二、相关工作

### 2.1 Image Compression

在过去的几十年里，传统的图像压缩方法，如 JPEG[33]，JPEG2000[28] 和 BPG[6]，已经被提出来减少空间冗余，其中利用手工制作的技术，如 DCT，以实现高压缩性能。近年来，基于学习的图像压缩方法引起了越来越多的关注。首先引入了基于 RNN 的图像压缩方法 [31，32，15] 来逐步压缩图像。其他方法 [4，5，25，29] 采用自动编码器结构，首先编码图像作为潜在表示，然后将特征空间的潜在表示解码到像素空间，达到了最先进的图像压缩性能。

### 2.2 Video Compression

在过去的几年里，已经提出了一些视频压缩标准 [37，27]。大多数方法都采用混合编码结构，其中采用运动补偿和残余编码技术来减少时空域的冗余。近年来，基于学习的视频压缩 [38，22，26，12，10，19，21，23，14，11，2，42，8，39，20，17，41] 已成为一个新的研究方向。遵循传统的混合视频压缩框架，Lu 等人。[22] 提出了第一个端到端优化的视频压缩框架，其中 H.264/H.265 中的所有关键组件都被深度神经网络所取代。Lin 等人 [19] 在不同的模块中使用多个帧来进一步删除冗余，而 Hu 等人 [14] 提出了一种分辨率自适应光流压缩方法，通过自动决定每一帧和每个块的最优分辨率。

大多数现有的方法，[22，19，14] 必须估计像素级光流图，并压缩相应的像素级残差信息。然而，可能难以产生可靠的像素级流图或残差信息，这往往会降低基于学习的视频编解码器的压缩性能。为了解决这个问题，[10，11] 最近的一些工作提出了计算视频压缩的特征级残差映射。与这些工作 [10，11] 相比，本文提出在特征空间中执行所有的操作，从而获得更好的视频压缩性能。

### 2.3 Deformable Convolution

最近，Dai 等人 [9] 提出利用可变形卷积和学习的偏移图来增强卷积神经网络的建模能力，在动作识别（action recognition）[18] 和视频超分辨率（video superresolution）[30，35] 等视频任务中取得了良好的效果。与这些工作相比，我们的工作是第一个研究如何在基于学习的视频压缩中使用可变形卷积的工作。考虑到在混合视频压缩系统中有多个复杂的组件，通过无缝地结合可变形的卷积和同时压缩相应的偏移图和其他特征来开发一个端到端优化的视频编解码器是一个非常重要的任务。

## 三、方法

### 3.1 Overview

<div align=center><img src="/images/FVC-2022-02-14-10-03-08.png" alt="FVC-2022-02-14-10-03-08" style="zoom:50%;" /></div>

$X = \{X_1, X_2, \cdots, X_{t−1}, X_t，\cdots \}$ 表示一个视频序列，其中 $X_t$ 是当前时间步长的原始视频帧。在本文视频压缩系统中，其目标是在任何给定的帧率下生成高质量的重构帧 $\hat{X}_t$。如图 1 所示，该框架中的所有模块，包括可变形补偿、残差压缩和多帧特征融合，都是在特征空间中进行。提出的框架的概述架构总结如下：

#### Feature Extraction

为了在特征空间中生成表示，原始的输入帧 $X_t$ 和先前的重构帧 $\hat{X}_{t-1}$ 分别被编码为特征表示 $F_t$ 和 $F^{ref}_{t-1}$。如图 2(a) 所示。特征提取模块使用步长为 2 的卷积层，然后再跟几个残差块（residual blocks）来生成特征表示。

#### Deformable Compensation

该过程包括三个步骤：运动估计、运动压缩和运动补偿。具体来说，基于 $F_t$ 和 $F^{ref}_{t-1}$，使用轻量级网络进行运动估计，在传输到解码器侧之前，将使用新提出的运动压缩网络来压缩输出偏移图 $O_t$。最后，给定重构的偏移图 $\hat{O}_t$ 和特征 $F^{ref}_{t-1}$，可以在运动补偿过程中使用可变形卷积来生成预测的特征 $\bar{F}_t$。详见第 3.2 节。

#### Residual Compression

利用自动编码式网络对输入特征 $F_t$ 与预测特征 $\bar{F}_t$ 之间的残差特征 $R_t$ 进行压缩，以实现更好的重构。在将重构的残差特征 $\hat{R}_t$ 添加到预测特征 $\bar{F}_t$ 之后，生成初始重构特征表示 $\tilde{F}_t$。

#### Multi-frame Feature Fusion

在该过程中，将前三个重构帧中提取的特征表示 $F^{ref}_{t-1}$， $F^{ref}_{t-2}$ 和 $F^{ref}_{t-3}$ 以及初始重构特征表示 $\tilde{F}_t$ 融合，生成最终重构特征 $\hat{F}_t$。详见第 3.3 节。

<div align=center><img src="/images/FVC-2022-02-14-10-08-15.png" alt="FVC-2022-02-14-10-08-15" style="zoom:50%;" /></div>

#### Frame Reconstruction

如图 2(b) 所示，具有几个残差块和反卷积层的特征解码器会将最终的重构特征 $\hat{F}_t$ 转换为重构帧 $\hat{X}_t$。

#### Entropy Coding

通过执行熵编码，将运动压缩和残差压缩模块的量化特征转换为比特流。在训练过程中，我们使用一个比特估计网络来估计比特数。详见第 3.4 节。

### 3.2 Deformable Compensation

以前的视频压缩框架 [22，10，14] 依赖于光流估计网络和像素级运动补偿网络，这可能导致帧预测结果不准确，从而给后续的残差压缩模块带来额外的冗余。为此，我们在特征空间中进行运动补偿，并使用可变形卷积操作生成预测特征。

针对分别来自先前重构帧和当前帧的特征 $F^{ref}_{t-1}$ 和 $F_t$，可变形补偿过程的目的是在当前时间步长生成预测特征 $\bar{F}_t$。整个网络架构如图 3 所示。具体地说，我们首先通过将 $F^{ref}_{t-1}$ 和 $F_t$ 输入一个 2 层运动估计网络来生成可变形的偏移图 $O_t$。受 [11] 的启发，我们提出了一个自动编码器风格的网络来压缩这个偏移图 $O_t$，其中编码器和解码器都由一组 Resblocks[13] 组成。通过将编码器转换到潜在空间，然后进行量化。之后，解码器将量化的潜在表示转换为重构的偏移图 $\hat{O}_t$。

最后，在运动补偿过程中，可变形卷积层以 $F^{ref}_{t-1}$ 为输入，然后借助 $\hat{O}_t$ 使用相应的滤波器进行可变形卷积操作。通过这种方法，我们可以使用学习到的动态偏移核（dynamic offset kernels）来更好地压缩具有复杂非刚性运动模式的视频，并产生更准确的预测特征。为了生成更准确的预测特征，我们利用两个卷积层进一步细化了可变形卷积层的输出特征，并最终生成了预测特征 $\bar{F}_t$。

<div align=center><img src="/images/FVC-2022-02-14-14-50-23.png" alt="FVC-2022-02-14-14-50-23" style="zoom:50%;" /></div>

#### Deformable Convolution

可变形卷积层 [9] 的网络结构如图 4 所示。给定参考特征图和相应的偏移图 $O$，我们的目标是生成预测的特征。对于每个核，使用相应的偏移量控制参考特征图中的采样位置，然后将参考特征图中相应位置的特征值融合在一起，以在预测的特征图中生成一个特征值。与以往视频压缩方法 [22] 中使用的基于运动补偿的像素级变形（warping）操作相比，我们的可变形补偿模块（deformable compensation module）通过使用不同的采样位置提供了更大的灵活性，可以更好地处理复杂的非刚性运动模式，提高补偿性能。在我们的实现中，我们将参考特征的信道划分为 G 组（本工作中的 G=8)，并对每组信道使用共享的偏移图，以更好地学习偏移图，并改进可变形补偿结果。

<div align=center><img src="/images/FVC-2022-02-14-14-50-58.png" alt="FVC-2022-02-14-14-50-58" style="zoom:50%;" /></div>

### 3.3 Multi-frame Feature Fusion

经过可变形补偿和残差压缩过程后，结合预测特征 $\bar{F}_t$ 和重构残差特征 $\hat{R}_t$，生成初始重构特征 $\tilde{F}_t$。如图 1 所示，为了生成更准确的最终重构特征 $\hat{F}_t$，提出了一个多帧特征融合模块来利用前多帧的特征表示 $F^{ref}_{t-1}, F^{ref}_{t-2}, F^{ref}_{t-3}$ 的时间上下文信息。

详细的网络体系结构如图 5(a) 所示。具体地说，我们首先使用共享特征空间可变形补偿模块生成预测特征表示 $\bar{F}^{ref}_{t-1}, \bar{F}^{ref}_{t-2}, \bar{F}^{ref}_{t-3}$。然后，基于初始重构特征 $\tilde{F}_t$ 与预测表示 $\bar{F}^{ref}_{t-1}, \bar{F}^{ref}_{t-2}, \bar{F}^{ref}_{t-3}$ 之间的相似性，利用非局部注意机制来细化预测特征表示。值得一提的是，也通过使用基于所谓的自注意机制（self-attention）的非局部注意机制来细化 $\tilde{F}_t$。最后，连接所有这些改进的特征表示，并使用另一个卷积层来生成最终重构的特征 $\hat{F}_t$。

可变形补偿模块的网络结构与第 3.2 节中的网络结构相同。对于非局部注意块，我们提供了详细的体系结构图 5(b)。这里以 $\bar{F}^{ref}_{t-1}$ 为例，描述在执行 $\tilde{F}_t$ 引导的注意力操作后，如何产生精化特征 $\hat{F}^{ref}_{t-1}$，其中我们假定这两个特征 $\hat{F}^{ref}_{t-1}$ 和 $\tilde{F}_t$ 的尺寸为 $w×h×c$。对于位置 $(i,j)$ 上 $\tilde{F}_t$ 的每个空间特征向量 $f_{i,j}$，其大小为 $1×1×c$，我们找到一个 $\bar{F}^{ref}_{t-1}$的 collocated patch $f^{ref}_{i,j}$，大小为 $p×p×c$。然后通过沿信道维数执行卷积操作来计算 $f_{i,j}$ 和 $f^{ref}_{i,j}$ 之间的相似性，然后在执行软 softmax 操作后生成相应的注意图，进一步用于重新计算 $f^{ref}_{i,j}$ 中的所有空间位置。之后，我们在该位置 $(i,j)$ 处生成细化的特征向量 $\hat{f}^{ref}_{i,j}$。我们对 $\tilde{F}_t$ 中的每个空间位置重复这个过程，然后生成细化的特征 $\hat{F}^{ref}_{t-1}$。最后，我们将所有细化的特征 $\hat{F}^{ref}_{t-3}, \hat{F}^{ref}_{t-2}, \hat{F}^{ref}_{t-1}$ 和 $\hat{F}^{ref}_{t}$ 连接起来，执行卷积操作，融合这些细化特征，通过添加初始重构特征 $\tilde{F}_t$，生成最终重构特征 $\hat{F}_t$。 

<div align=center><img src="/images/FVC-2022-02-14-14-54-35.png" alt="FVC-2022-02-14-14-54-35" style="zoom:50%;" /></div>

### 3.4 Residual Compression and Other Details

如图 1 所示，除了偏移图压缩外，还需要压缩残差特征图。为了简化整个系统，我们采用了与偏移特征图压缩相同的网络结构（见图 3(b))。

对于整个基于学习的视频压缩框架，使用比特率估计网络在训练阶段生成比特率。在 [25] 中使用超先验熵模型来进行精确的比特率估计。为了降低计算复杂度，在 FVC 中不使用耗时的自回归模型。

为了以端到端的方式优化整个模型，还需要设计一个可微的量化操作。在我们的方法中，遵循 [5] 中的方法，并通过在训练阶段添加均匀噪声来近似量化操作。在推理阶段，直接使用四舍五入的操作。

### 3.5 Loss Function

在提出的框架中，优化了以下速率失真 (RD) 权衡： 

$$ RD = R + \lambda D = R_o + R_r + \lambda d(X_t, \hat{X}_t)$$

其中， $R_o$ 和 $R_r$ 分别表示用于编码偏移图 $O_t$ 和残差特征图 $R_t$ 的位数。$d(X_t,\hat{X}_t)$ 表示输入帧 $X_t$ 与重构帧 $\hat{X}_t$ 之间的失真，其中 $d(·)$ 表示均方误差或 MS-SSIM[36]。 $\lambda$ 是一个用于控制速率-失真权衡的超参数。

## 四、实验

### 4.1 Experimental Setup

#### Training Dataset

在训练阶段使用 Vimeo-90K 数据集 [40]，其中包含 89,800 个视频片段，每个视频有 7 帧，分辨率为 448×256。在训练前将视频序列随机裁剪为 256×256 的分辨率。

#### Testing Datasets

为了评估 FVC 的性能，使用了四个数据集中视频序列，包括 HEVC[27]、UVG[24]、MCL-JCV[34] 和 VTL[1]。HEVC 数据集包含 16 个 B、C、D 和 E 类的视频，分辨率从 416×240 到 1920×1080 不等。UVG 数据集包含 7 个高帧率视频，分辨率为 1920×1080，其中相邻帧之间的差异很小。MCL-JCV 数据集被广泛用于视频质量评估，它由 30 个 1080p 的视频序列组成。对于 VTL 数据集，使用来自高分辨率视频序列的前 300 帧，分辨率为 352×288 来进行性能评估。

#### Evaluation Metrics

使用 bpp（每像素的比特）来测量用于每帧中一个像素的运动编码和残余编码的平均位数。我们使用 PSNR 和 MS-SSIM[36] 来评估重建帧和 ground-truth 之间的失真，每个视频序列的 PSNR/MS-SSIM 是通过计算所有重建帧的平均 PSNR/MS-SSIM 产生的。

#### Implementation Details

训练了四种具有不同 $\lambda$ 值的模型（$\lambda = 256，512，1024，2048$）。本文采用了一个两阶段的训练方案来训练我们的模型。

- 第一阶段，在不使用多帧特征融合模块的情况下，以 5e-5 的学习率训练模型 200 万 steps。
- 第二阶段，包括多帧特征融合模块，首先以 5e-5 的学习率训练整个框架 40 万 steps，然后学习率降低为 5e-6 并训练 10 万 steps 来优化模型。

当使用 MS-SSIM 进行性能评估时，通过使用 MS-SSIM 作为失真损失，进一步将第 2 阶段的模型微调 8 万 steps，以获得更好的 MS-SSIM 结果。我们的方法是基于 PyTorch 实现的。

所有的实验都是在使用一个 NVIDIA 2080TI GPU(11GB 内存）的机器上进行的。将 batch-size 设置为 4，并使用 Adam 优化器 [16]。第一阶段、第二阶段和微调阶段分别需要 4 天、3 天和 12 小时。

### 4.2 Experimental Results

#### Settings of the Baseline Methods

在本节中，提供了实验结果来证明我们提出的方法 FVC 在四个基准数据集 HEVC[27]，UVG[24]，VTL[1] 和 MCLJCV[34] 上的有效性。我们将我们的方法与其他最先进的方法进行了比较，包括传统方法 [37,27] 和最近提出的基于学习的方法 (DVC[22]，AD_ICCV19[10]，EA_CVPR20[2]，LU_ECCV20[21] 和 HU_ECCV20[14])。对于传统的方法 H.265[27]，使用 [14] 中的命令行，并使用 FFmpeg 和默认模式。为了进行公平的比较，我们还测试了 "DVC*" 的结果，这是 DVC 的一个增强版本，使用与我们的可变形补偿模块相同的运动和残差编解码器模块（见图 3)。按照之前的方法 [22，14，21]，我们将 HEVC 数据集的 GoP 大小设置为 10，将其他数据集设置为 12。我们使用与 H.265 中相同的 i 帧压缩方法来重建 i 帧。对于 “FVC#”，根据基于 MS-SSIM 的率失真权衡，进一步执行微调操作。

#### Results

<div align=center><img src="/images/FVC-2022-02-14-15-03-16.png" alt="FVC-2022-02-14-15-03-16" style="zoom:50%;" /></div>

在表 1 中，我们报告了在 HEVC，UVG，VTL 和 MCL-JCV 数据集上的不同视频压缩方法与 H.265 相比的 BDBR[7] 结果。具体来说，我们的方法在所有基准数据集的总体结果上节省了超过 18%的比特率。很明显，我们的方法优于 DVC 和其他最近最先进的基于学习的视频压缩方法。例如，与 H.265 相比，我们提出的 FVC 在 HEVC Class D 数据集上节省了 18.39%的比特率，而对于最近的方法 LU_ECCV20[21] 和 HU_ECCV20[14]，相应的比特率节省分别为 8.29%和 1.77%。

提供了不同压缩方法的 RD 曲线，见图 6。值得注意的是，我们的方法在所有数据集上都大幅优于基线方法 DVC[22] 和增强的版本 DVC*。我们想强调的是，DVC* 通过使用与我们所提出的方法 FVC 相同的压缩网络，直接压缩像素级光流图和像素级残差图，因此性能改进清楚地证明了我们所提出的方法的有效性。与传统方法 H.265 方法相比，我们的方法在所有比特率的 PSNRs 都取得了更好的结果。我们的方法在 MS-SSIM 也优于所有其他的基线方法。

<div align=center><img src="/images/FVC-2022-02-14-15-04-10.png" alt="FVC-2022-02-14-15-04-10" style="zoom:50%;" /></div>


### 4.3. Ablation Study and Analysis

#### Effectiveness of Different Components

<div align=center><img src="/images/FVC-2022-02-14-15-06-43.png" alt="FVC-2022-02-14-15-06-43" style="zoom:50%;" /></div>

如图 7 所示，以 HEVC Class D 数据集为例，证明了提出的框架 FVC 中不同模块的有效性。如图 7(a) 所示，为了证明非局部注意 (NLA) 模块的有效性，我们在多帧特征融合 (MFF) 模块中删除了 NLA，与我们的完整模型（即 FVC) 相比，FVC w/o NLA 的结果下降了近 0.2dB。我们还介绍了我们方法的基本模型（即 FVC w/o MFF 或 FVC-basic)，其中在完整模型中删除了多帧特征融合模块。与我们的完整模型（即 FVC) 相比，我们删除 MFF 模块后的基本模型 FVC-basic 的结果下降 0.5dB 至 0.3bpp（见绿色和黑色曲线）。这些实验结果证明了我们新提出的 MFF 模块配备 NLA 机制的融合多个先前帧特征的有效性。

为了验证我们提出的特征级操作的有效性，在图 7(b) 中我们报告了我们的 FVC-basic 的两种变体的结果。

1. FVC 基本（FS 运动和 P 残差）。利用我们的帧重构模块，将我们的可变形补偿模块后的预测特征 $\bar{F}_t$ 转换为预测帧，然后作为 DVC* 中像素空间残余压缩模块的输入。
2. FVC-基本的（PS-运动和 PS 残差）。我们在像素空间上执行与运动相关的操作和残差压缩，这在概念上与 DVC* 相同，除了在实现细节上有一些细微的差异。

值得注意的是，在 0.38bpp 时，FVC-basis 的性能优于 FVC-basic（FS-motion & PS-residual）0.3dB，这表明在特征空间上执行残差压缩是有益的。此外，与 FVC-basic（FS-motion & PS-residual）相比，FVC-basisc（PS-motion & PS-residual）在 0.4bpp 时实现了 1.2dB 的改进，这证明了在特征空间也需要进行运动补偿。

#### Analysis of Deformable Compensation

<div align=center><img src="/images/FVC-2022-02-14-15-08-54.png" alt="FVC-2022-02-14-15-08-54" style="zoom:50%;" /></div>

为了更好地比较特征空间和像素空间中的运动补偿模块，我们还将预测特征 $\bar{F}_t$ 作为帧重构模块的输入，生成预测帧，并评估预测帧的 PSNR 和相应的 bpp 来压缩运动信息。如图 8 所示，与 DVC* 的预测帧相比，FVC 的预测帧在 HEVC Class C 数据集上在 0.017bpp 时实现了 1.75dB 的改进。值得指出的是，DVC* 与 FVC 使用基于相同自动编码器风格网络的压缩方法，因此结果清楚地证明了可变形补偿模块的有效性。

#### Visualization of Deformable Compensation Results

在图 9 中，从 HEVC Class C 数据集以 RaceHorses 序列的第 6 帧为例，并将运动信息和运动补偿结果可视化。我们可视化了在 DVC* 中用于运动补偿的光流图（见图 9(b)）和两个具有代表性的偏移图（来自 72 张偏移图总数），用于可变形卷积（见图 9(c) 和图 9(d))。可以观察到，我们的工作中学习的偏移图编码与 DVC* 的光流图类似的运动模式。我们还观察到，与在 DVC* 中使用基于光流的运动补偿方法相比，使用我们提出的可变形补偿（DC）模块后，一个 patch 的运动补偿结果在视觉上与 ground-truth patch 更相似。在实践中，我们只通过 65% bpp 进行运动编码来实现 0.27dB 的增益（见图 9(e)，图 9(f) 和图 9(g))。

<div align=center><img src="/images/FVC-2022-02-14-15-11-59.png" alt="FVC-2022-02-14-15-11-59" style="zoom:50%;" /></div>

#### Running Time and Model Complexity

FVC 中，参数的总数约为 2600 万，其中用于偏移图和残差特征图的压缩网络的参数需要超过 2400 万。使用分辨率为 1920×1080 分辨率的视频来评估使用一个 2080TI GPU(11GB 内存）在机器上的推理时间。我们提出的框架 FVC 和 FVC-basic 以及 DVC 和 DVC* 的编码时间为 548ms、201ms、460ms 和 709ms，以 H.265 作为 anchor 方法在 HEVC Class D 数据集上相应的 BDBR 为 -18.39%、-7.09%、14.08%和 4.31%。从结果中，我们观察到我们的 FVC-basic 在效率和有效性方面都优于 DVC 和 DVC*。此外，提出的框架 FVC 通过多帧特征融合模块的 347ms 实现了最佳的 BDBR 结果，可以通过在未来的工作中使用更简单的融合模块来加速。 

## 五、总结

在本文中，提出了一种新的特征空间深度视频压缩框架 FVC，包括可变形补偿模块，特征级残差压缩模块和多帧特征融合模块。

可变形补偿模块首先将偏移图预测为运动信息，然后使用新提出的运动压缩模块进行压缩，最后将重构的偏移图用于可变形卷积进行运动补偿。所提出的多帧特征融合模块从当前帧和前一个帧中提取多种特征表示，并利用可变形补偿和非局部注意机制来细化初始重构特征，以获得更好的帧重构。

通过在特征空间中执行所有的操作，FVC 在 HEVC、UVG、VTL 和 MCL-JCV 数据集上取得了良好的结果。
