---
title: "Variational Image Compression With A Scale Hyperprior"
subtitle: "变分自编码器图像压缩"
layout: post
author: "L Hondong"
header-img: "img/post-bg-45.jpg"
mathjax: true
tags:
  - 笔记
---

# Variational Image Compression With A Scale Hyperprior

## 摘要

引入了一个超先验网络，用来捕获特征图之间的空域冗余问题。这种优先的边信息的使用在传统编码中已经通用，但是在自编码器中还未得到开发。且该模型与之前的模型不同，超先验网络结构是与自编码器一起进行端到端的训练优化的。

## 一、简介

### 1.1 Motivation

基于香农定理，使用估计所得熵模型对隐层表示建模理论上的编码下界为：

$$
R = \mathbb{E}_{\hat{y}\sim m}[-\log_2 p_{\hat{y}}(\hat{y}))]
$$

其中 $m$ 为隐层表示（latent representation）实际分布，$p_{\hat{y}}$ 为熵模型估计分布，熵模型是一个发送人与接收人共享的先验概率模型，用来估计真实隐层表示分布。上面的式子说明当熵模型估计 $p_{\hat{y}}$ 与实际分布 $m$ 完全相同的时候会有编码长度最小。这告诉我们，一方面，当熵模型使用全分解（fully factorized）概率分布的时候，如果实际分布中存在统计依赖，熵模型估计分布天然不能拟合实际分布；$y=\text{encoder}(x)$ 另一方面，因为是一个确定性的过程，如果想要在使用全分解概率分布的情况下效果提升，则需要 encoder 对输入的自然图像尽量多的去除统计依赖。

这篇文章，关键在于引入边信息，捕捉隐层表示的隐藏信息以辅助熵模型的参数生成，从而改善熵模型估计与隐层实际分布不匹配问题。将边信息导入比特流，这使得 decoder 也可以共享熵模型。解压时 decoder 先解压边信息，构建熵模型，之后基于正确的熵模型解压隐层信息。

### 1.2 Contributions

## 二、相关工作

### 2.1 

### 2.2 

## 三、方法

### 3.1 

算法的优化问题可以建模为 VAE 模型。VAE 是一个概率生成模型。熵模型对应 VAE 隐层表示的先验 $p_{\tilde{y}}$。边信息可以看做是熵模型参数的先验，先验的先验这里称之为超先验。于是 VAE 中使用一个带参变分密度 $q(\tilde{y}\vert x)$ 来拟合真实后验概率 $p_{\tilde{y}\vert x}(\tilde{y}\vert x)$，通过最小化优化目标——隐变量真实分布与模拟分布的 KL 散度来达到拟合的效果，这相当于最小化图像压缩中的率失真（rate-distortion）性能：

$$
\mathbb{E}_{x\sim p_x} D_{KL}[q\Vert p_{\tilde{y}\vert x}] = \mathbb{E}_{x\sim p_x}\mathbb{E}_{\tilde{y}\sim q}[\underbrace{\log q(\tilde{y}\vert x)}_0\underbrace{-\log p_{x\vert \tilde{y}}(x\vert \tilde{y})}_{weighted \ distortion}\underbrace{-\log p_{\tilde{y}}(\tilde{y})}_{rate}]+\text{const}
$$

1. 第一项如下所示，$U$ 是一个宽度为 1 的均匀分布（如第三章中描述，在训练的时候使用添加均匀噪声代替量化），$q$ 概率为 1。$\log q == 0$。

$$
q(\tilde{y}\vert x,\phi_g) = \prod_i \boldsymbol{U}(\tilde{y}\vert y_i - \frac{1}{2},y_i+\frac{1}{2})) , y=g_a(x;\phi_g)
$$

2. 如果假设满足下面的分布：

$$
p_{x|\tilde{y}}(x|\tilde{y},\theta_g) = \boldsymbol{N}(x|\tilde{x},(2\lambda)^{-1}I), \tilde{x} = g_s(\tilde{y};\theta_g)
$$

那么第二项就是 $x$ 和 $\tilde{x}$ 的平方差，以 $\lambda$ 为权重。也就是说，如果以 $\tilde{y}$ 为条件的 $x$ 的分布满足如上条件的多维高斯分布，那么第二项可以看做图像压缩中的类 MSE 的失真项 distortion，最小化目标函数等同于缩小重构图像的失真。



把压缩问题中的分析模型 $g_a$ 看做 VAE 中的推理模型，把合成模型 $g_s$ 看做生成模型。在 VAE 的推理模型，目的是要估计真实的后验概率 $p_{\tilde{y}\vert x}(\tilde{y}\vert x)$，这通常不可行。

输入图片经过主编码器 $g_a$ 得到输出 $y$，即通过编码得到隐空间的特征表示，然后经过量化器 $Q$ 后得到输出 $\hat{y}$。另一方面，隐变量 $y$ 作为超先验网络 $h_a$ 的输入，对进行尺度 $\sigma$ 捕获，对潜在表示每一个点进行均值为 0，方差为 $\sigma$ 的高斯建模，不同于以往的方法对整体潜在特征进行建模，即一个熵模型在推理阶段应用在所有的特征值熵，而超先验网络为每个特征点都进行了熵模型建模，通过对特征值的熵模型获取特征点的出现情况以用于码率估计和熵编码。

由于算术/范围编码器在编解码阶段都需要解码点的出现概率或者 CDF（累计概率分布）表。故而需要将这部分信息传输到解码端，以用于正确的熵解码。故而超先验网络对这部分信息先压缩成 $z$, 通过对 $z$ 进行量化熵编码传输至超先验解码端，通过超先验解码端解码学习潜在表示 $y$ 的建模参数。通过超先验网络获取得到潜在表示 $y$ 的建模分布后，通过对其建模并且对量化后的 $\hat{y}$ 进行熵编码得到压缩后的码流文件，而通过熵解码得到 $\hat{y}$，传统的编解码框架中往往设定反量化模块，而在解码网络中，包含了反量化的功能。故而在反量化模块在论文中并未部署，再将熵解码结果 $\hat{y}$ 输入到主解码端，得到最终的重建图片 $\hat{x}$。

对于整个网络的参数优化问题，依旧采用整体的率失真函数：

$$
L = \lambda\cdot D + R 
$$

其中 $D$ 代表重建图像与原图的失真度， $R=R_{\hat{y}}+R_{\hat{z}}$ 代表整体框架的压缩码率。

上述量化会引入误差，这种误差在有损压缩的情况下是可以容忍的，同时该模型也遵循 Balle 在 2017 年论文中的量化方式，采用添加均匀噪声的形式近似量化操作，传统方式通过调整量化步长的形式进行率失真控制，而端到端的优化技术通过对码率和失真的权衡进行控制，如使用上述公式中的 $\lambda$ 进行率失真折衷。

假设熵编码技术有效运行，则可以再次将其写为交叉熵：

$$
R=E_{x\sim p_x}[-\log p_{\hat{y}}(Q(g_a(x;\phi_g)))]
$$

其中 $Q$ 表示量化函数，而 $p_{\hat{y}}$ 表示熵模型。在这种情况下，潜在表示的边际分布来自（未知）图像分布 $p_x$，$Q$ 量化形式，以及分析变换的属性都会影响最终实际的编码码率值。

### 3.2 

## 四、实验

### 4.1 

### 4.2 

## 五、总结