# 图像视频压缩

随着数据量的爆炸性增长，存储空间和传输容量的需求也与日俱增，然而技术的增长率远远跟不上数据的增长率。传统编解码算法日渐接近信息熵极限，几乎达到了性能极致。而基于深度学习的方法因为能够更深层次的挖掘图像深层信息，因此有望突破已有的性能瓶颈。针对存储中的图像和视频数据（存储时未进行压缩或者压缩率不高），致力于构建数据压缩算法框架， 实现图像与视频高效有损压缩，降低数据存储、后期数据维护、数据备份成本，并且借鉴目前业界、学界数据压缩最新方式，构建出率数据压缩质量、数据压缩率处于领先地位的压缩算法框架。

数据量的指数级增长对存储空间和传输容量提出了新的需求，在一些专业场景中，如卫星图像，地理信息系统等，高质量的图片需要更大的存储空间，如微观组织 4096*4096 像素的阵列图像需要占用 48 MB 的存储空间。若没有数据压缩，则很难甚至有时无法存储或传递大量数据文件。传统的编解码技术达到瓶颈，而基于深度学习的方法具有天然的优势，使用 AI 技术实现、辅助数据压缩是当前热点。工业界与学术界不仅有领先的研究成果，相关的标准也在制定中。

## 任务

1. 调研学术界关于图像有损压缩、视频深度学习算法压缩研究的相关论文及 workshop，梳理出技术发展方向及当前在业界广泛应用的主流的技术、算法，产出系统化报告，详细阐述图像、视频压缩的主流方案；
2. 结合 JPG、 WebP 以及 BPG 编解码方法，研究基于生成网络模型的有损图像压缩方法，以传统编解码算法为 baseline，同当前 SoTA 深度学习压缩算法对比，在压缩率，或质量评价指标如 PSNR、MS-SSIM 等上取得优势；
3. 结合 H.264、H.265 等压缩算法，实现端到端基于学习的视频压缩框架，按需实现有损视频压缩在业务场景中落地，进而综合利用基于生成对抗网络的图像重构算法、超分算法、新型编解码算法等，实现视频图像高效压缩，形成算法壁垒。

运行平台：Linux  
开发语言：Python & C++

## 技术难点

当前，伴随着 AI 算法的迅猛发展，为获取更强表征能力、更高精度的模型，模型训练参数与复杂度增加速度远远超过硬件自身计算能力的发展（GPT 为例，自 2018 年 6 月至 2020 年 5 月，模型参数量增加超过 1500 倍，与此同时硬件处理器计算能力增加在 4 倍以内）。与此同时，随着 IoT、物联网等领域的迅猛发展，AI 模型在端侧（计算资源、内存均极为有限）部署的需求也日益强烈。

为解决模型压缩推理的相关问题，部分公司组建了大型团队持续投入以解决相关问题，例如商汤、依图等公司，在模型蒸馏、量化、剪枝、训练分布式等多个领域，已经形成了成熟的方法与方案，业界也有成熟论文进行相关领域最新进展的发表。

在 AI+数据压缩领域，对 AI 模型应用在图像、视频等格式的压缩场景方面进行了系统的调研和研究，方法包括多篇顶会顶刊文章、国内外厂商产品、专利书等探讨和涉及的 AI 技术。在图像和视频压缩方面，在各公司普遍使用的神经网络结构基础上， 针对特定场景优化网络结构，包括判别器优化以适应多尺寸图像、生成器判别器网络训练同步设计、模型场景判别等，通过结合 AI 模型实现各场景下的压缩构建技术壁垒。 主要涉及以下几个挑战：

1. AI 模型结构选型  
目前研究涉及的网络结构很多，主要包括 RNN、 CNN、 Transform、 GAN 等几种方式，如何选择网络结构，甚至需要选取多种网络结构进行组合，在结合调研各大厂商以及学界研究的基础上，需要结合具体的场景去进行选择。
2. 神经网络模型初始结构只能对特定尺寸进行压缩  
在实际场景应用中，单一的神经网络结构往往只能适应单一的数据尺寸，综合考虑采用数据预处理对图片进行裁剪或拉伸，或者修改网络结构以适应处理任意尺寸的图片。
3. 生成图像可能存在欺诈性的清晰与高分辨率  
在生成图像时有可能生成的图像具有清晰的纹理，很好的视觉效果，很高的分辨率和清晰度，但与原图对比却可能存在明显差异。主要是因为，实现图像的有损压缩使用的 AI 技术，除起到预测作用、与传统编解码结合的模型外，大部分网络结构采取重建图像来实现压缩效果，因此学习到的图像信息可能与原图像有差异。因此需要通过衡量与原图像的差异来评估生成图像的正确性，同时引入场景分类等方案，进一步提升图片质量（质量指标如：PSNR、MS-SSIM 等），解决上述问题。
