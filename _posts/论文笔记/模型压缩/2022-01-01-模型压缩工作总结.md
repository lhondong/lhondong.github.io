---
title: "模型压缩工作总结"
subtitle: "整理模型压缩经典工作"
layout: post
author: "L Hondong"
header-img: "img/post-bg-5.jpg"
tags:
  - 模型压缩
---

# 模型压缩工作总结

## 一、剪枝

剪枝就是通过去除网络中冗余的 channels, filters, neurons, or layers 以得到一个更轻量级的网络，同时不影响性能。代表性的工作有：

- 奇异值分解 SVD(NIPS 2014)：Exploiting linear structure within convolutional networks for efficient evaluation
- 韩松 (ICLR 2016)：Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding
- (NIPS 2015)：Learning both weights and connections for efficient neural network
- 频域压缩 (NIPS 2016)：Packing convolutional neural networks in the frequency domain
- 剪枝 Filter Reconstruction Error(ICCV 2017)：Thinet: A filter level pruning method for deep neural network compression
- LASSO regression(ICCV 2017)：Channel pruning for accelerating very deep neural networks
- Discriminative channels(NIPS 2018)：Discrimination-aware channel pruning for deep neural networks
- 剪枝 (ICCV 2017)：Channel pruning for accelerating very deep neural networks
- neuron level sparsity(ECCV 2017)：Less is more: Towards compact cnns
- Structured Sparsity Learning(NIPS 2016)：Learning structured sparsity in deep neural networks

## 二、轻量化

轻量化模块设计就是设计一些计算效率高，适合在端侧设备上部署的模块。代表性的工作有：

- Bottleneck(ICLR 2017)：Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size
- MobileNet(CVPR 2017)：Mobilenets: Efficient convolutional neural networks for mobile vision applications
- ShuffleNet(CVPR 2018)：Shufflenet: An extremely efficient convolutional neural network for mobile devices
- SE 模块 (CVPR 2018)：Squeeze-and-excitation networks
- 多用卷积核 (NIPS 2018)：Learning versatile filters for efficient convolutional neural networks
- GhostNet(CVPR 2020)：GhostNet: More features from cheap operations

Shift 操作：
- 无参数的 Shift 操作 (CVPR 2018)：Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions
- CVPR2019：All you need is a few shifts: Designing efficient convolutional neural networks for image classification
- arXiv：Deep-shift: Towards multiplication-less neural networks
- arXiv：Shift-based primitives for efficient convolutional neural networks
- NeurIPS 2020：ShiftAddNet: A Hardware-Inspired Deep Network

## 三、蒸馏

蒸馏就是过模仿教师网络生成的软标签将知识从大的，预训练过的教师模型转移到轻量级的学生模型。代表性的工作有：
- Hinton(NIPS 2015)：Distilling the knowledge in a neural network
- 中间层的特征作为提示 (ICLR 2015)：Fitnets: Hints for thin deep nets
- 多个 Teacher(SIGKDD 2017)：Learning from multiple teacher networks
- 两个特征得新知识再 transfer(CVPR 2017)：A gift from knowledge distillation: Fast optimization, network minimization and transfer learning

## 四、量化

量化就是减少权重等的表示的位数，比如原来网络权值用 32 bit 存储，现在我只用 8 bit 来存储，以减少模型的 Memory 为原来的 1/4；更有甚者使用二值神经网络。代表性的工作有：

### 量化

- 韩松 (ICLR 2016)：Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding
- (ICML 2015)：Compressing neural networks with the hashing trick
- (NIPS 2015)：Learning both weights and connections for efficient neural network
- 8-bit integers for both weights and activation (CVPR 2018)：Quantization and training of neural networks for efficient integer-arithmetic-only inference
- (CVPR 2018)：Explicit loss-error-aware quantization for low-bit deep neural networks
- (ECCV 2016)：XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks
- (CVPR 2016)：Quantized convolutional neural networks for mobile devices
- (ICLR 2017)：Trained Ternary Quantization
- (NIPS 2017)：Runtime Neural Pruning
- (ICML 2018)：Deep k-means: Re-training and parameter sharing with harder cluster assignments for compressing deep convolutions
- (arXiv 2018)：PACT: Parameterized Clipping Activation for Quantized Neural Networks
- (CVPR 2018)：Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
- (CVPR 2018)：Explicit loss-error-aware quantization for low-bit deep neural networks
- (CVPR 2019)：Learning to quantize deep networks by optimizing quantization intervals with task loss
- 1 位 weight，2 位 activation：DoReFa-Net - Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
- (CVPR 2019)：HAQ: Hardware-Aware automated quantization with mixed precision
- Quantizing deep convolutional networks for efficient inference - A whitepaper

### 二值神经网络

- Binarized weights(NIPS 2015)：BinaryConnect: Training deep neural networks with binary weights during propagations
- Binarized activations(NIPS 2016)：Binarized neural networks
- XNOR(ECCV 2016)：Xnor-net: Imagenet classification using binary convolutional neural networks
- more weight and activation(NIPS 2017)：Towards accurate binary convolutional neural network
- (ECCV 2020)：Learning Architectures for Binary Networks
- (ECCV 2020)：BATS: Binary ArchitecTure Search

## 五、低秩分解

低秩分解就是将原来大的权重矩阵分解成多个小的矩阵，而小矩阵的计算量都比原来大矩阵的计算量要小。代表性的工作有：
- 低秩分解 (ICCV 2017)：On compressing deep models by low rank and sparse decomposition
- 乐高网络 (ICML 2019)：Legonet: Efficient convolutional neural networks with lego filters
- 奇异值分解 (NIPS 2014)：Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation

## 六、加法网络

加法网络就是：利用卷积所计算的互相关性其实就是一种“相似性的度量方法”，所以在神经网络中用加法代替乘法，在减少运算量的同时获得相同的性能。代表性的工作有：
- (CVPR 20)：AdderNet: Do We Really Need Multiplications in Deep Learning?
- (NIPS 20)：Kernel Based Progressive Distillation for Adder Neural Networks
- (arXiv)：AdderSR: Towards Energy Efficient Image Super-Resolution

## 七、Slimming

- (ICCV 2017)：清华张长水，黄高团队：Learning Efficient Convolutional Networks through Network Slimming
- (ECCV 2020)：得克萨斯大学奥斯汀分校团队：GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework
