---
title: "轻量化模型总结"
subtitle: "SqueezeNet, Xception, MobileNet, ShuffleNet 总结整理"
layout: post
author: "L Hondong"
header-img: "img/post-bg-43.jpg"
mathjax: true
tags:
  - 模型压缩
  - 轻量化模型
---

# 轻量化模型总结

## [原文](https://blog.csdn.net/liuxiao214/article/details/81875251)

SqueezeNet
- 在 ImageNet 上实现了与 Alexnet 相似的效果，参数只有其 1/50， 模型是 0.5MB，占其 1/510

Xception
- 虽然降低了参数量，但加宽了网络结构，因此 Xception 重点不在于压缩模型，旨在于提高性能

MobileNet
- 采用 depthwise separable convolution，就是分离卷积核
- 设置宽度因子 width multipler 和分辨率因子 resolution multiplier

ShuffleNet
- 借鉴 resnext 分组卷积思想，但不同的是采用 1x1 卷积核
- 进行通道清洗，加强通道间的信息流通，提高信息表示能力

总结今年来的几个轻量化模型：SqueezeNet、Xception、MobileNet、ShuffleNet，下面给出时间轴：
- 2016.02 伯克利&斯坦福提出 SqueezeNet
- 2016.10 google 提出 Xception
- 2017.04 google 提出 MobileNet
- 2017.07 face++提出 ShuffleNet

其次，说一下模型轻量化的一些方法：
- 卷积核分解：使用 1xN 和 NX1 卷积核代替 NXN 卷积核；
- 使用深度压缩 deep compression 方法：网络剪枝、量化、哈弗曼编码；
- 奇异值分解；
- 硬件加速器；
- 低精度浮点数保存；

小模型的好处有哪些：
- 在分布式训练中，与服务器通信需求小；
- 参数少，从云端下载模型的数据量小；
- 更适合在 FPGA 等内存首先的嵌入式、移动端设备上部署；

## 1. SqueezeNet

地址链接：https://arxiv.org/pdf/1602.07360.pdf

### 1.1 核心思想

在 ImageNet 上实现了与 Alexnet 相似的效果，参数只有其 1/50， 模型是 0.5MB，占其 1/510。  
SqueezeNet 核心内容有以下几点：
1. 使用 1x1 卷积核代替 3x3 卷积核，减少参数量；
2. 通过 squeeze layer 限制通道数量，减少参数量；
3. 借鉴 inception 思想，将 1x1 和 3x3 卷积后结果进行 concat；为了使其 feature map 的 size 相同，3x3 卷积核进行了 padding；
4. 减少池化层，并将池化操作延后，给卷积层带来更大的激活层，保留更多地信息，提高准确率；
5. 使用全局平均池化代替全连接层；

上述 1-3 是通过 fire module 实现的，fire module 主要分为两部分，如下图所示

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-14-44.png" alt="轻量化模型总结-2022-01-24-13-14-44" style="zoom:50%;" /></div>

- squeeze：1x1 卷积核，参数 $s_{1x1}$ 表示卷积核数量
- expand：1x1 卷积核和 3x3 卷积核，参数 $e_{1x1}$ 和 $e_{3x3}$ 分别表示两种卷积核的数量。

该模块一共三参数 $s_{1x1}$、$e_{1x1}$、$e_{3x3}$，关系保持 $s_{1x1}< e_{1x1}+e_{3x3}$

### 1.2 网络结构

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-16-44.png" alt="轻量化模型总结-2022-01-24-13-16-44" style="zoom:50%;" /></div>

### 1.3 实验结果

实验结果表示模型小，且准确率不降，反而有点提高；

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-17-20.png" alt="轻量化模型总结-2022-01-24-13-17-20" style="zoom:50%;" /></div>

参考链接：  
1. https://blog.csdn.net/csdnldp/article/details/78648543  
2. https://blog.csdn.net/u011995719/article/details/78908755

## 2. Xception

地址链接：https://arxiv.org/abs/1610.02357

### 2.1 核心思想
虽然本文中方法可以降低参数量，但是论文加宽了网络结构，因此这篇论文不在于压缩模型，旨在于提高性能，与同等参数量的 inception v3 相比，效果更好。

首先是 inception v3 的一系列延伸，见下图：  
版本 1：最初的 inception v3

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-18-42.png" alt="轻量化模型总结-2022-01-24-13-18-42" style="zoom:50%;" /></div>

版本 2：对 1 进行简化

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-19-01.png" alt="轻量化模型总结-2022-01-24-13-19-01" style="zoom:50%;" /></div>

版本 3：对 2 简化，可以先使用一个统一的 1x1 卷积核，然后每个 3x3 卷积核的输入只是 1x1 卷积后的 feature map 的一部分。本图中是 1/3；

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-19-29.png" alt="轻量化模型总结-2022-01-24-13-19-29" style="zoom:50%;" /></div>

版本 4：在 3 的基础上进一步延伸，将 1x1 卷积后的所有 feature map 按通道全部划分，每一个通道对应一个 3x3 卷积，即 3x3 卷积核的数量就是 1x1 卷积后 feature map 的通道数。

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-19-49.png" alt="轻量化模型总结-2022-01-24-13-19-49" style="zoom:50%;" /></div>

然后在 Xception 中，主要采用 depthwise separable convolution 思想（这个后面在 MobileNet 中详细解释，好奇怪，明明是 MobileNet 后出现的，反正都是一家的，估计公布先后的问题吧。）

首先 Xception 类似于图 4，但是区别有两点：

1. xception 中没有 relu 激活函数； 
2. 图 4 是先 1x1 卷积，后通道分离；xception 是先进行通道分离，即 depthwise separable convolution，然后再进行 1x1 卷积。

此外，进行残差连接时，不再是 concat，而是采用加法操作。

### 2.2 网络结构

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-20-39.png" alt="轻量化模型总结-2022-01-24-13-20-39" style="zoom:50%;" /></div>

### 2.3 实验结果

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-20-59.png" alt="轻量化模型总结-2022-01-24-13-20-59" style="zoom:50%;" /></div>

参考链接：
1. https://blog.csdn.net/u014380165/article/details/75142710
2. https://www.baidu.com/link?url=ERPMsc_io0x0usMhZdLD5POp-4p3dHyNtg4z92eeNsIpzxbKJMtmEH39A5op8p2XiQ4CWDPMu03Ygbrs8GAOAK&wd=&eqid=e517c5270000c4ed000000035b7a2a44

## 3. MobileNet

地址链接：https://arxiv.org/pdf/1704.04861.pdf

### 3.1 核心思想

主要是两个策略：
1. 采用 depthwise separable convolution，就是分离卷积核；
2. 设置宽度因子 width multipler 和分辨率因子 resolution multiplier；

#### 3.1.1 depthwise separable convolution

假设上一层得到的 feature map 的 size 为 $D_K \cdot D_K \cdot M$，本层的卷积核大小为 $D_K \cdot D_K$，卷积核个数为 M。

1. 首先介绍传统卷积核的操作方式，如下图。

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-22-14.png" alt="轻量化模型总结-2022-01-24-13-22-14" style="zoom:50%;" /></div>

卷积核 $D_K \cdot D_K$ 需要在 feature map 的每个通道上进行 $D_K \cdot D_K$ 次卷积，然后一共 M 个卷积核，因此计算量为：$D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F$

2. 介绍 depthwise separable convolution，如下图

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-24-10.png" alt="轻量化模型总结-2022-01-24-13-24-10" style="zoom:50%;" /></div>

将卷积核进行拆解，分为两步，首先用 M 个 $D_K \cdot D_K$ 卷积核在 feature map 进行卷积，计算量为：$D_K \cdot D_K \cdot M \cdot D_F \cdot D_F$。

然后再使用 N 个 $1 \cdot 1 \cdot M$ 卷积核在前面得到的结果上进行 feature map，计算量为：$M \cdot N \cdot D_F \cdot D_F$。

所以，进行分解后的总计算量为：$D_K \cdot D_K \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F$。

1. 计算量比较

$$\frac{D_K \cdot D_K \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F}{D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F} = \frac{1}{N} + \frac{1}{D^2_K}$$

可以看到，随着卷积核个数的增加，即通道数变多，feature map 的大小，传统方式的计算量比分解要大得多。

#### 3.1.2 宽度因子和分辨率因子

怎么才能使网络进一步压缩呢？可以进一步减少 feature map 的通道数和 size，通过宽度因子减少通道数，分辨率因子减少 size。

1. 宽度因子 $\alpha$  

$$D_K \cdot D_K \cdot \alpha M \cdot D_F \cdot D_F + \alpha M \cdot \alpha N \cdot D_F \cdot D_F$$

2. 分辨率因子 $\beta$  

$$D_K \cdot D_K \cdot \alpha M \cdot \beta D_F \cdot \beta D_F + \alpha M \cdot \alpha N \cdot \beta D_F \cdot \beta D_F$$

两个参数都属于 (0,1] 之间，当为 1 时则是标准 mobileNet。

### 3.2 网络结构

#### 3.2.1 基本结构块

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-25-01.png" alt="轻量化模型总结-2022-01-24-13-25-01" style="zoom:50%;" /></div>

#### 3.2.2 网络结构

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-25-27.png" alt="轻量化模型总结-2022-01-24-13-25-27" style="zoom:50%;" /></div>

#### 3.2.3 训练细节

1. 使用 RMSprop 优化器；
2. 未做大量数据增强，因为参数量小过拟合不严重；
3. 采用了随机图像裁剪输入；
4. 使用较小的 weight decay，或者不使用；

### 3.3 实验结果

给出几个基本的实验比较结果。

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-26-04.png" alt="轻量化模型总结-2022-01-24-13-26-04" style="zoom:50%;" /></div>

参考链接：
1. https://blog.csdn.net/t800ghb/article/details/78879612
2. https://blog.csdn.net/wfei101/article/details/78310226
3. https://blog.csdn.net/u014380165/article/details/72938047

## 4. ShuffleNet

地址链接：https://arxiv.org/pdf/1707.01083.pdf

### 4.1 核心思想

核心思想有两点：
1. 借鉴 ResNext 分组卷积思想，但不同的是采用 1x1 卷积核；
2. 进行通道清洗，加强通道间的信息流通，提高信息表示能力。

此外本篇论文中也采取了 Mobilenet 的 depthwise separasable convolution 的方式。

#### 4.1.1 逐点群卷积 pointwise group convolution

这个就是采用 resnext 的思想，将通道分组，每组分别进行卷积操作，然后再把结果进行 concat。但是不同于 resnext 的是，shufflenet 采用的是 1x1 卷积核。

#### 4.1.2 通道清洗 channel shuffle

什么是通道 shuffle，就是在分组卷积后得到的 feature map 不直接进行 concat，先将每组 feature map 按通道打乱，重新 concat，如下图所示：

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-27-23.png" alt="轻量化模型总结-2022-01-24-13-27-23" style="zoom:50%;" /></div>

如何进行 shuffle，这里参考 [链接](https://blog.csdn.net/u011974639/article/details/79200559)  

下面是 pytorch 实现 channel shuffle 的代码。

```python
def shuffle_channels(x, groups):
    """shuffle channels of a 4-D Tensor"""
    batch_size, channels, height, width = x.size()
    assert channels % groups == 0
    channels_per_group = channels // groups
    # split into groups
    x = x.view(batch_size, groups, channels_per_group,
               height, width)
    # transpose 1, 2 axis
    x = x.transpose(1, 2).contiguous()
    # reshape into orignal
    x = x.view(batch_size, channels, height, width)
    return x
```


对于一个卷积层分为 g 组：

1. 卷积后一共得到 g×n 个输出通道的 feature map；
2. 将 feature map 进行 reshape 为 (g,n);
3. 进行转置为 (n,g)；
4. 对转置结果 flatten，再分回 g 组作为下一层的输入。

### 4.2 网络结构

#### 4.2.1 shuffle unit

下图中，a 是标准的残差结构，不过是 3x3 卷积核使用了 mobilenet 中的 depthwise convolution 操作；

b 是在 a 的基础上加了本文的通道 shuffle 操作，先对 1x1 卷积进行分组卷积操作，然后进行 channel shuffle；

c 是在旁路加了一步长为 2 的 3x3 的平均池化，并将前两者残差相加的操作改为了通道 concat，增加了通道数量。

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-27-50.png" alt="轻量化模型总结-2022-01-24-13-27-50" style="zoom:50%;" /></div>

#### 4.2.2 网络结构

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-28-21.png" alt="轻量化模型总结-2022-01-24-13-28-21" style="zoom:50%;" /></div>

### 4.3 实验结果

1. 评估逐点组卷积：分组的效果均比没有分组的效果好，但是某些模型随着组数增加，性能有下降，这就是通道间失去联系带来的问题；

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-28-58.png" alt="轻量化模型总结-2022-01-24-13-28-58" style="zoom:50%;" /></div>

2. 评估 channel shuffle，shuffle 会比没有 shuffle 效果好，而且对于组数越大，效果越好，说明了 shuffle 的重要性，也说明了上图中组数增加性能下降的问题。

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-29-23.png" alt="轻量化模型总结-2022-01-24-13-29-23" style="zoom:50%;" /></div>

3. 与 MobileNet 的比较

<div align=center><img src="/assets/轻量化模型总结-2022-01-24-13-29-47.png" alt="轻量化模型总结-2022-01-24-13-29-47" style="zoom:50%;" /></div>

参考链接：
1. https://blog.csdn.net/u011974639/article/details/79200559
2. https://blog.csdn.net/u014380165/article/details/75137111