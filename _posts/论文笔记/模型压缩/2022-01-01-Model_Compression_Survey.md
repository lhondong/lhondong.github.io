---
title: "深度学习模型压缩与加速综述"
subtitle: ""
layout: post
author: "L Hondong"
header-img: "img/post-bg-3.jpg"
mathjax: true
tags:
  - 模型压缩
  - 综述
---

# 深度学习模型压缩与加速综述

## 摘要

随着训练可用数据量的增长与计算平台处理能力的增强，基于深度学习的智能模型能完成越来越复杂的任务，其在计算机视觉、自然语言处理等人工智能领域已经取得重大突破。然而这些深度模型具有庞大的参数规模，与之伴随着可畏的计算开销与内存需求，使得其在计算能力受限平台（例如移动嵌入式设备）的部署中遇到了巨大的困难与挑战，因此如何在不影响深度学习模型性能的情况下进行模型压缩与加速成为研究热点。

深度学习模型的压缩和加速是指利用**神经网络参数的冗余性和网络结构的冗余性**精简模型，在不影响任务完成度的情况下，得到参数量更少、结构更精简的模型。被压缩后的模型计算资源需求和内存需求更小，相比原始模型能满足更广泛的应用需求。

本文首先对国内外学者提出的经典深度学习模型压缩与加速方法进行分析，从参数剪枝、参数量化、紧凑网络、知识蒸馏、低秩分解、参数共享和混合方式等 7 个方面分类总结。其次，总结对比几种主流技术的代表性方法在多个公开模型上的压缩与加速效果。最后，对于模型压缩与加速领域的未来研究方向进行展望。

## 简介

### 研究背景

深度学习模型性能提高的同时伴随着其计算越来越复杂，计算开销和内存需求逐渐增加。仅 8 层的 AlexNet[4] 需要 0.61 亿个网络参数和 7.29 亿次浮点型计算，花费约 233MB 内存；随后的 VGG-16[5] 的网络参数达到 1.38 亿，浮点型计算次数为 1.56 亿，需要约 553MB 内存；为了克服深层网络的梯度消失问题，Kaiming He 提出 ResNet[6] 网络，首次在 ILSVRC 比赛 [3] 实现低于 5%的 top-5 分类错误，偏浅的 ResNet-50 网络参数就达到 0.25 亿，浮点型计算次数高达 3.9 亿，内存花费约 102MB。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-42-05.png" alt="Model_Compression_Survey-2022-01-12-09-42-05" style="zoom:50%;" /></div>

庞大的网络参数意味着更大的内存存储，而增长的浮点型计算次数意味着训练成本和计算时间的增长，这极大限制了在资源受限设备例如智能手机、智能手环等的部署。如表 1 所示，深度模型在 Samsung Galaxy S6 的推理时间远超 Titan X 桌面级显卡，实时性较差，无法满足实际应用需要。

| 模型      | Samsung Galaxy S6 | Titan X |
| --------- | ----------------- | ------- |
| AlexNet   | 117ms             | 0.54ms  |
| GoogleNet | 273ms             | 1.83ms  |
| VGG-16    | 1926ms            | 10.67ms |

在深度学习技术日益火爆的背景下，对深度学习模型强烈的应用需求使得人们对内存占用少、计算资源要求低、同时依旧保证相当正确率的“小模型”格外关注。利用神经网络的冗余性进行深度学习的模型压缩和加速引起了学术界和工业界的广泛兴趣，各种各样的工作层出不穷。

## 压缩方法

从压缩参数和压缩结构两个角度可以分成 7 类

**压缩参数**

1. 参数剪枝：设计关于参数重要性的评价准则，基于该准则判断网络参数的重要程度，删除冗余参数
2. 参数量化：将网络参数从 32 位全精度浮点数量化到更低位数
3. 低秩分解：利用矩阵或张量分解技术，将高维参数向量降维分解为稀疏的低维向量 Low-rank factorization
4. 参数共享：利用结构化矩阵或聚类等方法映射网络内部参数，探索模型参数的冗余性，利用 Hash 或量化的技术对权值进行压缩

**压缩结构**

1. 紧凑网络：从卷积核、特殊层和网络结构三个级别设计新型网络
2. 知识蒸馏：将较大的教师模型的信息提炼到较小的学生模型

**混合方式**

### 1. 参数剪枝

参数剪枝是指在预训练好的大型模型基础上，设计对网络参数的评价准则，以此为根据删除“冗余”参数。

根据剪枝粒度粗细，参数剪枝可分为非结构化剪枝和结构化剪枝。

非结构化剪枝的粒度比较细，可以无限制去掉网络中期望比例的任何“冗余”参数，但是会带来裁剪后网络结构不规整难以有效加速的问题。

结构化剪枝的粒度比较粗，剪枝的最小单位是 filter 内参数的组合，通过对 filter 或者 feature map 设置评价因子，甚至可以删除整个 filter 或者某几个 channel，使网络“变窄”，可以直接在现有软硬件上获得有效加速，但可能带来预测精度 (accuracy) 的下降，需要通过对模型微调 (fine-tuning) 恢复性能。

#### 非结构化剪枝

LeCun 在上世纪 80 年代末提出的 OBD (optimal brain damage) 算法 [19] 使用 loss 对参数求二阶导数判断参数重要程度。在此基础上，Hassibi 等人不再限制于 OBD 算法 [19] 的对角假设，提出 OBS(optimal brain surgeon) 算法 [20]，除了将次重要权重值置 0，还重新计算其他权重值来补偿激活值，压缩效果更好。与 OBS 算法 [20] 类似，Srinivas 等人 [21] 提出删除全连接层稠密的连接，不依赖训练数据，大大减少计算复杂度。最近，Dong 等人 [22] 提出逐层 OBS 算法，每一层都基于逐层 loss 函数对相应参数的二阶导数独立剪枝，修剪后经过轻量再训练恢复性能。

如图 3 所示，卷积层和全连接层的输入与输出之间都存在稠密的连接，对神经元之间的连接的重要性设计评价准则，删除冗余连接可起到模型压缩的目的。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-42-51.png" alt="Model_Compression_Survey-2022-01-12-09-42-51" style="zoom:50%;" /></div>

- Han 等人 [23] 提出根据神经元连接权值的范数值大小，删除范数值小于指定阈值的连接，重新训练恢复性能。
- 为了避免错误删除重要连接，Guo 等人 [24] 提出 DNS(Dynamic Network Surgery) 方法，恢复被误删除的重要连接。
- Lin 等人 [25] 利用生物学上的神经突触概念，定义突触强度为 Batch Normalization (BN) 层放缩因子$\gamma$和 filter 的 Frobinus 范数的乘积，用突触强度来表示神经元之间连接的重要性。
- 不同于其他方法在预训练模型上做剪枝，Lee 等人提出的 SNIP(Single-shot Network Pruning) 方法 [26] 在模型初始化阶段通过对训练集多次采样，判断连接重要性，生成剪枝模板再进行训练，不需迭代进行剪枝-微调的过程。

除了神经元之间的连接进行评估，也可以如图 4 所示，直接对神经元权重进行评估，相比原始权重，3 个 filter 各自进行权重置零操作（即删去某几个小方块），置零的神经元可能各不相同。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-44-22.png" alt="Model_Compression_Survey-2022-01-12-09-44-22" style="zoom:50%;" /></div>

- 行列式点过程 (determinantal point process, DPP)[27] 常用来解决机器学习中的子集选择问题，Mariet 等人 [28] 将 DPP 应用于神经元的选择，再通过重新加权将删除神经元的信息直接融合到剩余神经元中，这种方法不需要再微调模型。
- 受 Kingma 等人提出的变分 dropout 技术 [29] 的启发，Molchanov 等人 [30] 将其用于模型压缩，同时对卷积层和全连接层进行稀疏化。

另外，正则化项作为机器学习中 loss 函数的惩罚项，常用于对某些参数进行限制，所以关于权重参数的正则化项也可以用于惩罚次重要参数的存在，达到模型压缩的目的。

- 由于参数的 L0 范数不可微分，很难与 loss 共同优化，Louizos 等人 [31] 对权重设置非负随机门来决定哪些权重设置为零，转化为可微问题，门上参数可以与原始网络参数共同优化。
- Tartaglione 等人 [32] 量化权重参数对于输出的敏感度，将其作为正则化项，逐渐降低敏感度较低的参数值。
- 延迟、能耗等硬件约束条件也可以作为模型压缩的惩罚项，Chen 等人 [13] 引入硬件约束（例如延迟），使任务目标（如分类精度）最大化，基于权重大小删除范数值较低的权重。
- Yang 等人 [14] 利用加权稀疏投影和输入遮蔽来提供可量化的能耗，将能耗预算作为网络训练的优化约束条件，并且由于手工设置的压缩阈值对网络的自适应性不好，使用能恢复误删除重要连接的动态剪枝法获得稀疏网络。
- Carreira-Perpinán 等人 [33] 提出交替使用“学习”和“压缩”步骤，探索使 loss 最小化的权重子集。
- Liu 等人 [34] 证明卷积可以通过 DCT 域乘法实现，然后对 filter 的 DCT 系数进行动态剪枝。

#### 结构化剪枝

##### group 级别剪枝

如图 4 所示，group 级别剪枝是指对每一层的 filter 设置相同的稀疏模式，（即图中每个立方体都删去相同位置的小方块），变成结构相同的稀疏矩阵。

- Wen 等人 [35] 利用 group lasso 回归进行正则化规约，探索 filter、channel 等不同层次的结构稀疏性。
- Alvarez 等人 [36] 提出不需要预训练模型，加入组稀疏正则化项，在网络训练的同时自动选择各层神经元数目。
- Figurnov 等人 [37] 提出 Perforatedcnns，使用不同策略遮蔽激活值，被遮蔽的值用邻近值表示。
- Lebedev 等人 [38] 利用 [19] 提出的 OBD 算法，将卷积操作视作矩阵乘法计算，以 group 方式稀疏化卷积核， 变为稀疏矩阵乘法，提高运算速度。
- Zhou 等人 [39] 提出引入稀疏约束，减少最后一个全连接层的参数数量。

##### filter 级别剪枝

filter 级别剪枝也可以看作 channel 级别剪枝。如图 4 所示，删去该层的某些 filter，（即图中删去整个立方体），相当于删去其产生的部分 feature map 和原本需要与这部分 feature map 进行卷积运算的下一层部分 filter。对 filter 的评价准则可分为以下四种。

###### 基于 filter 范数大小

- Li 等人 [40] 提出计算 filter 的 L1 范数，过滤掉较小 L1 范数的 filter 对应的 feature map，剪枝后再训练；
- Yang 等人 [15] 利用 Chen 等人的工作 [41] 提出的模型能耗工具 Eyeriss 计算每一层能耗，对能耗大的层优先剪枝，同时为了避免不正确的剪枝，保留剪枝后精确度下降最大的权重；
- Yang 等人的另一篇工作 [42] 提出的 Netadapt 同样也是将硬件度量指标（延迟和能耗等）作为剪枝评价准则，但与 [15] 不同的是使用经验度量来评估，不需要对平台有详细了解；
- 算法在移动平台上自动迭代对预训练网络进行剪枝，直到满足资源预算；
- He 等人 [43] 提出设置剪枝概率删去 L2 范数最小的几个卷积核，即将该 filter 置 0，特殊之处是每次训练完一个 epoch 进行剪枝，但在上一个 epoch 中被剪枝的 filter 在当前 epoch 训练时仍然参与迭代。

###### 自定义 filter 评分因子

- Hu 等人 [17] 提出 Network trimming 方法，他们认为激活值为 0 的神经元是冗余的，所以统计每一个 filter 中激活值为 0 的数量，将其作为判断一个 filter 是否重要的标准；
- Liu 等人 [44] 根据 BN 层放缩因子 $\gamma$ 判断 channel 重要性；
- Huang 等人的工作 [45] 可以看作是 [44] 的泛化，引入额外的放缩因子对 channel 评价；
- Ye 等人 [46] 在 [45] 基础上进行优化，提出基于 ISTA 和重标技术的梯度学习算法；
- Dai 等人 [47] 提出基于变分信息瓶颈剪枝方法，在每一层只提取与任务相关的信息，将冗余神经元的激活值推向零；
- He 等人 [48] 利用强化学习 (reinforcement learning) 提供压缩策略，相比手动启发式方法效果更好。

###### 最小化重建误差

设神经网络中某一卷积层权重为 W，通道数为 C，输入为 X，输出为 Y，忽略偏置项 B。则：

$$
Y=\sum\limits_{c=1}^{C} \sum\limits_{k_1=1}^{K_1} \sum\limits_{k_2=1}^{K_2} W_{c,k_1,k_2} * C_{c,k_1,k_2}
$$

令：

$$
\hat{X}_c=\sum\limits_{k_1=1}^{K_1} \sum\limits_{k_2=1}^{K_2} W_{c,k_1,k_2} * C_{c,k_1,k_2}
$$

则：

$$
Y=\sum\limits_{c=1}^{C}\hat{X}_c
$$

令 S 作为从 C 个通道中取得的最优子集，裁剪过程其实就是使子集 S 的最终输出与原始 C 个通道的最终输出 Y 的差别最小。即：

$$
\mathop{\arg\min}\limits_{S}(Y-\sum\limits_{j \in S}\hat{X}_j)
$$

- Luo 等人 [49] 提出 Thinet，贪婪地剪去对下一层激活值影响最小的 channel；
- He 等人 [50] 并没有像 [49] 那样直接使用贪心策略，而是通过 Lasso 回归对 channel 进行选择性删除，然后利用最小二乘法重构 feature map；
- Yu 等人 [51] 定义最后一个与 softmax 层相连的 hidden layer 为 final response layer(FRL)，通过特征选择器来确定各个特征的重要性得分，反向传播得到整个网络各层的得分，再根据裁剪比率进行裁剪，裁剪原则是 FRL 输出的重建误差最小；
- Zhuang 等人 [52] 引入额外的识别感知 loss 辅助选择真正有助于识别的 channel，联合重建误差共同优化。

###### 其他方法

- Molchanov 等人 [53] 将剪枝问题当作一个优化问题，从权重参数中选择一个最优组合使得 loss 的损失最小，认为剪枝后预测精度衰减小的参数是不重要的；
- Lin 等人 [54] 工作的独特之处是能全局评估各个 filter 的重要度，动态地和迭代地剪枝，并且能重新调用之前迭代中错误剪枝的 filter；
- Zhang 等人 [55] 将剪枝问题视为具有组合约束条件的非凸优化问题，利用交替方向乘法器 (ADMM) 分解为两个子问题，可分别用 SGD 和解析法求解；
- Yang 等人 [16] 相比 [55] 加入能耗作为约束条件，通过双线性回归函数进行建模。

### 2. 参数量化

参数量化是指用较低位宽表示典型的 32 位浮点网络参数，包括权重、激活值、梯度和误差等等，可以使用统一的位宽（如 16-bit、8-bit、2-bit 和 1-bit 等），也可以根据经验或一定策略自由组合不同的位宽。

参数量化的优点在于

1. 能够显著减少参数存储空间与内存占用空间，将参数从 32 位浮点型量化到 8 位整型能够减少 75%的存储空间，这对于计算资源有限的边缘设备和嵌入式设备进行深度学习模型的部署和使用都有很大帮助；
2. 能够加快运算速度，减少设备能耗。读取 32 位浮点数所需的带宽可以同时读入 4 个 8 位整数，并且整型运算相比浮点型运算更快，自然能降低设备功耗

但其仍存在一定的局限性，网络参数的位宽减少损失一部分信息量，造成推理精度下降，虽然能通过微调恢复部分精确度，但带来时间成本；量化到特殊位宽时，很多现有的训练方法和硬件平台不再适用，需要设计专用的系统架构，灵活性不高。

#### 二值化

二值化是指限制网络参数取值为 1 或-1，极大降低模型对存储空间和内存空间的需求，并且将原来的乘法操作转化成加法或者移位操作，显著提高运算速度。但是也带来训练难度和精度下降的问题。

##### 二值化权重

由于权重占据网络参数的大部分，一些研究者提出对网络权重进行二值化，达到压缩网络的目的。

- Courbariaux 等人 [56] 提出 Binaryconnect，将二值化策略用于前向计算和反向传播，但在使用随机梯度更新法 (SGD) 更新参数时仍使用较高位宽；
- Hou 等人 [57] 提出一种直接考虑二值化权重对 loss 的影响的二值化算法，采用对角海森近似的近似牛顿算法得到二值化权重；
- Xu 等人 [58] 提出局部二值卷积 (LBC) 来替代传统卷积，LBC 由一个不可学习的预定义 filter，一个非线性激活函数和一部分可以学习的权重组成，其组合达到与激活的传统卷积 filter 相同效果；
- Guo 等人 [59] 提出 Network sketching 方法，使用二值权重共享的卷积，即对于同层的卷积运算（即拥有相同输入），保留前一次卷积的结果，卷积核的相同部分直接复用结果；
- McDonnell 等人 [60] 将符号函数作为实现二值化的方法；
- Hu 等人 [61] 通过哈希将数据投影到汉明空间，将学习二值参数的问题转化为一个在内积相似性下的哈希问题。

##### 二值化权重和激活值

在二值化网络权重的基础上，研究人员提出可以同时二值化权重和激活值，加快推理速度。

- Courbariaux 等人 [62] 首先提出 Binarized Neural Network(BNN)，将权重和激活值量化到±1。
- Rastegari 等人 [63] 在 [62] 基础上提出 Xnor-net，将卷积通过 Xnor 和位操作实现，从头训练一个二值化网络。
- Li 等人 [64] 在 Xnor-net[63] 基础上改进其激活值量化，提出 High-Order Residual Quantization(HORQ) 方法。
- Liu 等人 [65] 提出 Bi-real net，针对 Xnor-net[63] 进行网络结构改进和训练优化。
- Lin 等人 [66] 提出 ABC-Net，用多个二值操作加权来拟合卷积操作。

#### 三值化

三值化是指在二值化的基础上引入零作为第三阈值，减少量化误差。

- Li 等人 [67] 提出三元权重网络 TWN，将权重量化为{-w,0,+w}；
- 不同于传统的 1 或者权重均值，Zhu 等人 [68] 提出 Trained ternary quantization (TTQ)，使用两个可训练的全精度放缩系数，将权重量化到{-wn,0,wp}，权重不对称使网络更灵活。
- Achterhold 等人 [69] 提出 Variational network quantization(VNQ)，将量化问题形式化为一个变分推理问题，引入量化先验，最后可以用确定性量化值代替权值。

### 聚类量化

参数数量庞大时，可利用聚类方式进行权重量化。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-46-43.png" alt="Model_Compression_Survey-2022-01-12-09-46-43" style="zoom:50%;" /></div>

- Gong 等人 [70] 最早提出将 k-means 聚类用于量化全连接层参数，如图 5 所示，对原始权重聚类形成码本，为权值分配码本中的索引，所以只需存储码本和索引，不再需要存储原始权重信息。
- Wu 等人 [71] 将 k-means 聚类拓展到卷积层，将权值矩阵划分成很多块，再通过聚类获得码本，并提出一种有效的训练方案抑制量化后的多层累积误差。
- Choi 等人 [72] 分析了量化误差与 loss 的定量关系，确定海森加权失真测度是量化优化的局部正确目标函数，提出基于海森加权 k-means 聚类的量化方法。
- Xu 等人 [73] 提出分别针对不同位宽的 Single-level Network Quantization (SLQ) 和 Multi-level Network Quantization (MLQ) 两种方法，SLQ 方法针对高位宽，利用 k-means 聚类将权重分为几簇，依据量化 loss 将簇分为待量化组和再训练组，待量化组的每个簇用簇内中心作为共享权重，剩下的参数再训练。而 MLQ 方法针对低位宽，不同于 SLQ 方法一次量化所有层，MLQ 方法采用逐层量化的方式。

#### 混合位宽

##### 手工固定

由于二值网络会降低模型的表达能力，研究人员提出可以根据经验手工选定最优的网络参数位宽组合。

- Lin 等人 [74] 在 BNN[62] 基础上，提出把 32-bit 权重概率性地转换为二元和三元值的组合。
- Zhou 等人 [75] 提出 DoReFa-Net，将权重和激活值分别量化到 1-bit 和 2-bit。
- Mishra 等人 [76] 提出 WRPN，将权重和激活值分别量化到 2-bit 和 4-bit。
- Köster 等人 [77] 提出的 Flexpoint 向量有一个能动态调整的共享指数，证明 16 位尾数和 5 位共享指数的 Flexpoint 向量表示在不修改模型及其超参数的情况下，性能更优。
- Wang 等人 [78] 使用 8 位浮点数进行网络训练，部分乘积累加和权重更新向量的精度从 32-bit 降低到 16-bit，达到和 32-bit 浮点数基线相同的精度水平。 

除了权重和激活值，研究者们将梯度和误差也作为可优化的因素。这些同时考虑权重、激活值、梯度和误差的方法的量化位数和特点对比见表 4。表中的 W,A,G 和 E 分别代表权重、激活值、梯度和误差。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-48-04.png" alt="Model_Compression_Survey-2022-01-12-09-48-04" style="zoom:50%;" /></div>

##### 自主确定

由于手工确定网络参数位宽有一定局限性，可以设计一定策略，帮助网络选择合适的位宽组合。

- Khoram 等人 [18] 迭代使用 loss 的梯度来确定每个参数的最优位宽，使得只有对预测精度重要的参数才有高精度表示。
- Wang 等人 [84] 提出两步量化，先量化激活值再量化权重。针对激活值量化，提出稀疏量化方法。对于权重量化， 将其看成非线性最小二乘回归问题。
- Faraone 等人 [85] 提出基于梯度的对称量化方法 SYQ，设计权值二值化或三值化，并在 pixel 级别、row 级别和 layer 级别定义不同粒度的缩放因子估计网络权重，至于激活值则量化为 2-bit 到 8-bit 的定点数。
- Zhang 等人 [86] 提出 Learned Quantization(LQ-Nets)，使量化器可以与网络联合训练，自适应学习最佳量化位宽。

#### 训练技巧

由于量化网络的网络参数不是连续的数值，所以不能像普通的卷积神经网络那样直接使用梯度下降方法进行训练，需要特殊的方法对这些离散的参数值进行处理，使其不断优化，最终实现训练目标。

- Zhou 等人 [87] 提出一种增量式网络量化方法 INQ，先对权重进行划分，将对预测精度贡献小的权重划入量化组，最后通过再训练恢复性能。
- Cai 等人 [88] 提出 Halfwave Gaussian Quantizer (HWGQ) 方法，设计两个 ReLU 非线性逼近器（前馈计算中的半波高斯量化器和反向传播的分段连续函数），训练低精度的深度学习网络。
- Leng 等人 [89] 提出利用 ADMM[90] 解决低位宽网络训练问题。
- Zhuang 等人 [91] 针对低位宽卷积神经网络提出三个训练技巧以得到较高精度。
- Zhou 等人 [92] 提出一种显式的 loss-error-aware 量化方法，综合考虑优化过程中的 loss 扰动和权值近似误 差，采用增量量化策略。
- Park 等人 [93] 提出价值感知量化方法来降低训练中的内存成本和推理中的计算/内存成本，并且提出一种仅在训练过程中使用量化激活值的量化反向传播方法。
- Shayer 等人 [94] 展示如何通过对局部再参数化技巧的简单修改来实现离散权值的训练，该技巧以前用于训练高斯分布权值。
- Louizos 等人 [95] 引入一种可微量化方法，将网络的权值和激活值的连续分布转化为量化网格上的分类分布，随后被放宽到连续代理，可以允许有效的基于梯度的优化。

### 3. 低秩分解

神经网络的 filter 可以看作是四维张量宽度 w，高度 h，通道数 c，卷积核数 n，由于 c 和 n 对网络结构的整体影响较大，所以基于卷积核 (w*h) 矩阵信息冗余的特点和其低秩特性，可以利用低秩分解方法进行网络压缩。

低秩分解是指通过合并维数和施加低秩约束的方式稀疏化卷积核矩阵，由于权值向量多分布在低秩子空间，所以可以用少数的基向量来重构卷积核矩阵，达到减少存储空间的目的。低秩分解方法在大卷积核和中小型网络上有不错的压缩和加速效果，过去的研究已经比较成熟，但近两年不再流行了。原因在于除了矩阵分解操作成本高，逐层分解不利于全局参数压缩，需要大量的重新训练才能达到收敛等问题之外，近两年提出的新网络越来越多的采用 1*1 卷积，这种小卷积核不利于低秩分解方法的使用，很难实现网络压缩与加速。

#### 二元分解

- Jaderberg 等人 [96] 将 w\*h 的卷积核分解为 w\*1 和 1\*h 的卷积核，学习到的字典权重线性组合重构，得到输出 feature map。
- Liu 等人 [97] 使用两阶段分解法研究 filter 的通道间和通道内冗余。
- Tai 等人 [98] 提出一种计算低秩张量分解的新算法，利用 BN 层转换内部隐藏单元的激活。
- Masana 等人 [99] 主要解决在大数据集上训练的网络在小目标域的使用问题，证明在压缩权重时考虑激活统计量会导致一个具有闭型解的秩约束回归问题。
- Wen 等人 [100] 提出 Force Regularization，将更多权重信息协调到低秩空间中。
- Wang 等人 [101] 提出定点分解，再通过伪全精度权重复原，权重平衡和微调恢复性能。
- 与其他基于 filter 空间或信道数的低秩分解算法不同，Peng 等人 [102] 的工作基于 filter 组近似，达到降低参数冗余的目的。
- Qiu 等人 [103] 提出将 filter 分解为带预固定基的截断展开，展开系数从数据中学习。
- Novikov 等人 [104] 提出 Tensor Train 分解来压缩全连接层的稠密权值矩阵，而 Garipov 等人 [105] 将其推广到卷积层。
- Wang 等人 [106] 提出 Tensor Ring 分解用于压缩卷积层和全连接层。

#### 多元分解

对 filter 的二元分解会引入 $w*h*c*d$ 张量和 $d*n 张量，由于第一个张量 $w*h*c*d$ 很大并且耗时，提出三元分解对其进行分解。

- Kim 等人 [107] 提出 Tucker 分解，对第一个张量沿输入通道维进行二元分解，得到 $w*1$,$1*h$ 和 $1*1$ 的卷积。
- 由于第二个分量 $d*n$ 也需要大量计算，但其在输入和输出通道维数上的秩已经很低，Wang 等人 [108] 提出基于低秩和群稀疏分解的块项分解 (BTD)。用一些较小的子张量之和近似原始权重张量。
- 在三元分解的基础上，Lebedev 等人 [109] 提出 CP 分解即位 tensor 分解，将 4 维卷积核分解成 4 个 $1*1$、$w*1$、$1*h$ 和 $1*1$ 的卷积，即将一层网络分解为五层低复杂度的网络层。

### 4. 参数共享

参数共享是指利用结构化矩阵或聚类等方法映射网络参数，减少参数数量。参数共享方法的原理与参数剪枝类似，都是利用参数存在大量冗余的特点，目的都是为了减少参数数量。但与参数剪枝直接裁剪不重要的参数不同，参数共享设计一种映射形式，将全部参数映射到少量数据上，减少对存储空间的需求。

由于全连接层参数数量较多，参数存储占据整个网络模型的大部分，所以参数共享对于去除全连接层冗余性能够发挥较好效果，也由于其操作简便，适合与其他方法组合使用。但其缺点在于不易泛化，如何应用于去除卷积层的冗余性仍是个挑战，并且对于结构化矩阵这一常用映射形式，很难为权值矩阵找到合适的结构化矩阵，并且理论依据不充足。

#### 循环矩阵

如果一个大小为 m\*n 的矩阵能用少于 m\*n 个参数来描述，这个矩阵就是一个结构化矩阵。循环矩阵作为结构化矩阵的一种，是参数共享法常采用的一种映射形式。令向量 $r=(r_0,r_1,\cdots,r_{d-1})$

循环矩阵的每一行都是上一行的各元素依次右移一个位置得到，即：

$$
R=cir(r)=\left[\begin{matrix} r_0 &r_1 &\cdots &r_{d-2} &r_{d-1} \\ r_{d-1} &r_0 &\cdots &r_{d-3} &r_{d-2} \\ \vdots &\vdots&\ddots&\vdots&\vdots\\ r_2 &r_3 &\cdots &r_0&r_1\\ r_1 &r_2 &\cdots &r_{d-1}&r_0\end{matrix} \right]
$$

- Cheng 等人 [110] 提出用循环投影代替传统的线性投影。对于具有 d 个输入节点和 d 个输出节点的神经网络层，将时间复杂度从 O(d2) 降低到 O(d*log d)，空间复杂度从 O(d2) 降低到 O(d)。
- Wang 等人 [111] 利用循环矩阵来构造特征图，对 filter 进行重新配置，建立从原始输入到新的压缩特征图的映射关系。
- Sindhwani 等人 [112] 提出一个统一的框架来学习以低位移秩 (LDR) 为特征的结构参数矩阵。
- Zhao 等人 [113] 证明具有 LDR 权值矩阵的神经网络，在保持较高精度的同时，可以显著降低空间和计算复杂度。
- Le 等人 [114] 提出 Fastfood 变换，通过一系列简单矩阵的乘法来代替稠密矩阵与向量的乘积，这些简单矩阵通过规则一次生成，后面无需调整。
- Yang 等人 [115] 在 [114] 基础上提出自适应 Fastfood 变换，对全连接层的矩阵-向量乘法进行重新参数化，替换成 Fastfood 层。

#### 聚类共享

- Chen 等人 [116, 117] 使用哈希函数将网络参数随机分组到哈希桶中，同一个桶的参数共享一个通过标准反向传播学习到的值。
- Wu 等人 [118] 提出对权值进行 k-means 聚类，并引入一种新的频谱松弛的 k-means 正则化方法。
- Son 等人 [119] 将 k-means 聚类应用于 3\*3 卷积核，一个 filter 用放缩因子*聚类中心表示。

#### 其他方法

- Reagen 等人 [120] 提出有损权值编码方案 Bloomier filter，以引入随机误差为代价来节省空间，利用神经网络 的容错能力进行再训练。
- Havasi 等人 [121] 提出放松权重决定论，使用权重上的全变分分布，实现更有效的编码方案，提高压缩率。
- Jin 等人 [122] 提出 Weight Sampling Network (WSNet)，沿着空间维度和通道维度进行加权采样。
- Kossaifi 等人 [123] 提出 Tensorized-Network (T-Net)，使用单个高阶张量来参数化表示整个网络。

### 5. 紧凑网络

以上四种利用参数冗余性减少参数数量或者降低参数精度的方法虽然能精简网络结构，但往往需要庞大的预训练模型，在此基础上进行参数压缩，并且这些方法大都存在精确度下降的问题，需要微调来提升网络性能。

设计更紧凑的新型网络结构是一种新兴的网络压缩与加速理念，构造特殊结构的 filter、网络层甚至网络，从头训练，获得适宜部署到移动平台等资源有限设备的网络性能，不再需要像参数压缩类方法那样专门存储预训练模型，也不需要通过微调来提升性能，降低时间成本，具有存储量小，计算量低和网络性能好的特点。但其缺点在于由于其特殊结构很难与其他的压缩与加速方法组合使用，并且泛化性较差，不适合作为预训练模型帮助其他模型训练。

#### 卷积核级别

##### 新型卷积核

- Iandola 等人 [124] 提出 SqueezeNet，使用 1\*1 卷积代替 3\*3 卷积，为减少 feature map 的数量，将卷积层转变成两层：squeeze 层和 expand 层，减少池化层。
- Howard 等人 [125] 提出 MobileNet，将普通卷积拆分成 depth-wise 卷积和 point-wise 卷积，减少乘法次数。
- Sandler 等人 [126] 提出的 MobileNetV2 相比 MobileNet[125]，在 depth-wise 卷积之前多加了一个 1*1 expand 层提升通道数，获得更多特征。
- Zhang 等人 [127] 提出 ShuffleNet，为克服 point-wise 卷积的昂贵成本和通道约束，使用逐点组卷积 (point-wise group convolution) 和通道混洗 (channel shuffle)。
- Ma 等人 [128] 提出的 ShuffleNetV2 相比 ShuffleNet[127]，为减少内存访问成本，提出通道分割 (channel split)。
- Zhang 等人 [129] 提出交错组卷积 (IGC)，引入第二次组卷积，其每组输入通道来自于第一次组卷积中不同的组，从而与第一次组卷积交替互补。
- Xie 等人 [130] 在 [129] 基础上泛化，提出交错的稀疏化组卷积，将两个结构化稀疏卷积核组成的构建块扩展到多个。
- Wan 等人 [131] 提出完全可学习的组卷积模块 (FLGC)，可以嵌入任何深度神经网络进行加速。
- Park 等人 [132] 提出直接稀疏卷积，用于稠密的 feature map 和稀疏的卷积核之间的卷积操作。
- Zhang 等人 [133] 证明高性能的直接卷积在增加线程数时性能更好，消除所有内存开销。

##### 简单 filter 组合

- Ioannou 等人 [134] 提出从零开始学习一组小的不同空间维度的基 filter，在训练过程中将这些基 filter 组合成更复杂的 filter。
- Bagherinezhad 等人 [135] 提出对每层构建一个字典，每个 filter 由字典中的某些向量线性组合得到。将输入向量和整个字典里的向量进行卷积，查表得到该输入向量和 filter 的卷积结果。
- Wang 等人 [136] 提出构建高效 CNN 的通用 filter，二级 filter 从主 filter 中继承，通过整合从不同感受域提取的信息来增强性能。

#### 层级别

- Huang 等人 [137] 提出随机深度用于类似 ResNet 含残差连接的网络的训练，对于每个 mini-batch，随机删除 block 子集，并用恒等函数绕过它们。
- Dong 等人 [138] 为每个卷积层配备一个低成本协同层 (LCCL) 预测哪些位置的点经过 ReLU 后会变成 0，测试时忽略这些位置的计算。
- Li 等人 [139] 将网络层分为权重层（如卷积层和全连接层）和非权重层（如池化层、ReLU 层等），提出将非权重层与权重层进行合并的方法，去除独立的非权重层后，运行时间显著减少。
- Prabhu 等人 [140] 使用同时稀疏且连接良好的图来建模卷积神经网络 filter 之间的连接。
- Wu 等人 [141] 通过平移 feature map 的形式取代传统的卷积，从而减小计算量。
- Chen 等人 [142] 引入稀疏移位层 (SSL) 来构造高效的卷积神经网络。在该体系结构中，基本块仅由 1*1 卷积层组成，对中间的 feature map 只进行少量移位操作。

#### 网络结构级别

- Kim 等人 [143] 提出 SplitNet，自动学会将网络层分成多组，获得一个树形结构的网络，每个子网共享底层权重。
- Gordon 等人 [144] 提出 Morphnet，通过收缩和扩展阶段循环优化网络。在收缩阶段，通过稀疏正则化项识别效率低的神经元从网络中去除。在扩展阶段，使用宽度乘数来统一扩展所有层的大小，所以含重要神经元更多的层拥有更多计算资源。
- Kim 等人 [145] 提出嵌套稀疏网络 NestedNet，每一层由多层次的网络组成，高层次网络与低层次网络以 Network In Network (NIN) 方式共享参数，低层次网络学习公共知识，高层次网络学习特定任务的知识。

### 6. 知识蒸馏

知识蒸馏最早由 Buciluǎ等人 [146] 提出，训练了带有伪数据标记的强分类器的压缩模型，复制了原始分类器的输出。

与其他压缩与加速方法只使用需要被压缩的目标网络不同，知识蒸馏法需要两种类型的网络，教师模型和学生模型。预先训练好的教师模型通常是一个大型的神经网络模型，具有很好的性能。如图 6 所示，将教师模型的 softmax 层输出作为 soft target 与学生模型的 softmax 层输出作为 hard target 一同送入 total loss 计算，指导学生模型训练，将教师模型的知识迁移到学生模型中，使学生模型达到与教师模型相当的性能。学生模型更加紧凑 高效，起到模型压缩的目的。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-49-42.png" alt="Model_Compression_Survey-2022-01-12-09-49-42" style="zoom:50%;" /></div>

知识蒸馏法能使深层网络变浅，大大降低计算成本，但也有其局限性。由于使用 softmax 层输出作为知识，所以一般多用于具有 softmax 损失函数的分类任务，在其它任务的泛化性不好；并且就目前来看，其压缩比与蒸馏后的模型性能还存在较大进步空间。

#### 学生模型的网络结构

知识蒸馏法的研究方向之一就是如何为学生模型选择合适的网络结构，帮助学生模型更好地学习教师模型的知识。

- Ba 等人 [147] 提出在保证教师模型和学生模型网络参数数量相同的情况下，设计更浅的学生模型，每一层变得更宽。
- Romero 等人 [148] 与 [147] 观点不同，认为更深的学生模型分类效果更好，提出 Fitnets 使用教师网络的中间层输出 Hints 作为监督信息训练学生网络的前半部分。
- Chen 等人 [149] 提出使用生长式网络结构，以复制的方式重用预训练的网络参数，在此基础上进行结构拓展。
- Li 等人 [150] 与 [149] 观点一致，提出分别从宽度和深度上进行网络生长。
- Crowley 等人 [151] 提出将知识蒸馏与设计更紧凑的网络结构结合，将原网络作为教师模型，将使用简化卷积的网络作为学生模型。
- Zhu 等人 [152] 提出基于原始网络构造多分支结构，将每个分支作为学生网络，融合生成推理性能更强的教师网络。

#### 教师模型的学习信息

除了使用 softmax 层输出作为教师模型的学习信息，一些研究者认为可以使用教师模型中的其他信息帮助知识迁移。

- Hinton 等人 [153] 首先提出使用教师模型的类别概率输出计算 soft target，为了方便计算还引入温度参数。 
- Yim 等人 [154] 将教师模型网络层之间的数据流信息作为学习信息，定义为两层特征的内积。
- Chen 等人 [155] 将教师模型在某一类的不同样本间的排序关系作为学习信息传递给学生模型。

#### 训练技巧

- Czarnecki 等人 [156] 提出 Sobolev 训练方法，将目标函数的导数融入到神经网络函数逼近器的训练中。
- 当训练数据由于隐私等问题对于学生模型不可用时，Lopes 等人 [157] 提出如何通过 extra metadata 解决。
- Zhou 等人 [158] 主要有两点创新，第一不用预训练教师模型，而是教师模型和学生模型同时训练；第二教师模型和学生模型共享网络参数。

#### 其他场景

由于 softmax 层的限制，知识蒸馏法被局限于分类任务的使用场景。但近年来，研究人员提出多种策略使其能应用于其他深度学习场景。

- 在目标检测任务中，Li 等人 [159] 提出匹配 proposal 的方法。
- Chen 等人 [160] 结合使用 [148] 和 [153] 提出的方法，提升多分类目标检测网络的性能。
- 在解决人脸检测任务时，Luo 等人 [161] 提出将更高隐层的神经元作为学习知识，其与类别输出概率信息量相同，但更紧凑。
- Gupta 等人 [162] 提出跨模态迁移知识的做法，将在 RGB 数据集学习到的知识迁移到深度学习的场景中。
- Xu 等人 [163] 提出一种多任务指导预测和蒸馏网络 (PAD-Net) 结构，产生一组中间辅助任务，为学习目标任务提供丰富的多模态数据。

### 7. 混合方式

以上这些压缩与加速方法单独使用时能起到很好效果，但也都存在各自的局限性，组合使用能使他们互为补充。

研究人员通过组合使用不同的压缩与加速方法或者针对不同网络层选取不同的压缩与加速方法，设计一体化的压缩与加速框架，能够获得更好的压缩比与加速效果。

参数剪枝，参数量化，低秩分解和参数共享经常组合使用，极大降低模型的内存需求和存储需求，方便模型部署到计算资源有限的移动平台 [164]。知识蒸馏可以与紧凑网络组合使用，为学生模型选择紧凑的网络结构，在保证压缩比的同时，提升学生模型的性能。

混合方式能够综合各类压缩与加速方法的优势，进一步加强压缩与加速效果，将会是未来在深度学习模型压缩与加速领域的重要研究方向。

#### 组合参数剪枝和参数量化

- Ullrich 等人 [165] 基于 Soft weight sharing 的正则化项在模型再训练过程中实现了参数量化和参数剪枝。
- Tung 等人 [166] 提出参数剪枝和参数量化的一体化压缩与加速框架 Compression Learning by In Parallel Pruning-Quantization (CLIP-Q)。
- 如图 7 所示，Han 等人 [167] 提出 Deep Compression，将参数剪枝、参数量化和哈夫曼编码结合，达到很好的压缩效果，并在其基础上考虑到软硬件的协同压缩设计，提出 Efficient Inference Engine (Eie) 框架 [168]。
- Dubey 等人 [169] 同样利用这三种方法的组合进行网络压缩。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-10-29-11.png" alt="Model_Compression_Survey-2022-01-12-10-29-11" style="zoom:50%;" /></div>

#### 组合参数剪枝和参数共享

- Louizos 等人 [170] 采用贝叶斯原理，通过先验分布引入稀疏性对网络进行剪枝，使用后验不确定性确定最优的定点精度来编码权重。
- Ji 等人 [171] 通过重新排序输入/输出维度进行剪枝，并将具有小值的不规则分布权重聚类到结构化组中，实现更好的硬件利用率和更高的稀疏性。
- Zhang 等人 [172] 不仅采用正则化器鼓励稀疏性，同时学习哪些参数组应该共享一个公共值来显式地识别出高度相关的神经元。

#### 组合参数量化和知识蒸馏

- Polino 等人 [173] 提出加入知识蒸馏 loss 的量化训练方法。有浮点模型和量化模型，用量化模型计算前向 loss，并对其计算梯度，用以更新浮点模型。每次前向计算之前用更新的浮点模型更新量化模型。
- Mishra 等人 [174] 提出用高精度教师模型指导低精度学生模型的训练。有 3 种思路：教师模型和量化后的学生模型联合训练；预训练的教师模型指导量化的学生模型从头开始训练；教师模型和学生模型都进行了预训练，但学生模型已经量化，之后在教师模型的指导下进行微调。

## 压缩效果比较

我们从以上介绍的七种主流网络压缩技术中选出其中一些代表性方法，按照论文中声明的压缩与加速效果进行对比。通过对论文中使用较多的数据集和模型的统计，我们使用 MNIST[175]，CIFAR-10[176] 和 ImageNet[177] 三大常用数据集，在 LeNet、AlexNet、VGG-16、ResNet 等公开深度模型上进行压缩方法测试，比较其压缩效果。

图表中的Δaccuracy = 压缩后模型 accuracy - 原始模型 accuracy ，#Params↓= 原始模型参数量 / 压缩后模型参数量，#FLOPs↓= 原始模型浮点计算次数 / 加速后模型浮点计算次数。weight bits 和 activation bits 分别代表权值和激活值被量化后的表示位数。T-accuracy 代表教师模型的 accuracy，S-accuracy 代表学生模型的 accuracy。T-#Params 代表教师模型的参数数量，S-#Params 代表学生模型的参数数量。

表 5 展示了参数剪枝、紧凑网络、参数共享、知识蒸馏和混合方式五类压缩技术的一些代表性方法使用 MNIST 数据集在 LeNet-5 上的压缩效果，可以看出除了 [157] 造成较大的 accuracy 损失，其他方法的压缩效果都不错，从 accuracy 角度，自适应 fastfood 变换 [115] 效果更好，在达到压缩效果的同时提升 accuracy；从参数压缩量角度，混合方式在 accuracy 轻微下降的情况下都实现了较大压缩比，其中 [169] 效果最好。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-50-47.png" alt="Model_Compression_Survey-2022-01-12-09-50-47" style="zoom:50%;" /></div>

表 6 展示了参数剪枝、紧凑网络、参数共享和混合方式四类压缩技术的一些代表性方法使用 CIFAR-10 数据集在 VGG-16 上的压缩效果，可以看出这四类方法的压缩效果差别比较大。整体来看，结构化剪枝 [40] 效果更好，同时起到网络压缩和加速的效果，accuracy 甚至有些提升。权值随机编码方法 [121] 能实现高达 159x 的参数压缩比，accuracy 略有下降。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-50-56.png" alt="Model_Compression_Survey-2022-01-12-09-50-56" style="zoom:50%;" /></div>

表 7 展示了参数剪枝、紧凑网络、低秩分解、参数共享和混合方式五类压缩技术的一些代表性方法使用 ImageNet 数据集在 AlexNet 上的压缩效果。整体来看，五类方法达到的压缩效果和加速效果比较均衡，accuracy 都略有下降。其中参数剪枝和混合方式能实现更大的压缩比，但低秩分解的加速效果更好。另两类方法都有不同程度的 accuracy 下降。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-51-11.png" alt="Model_Compression_Survey-2022-01-12-09-51-11" style="zoom:50%;" /></div>

表 8 展示了参数剪枝、低秩分解、参数共享和混合方式四类压缩技术的一些代表性方法在使用 ImageNet 数据集在 VGG-16 上的压缩效果。整体的压缩与加速效果都很明显，其中剪枝方法的 accuracy 有略微提升，混合方式达到的压缩比最高，另外两类方法虽然 accuracy 有下降，但加速效果更优秀。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-51-28.png" alt="Model_Compression_Survey-2022-01-12-09-51-28" style="zoom:50%;" /></div>

表 9 展示了参数剪枝、紧凑网络、参数共享和混合方式四类压缩技术的一些代表性方法在使用 ImageNet 数据集在 ResNet-50 上的压缩效果。整体来看，accuracy 下降趋势比较明显，压缩与加速效果不如其他网络在 ImageNet 上好。其中混合方式压缩效果最好，[169] 达到 15.8x 的压缩比，而参数共享法中循环矩阵 [111] 达到最高加速比 5.82x。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-51-36.png" alt="Model_Compression_Survey-2022-01-12-09-51-36" style="zoom:50%;" /></div>

表 10 展示了一些主流量化技术使用 ImageNet 数据集在 AlexNet 上的压缩效果，其中 weight bits 为 1 表示 二值化网络，weight bits 为 2 时表示三值化网络，除此之外还有一些特殊位宽。其中 [89] 的 3{±2}表示权值从{0，±1,±2}中选择，3{±2 ±4}表示权值从{0,±1,±2,±4}中选择。XNOR-Net[63] 虽然能达到比较好的压缩性能，但 accuracy 损失太大。SYQ[85] 在实现权重二值化、三值化的同时将激活值也量化到 8 位，accuracy 几乎没有损失，略有提升。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-57-30.png" alt="Model_Compression_Survey-2022-01-12-09-57-30" style="zoom:50%;" /></div>

表 11 展示了一些主流量化技术使用 ImageNet 数据集在 ResNet-18 上的压缩效果。整体来看，accuracy 的下降程度更大，对权值和激活值的大尺度量化造成不同程度的精度损失，SLQ[73] 和 INQ[87] 将权值量化到 5 位，accuracy 略有提升。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-57-50.png" alt="Model_Compression_Survey-2022-01-12-09-57-50" style="zoom:50%;" /></div>

表 12 展示了一些有代表性的知识蒸馏方法在 MNIST、CIFAR-10、CIFAR-100 和 ImageNet 数据集上的压缩效果。由于使用的教师模型和学生模型的网络结构不同，所以我们将两个模型的 accuracy 和参数数量都展示出来，方便读者对比。可以看出，相比其他方法，知识蒸馏的模型 accuracy 下降更多，压缩比更小。目前来看，未来知识蒸馏在模型压缩与加速领域还有很大发展空间。

<div align=center><img src="/images/Model_Compression_Survey-2022-01-12-09-58-02.png" alt="Model_Compression_Survey-2022-01-12-09-58-02" style="zoom:50%;" /></div>

结论：我们总结的 7 类压缩与加速方法各有利弊，由于实验使用的硬件平台不同，并不能量化地确定孰优孰劣。依据不同的应用场景和现实需要，可以进行方法的选取。例如对于存储有限的嵌入式设备，可以使用非结构化剪枝或者二值、三值量化，大幅减少模型占用的内存大小。对于没有预训练模型的情况，可以考虑紧凑网络法，直接训练网络。对于期望较高压缩比与加速比的应用场景，可以使用混合方式，组合使用几种压缩与加速方法。

## 未来研究方向

截止到目前，深度学习模型压缩与加速技术尚未发展成熟，在实际部署和产品化水平上有很大进步空间。以下介绍几点值得关注与讨论的研究方向。

1. 知识蒸馏作为一种迁移学习的形式，使小模型尽可能多地学习大模型的知识，具有方法灵活，不依赖硬件平台的特点，但目前其压缩比和蒸馏后性能都有待提高。未来知识蒸馏可以以下几个方向研究：打破 softmax 函数的限制，结合中间特征层，使用不同形式的知识；选择学生模型的结构时，可以与其他方法集成；打破任务的限制，例如将图片分类领域的知识迁移到其他领域。
2. 将模型压缩技术与硬件架构设计相结合。目前的压缩与加速方法大多仅从软件层面对模型进行优化，并且不同方法由于使用的硬件平台不同也很难比较其加速效果的好坏，未来可以针对主流的压缩与加速方法专门设计硬件架构，既能在现有基础上加速模型，又方便不同方法的比较。
3. 制定更智能的模型结构选择策略。目前无论是参数剪枝方法或者是设计更紧凑的网络结构，都是基于现有模型作为主干网络，手动选择或使用启发式策略进行结构缩减，降低了模型搜索空间，未来可以利用强化学习等策略进行自动网络结构搜索，得到更优的网络结构。
4. 将模型压缩技术推广到更多任务和更多平台。目前的压缩与加速方法多是为图片分类任务的卷积神经网络模型设计，然而实际应用中还有大量其他模型应用于人工智能领域，例如语音识别和机器翻译领域常使用的循环神经网络 (RNN)，知识图谱领域的图神经网络 (GNN)。为卷积神经网络模型设计的压缩与加速方法能否直接用于 RNN 与 GNN 还需要探索。同时，小型移动平台（如智能手机、机器人、 无人驾驶汽车等）的硬件限制和其有限的计算资源阻碍深度学习模型的直接部署，如何为这些平台设计独有的压缩方法仍是一个巨大挑战。
5. 模型压缩后的安全问题。由于当前压缩与加速方法更注重压缩完成后的任务完成度（例如分类任务 accuracy 是否有降低），忽略了模型压缩可能带来的安全隐患，例如相比原模型是否更容易被对抗样本攻击。所以未来在提升模型性能的同时，也应注意模型是否安全。
