---
title: "SimMIM: A Simple Framework for Masked Image Modeling"
subtitle: "Masked Image Modeling"
layout: post
author: "L Hondong"
header-img: "img/post-bg-28.jpg"
mathjax: true
tags:
  - Transformer
  - 笔记
---

# SimMIM: A Simple Framework for Masked Image Modeling

CVPR 2022

MSRA

## 摘要

SimMIM 是继 BEiT 之后，MSRA 又提出的一个 MIM 任务上的预训练 CV 模型。这个模型更像是 MAE，也是直接回归预测原始像素 RGB 值，而不是像 BEiT 或者 iBOT 一样重建 tokens。

简化了最近提出的方法，而无需任何特殊设计，如利用离散 VAE 或聚类进行块级别的掩码和分词。为了探索是什么让掩码图像模型任务学习到好的表征，本文系统地研究了框架中的主要组成部分，发现每个组成部分的简单设计都显示出很强的表征学习能力：

1. 在 MIM 任务中，mask patch 的大小如果是 32×32，对输入图像进行随机掩码，就能使得预训练任务成为一个 strong pre-text task，非常有利于预训练大模型性能的提升。
2. 直接回归预测原始像素 RGB 值的效果并不比复杂设计的 patch 分类方法差。
3. prediction head 可以设计成轻量化的模型，比如一个线性层，它的表现不比 heavy 模型差。

使用 ViT-B，通过对该数据集进行预训练，该方法在 ImageNet-1K 上实现了 83.8%的 top-1 微调精度，比之前的最佳方法高出 0.6%。将其应用在约 6.5 亿个参数的更大模型时，SwinV2-H 仅使用 ImageNet-1K 数据可以达到 87.1%的 top-1 精度。此外，还利用这种方法来加速 3B 模型 (SwinV2-G) 的训练，通过比以前少 40 倍的数据，在四个具有代表性的视觉基线数据集上达到了 SOTA。

## 一、简介

### 1.1 Motivation

> “What I cannot create, I do not understand.” — Richard Feynman

Masked Signal Learning 任务是指，挡住输入信号的一部分，把残缺的信号送入模型，希望模型能够预测出这些被 masked 掉的信号。也就是说，要想让模型创造出这些被 masked 掉的信号，就得使得模型首先理解它们。

在 NLP 中，掩码语言建模任务的自监督学习方法在很大程度上重塑了这一领域，即通过使用巨大的无标签数据来学习非常大规模的语言模型，并被证明可以广泛推广到 NLP 应用中。而在计算机视觉中，虽然有先驱者利用这一理念进行自监督表示学习，但在发展初期，这方面的工作几乎被对比学习的方法完全掩盖。

对于自监督学习的任务而言，如 MoCo，MoCo v2，MoCo v3，SimCLR，BYOL 等都是采用对比学习的策略完成，属于自监督学习范式中的 Contrastive 系列。

但是，对于图像而言，Contrastive Learning 的做法真的是最好的吗？或者直接把 NLP 领域的 MLM 方法迁移到 CV 领域的 MIM，是可取的吗？根据语言和视觉领域的特性，在二者上导致明显差异的原因，分析出以下三点：

1. 图像具有更强的局部关系，相邻像素之间联系非常紧密 (highly correlated)，所以解决 MIM 问题的好方法可能是**更多地借助或者复制靠近的像素分周围的像素**，而不是更高维的语义信息的推理。
2. 语言和视觉信号的语义高度不同，这点和 MAE 的信息密度的观点是一致的，即：视觉信号是 raw, low-level 的，而文本信号是 high-level 的。那么，预测 low level 信号会有利于 high level 的图像识别问题吗？
3. 视觉信号通常是连续的，而文本信号通常是离散的。

基于以上三点分析，本文提出了 SimMIM 模型，和一周之前的 MAE 在思路上面十分相似，很多具体做法都是一样的，但是在模型设计上给出了不同的见解。不一样的地方是：MAE 重建所有的 patches，不论是 masked 还是 unmasked，MAE 本质上属于 Reconstruction 的任务；而 SimMIM 实验证明重建所有的 patches 的效果不如只重建 masked patches 的效果，SimMIM 本质上属于 Prediction 的任务。

值得注意的是二者是完全同期的工作 （前后只差一周），说明实力很强的大厂对于 CV 大模型的训练和设计都得出了比较相似的结论。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-25-23-01-06.png" alt="SimMIM-2022-04-25-23-01-06" style="zoom:50%;" /></div>

SimMIM 的核心设计如下：

1. 将随机掩码应用到图像块 (patch) 中方便了 Vision Transformer(ViT) 的应用。对于掩码块的像素点而言，更大的 patch 大小或掩码的更多都可能更难找到附近的可见像素来预测自己。当掩码大小为 32 的块（相对大的块）时，可以在较宽的掩码率范围内 (10%-70%) 实现极具竞争力的性能；当掩码大小为 8 的块（相对小的块）时，需要高达 80% 的掩码率才能表现的比较好。这里需要注意，在语言中预设的掩码率与在文本中的有很大的不同，自然语言领域默认掩码率为 0.15，这种差异可能来自于两种模态信息冗余的程度不同。
2. 使用原始像素回归任务。回归任务与具有有序性的视觉信号的连续性很好地吻合。这个简单的任务执行起来并不比使用由分词、聚类或离散化专门定义的分类方法差。
3. 采用极轻量的预测头（如线性层），其迁移性能与较繁琐的预测头（如 inverse Swin-B) 相似或略好。使用轻量级的预测头在预训练中带来了显著的加速。此外，还注意到在广阔范围内（例如 12×12-96×96) 的目标分辨率与最高到 192×192 的分辨率具有竞争力。虽然较大的头或更高的分辨率通常会导致更强的生成能力，但这种更强的能力不一定有利于下游的微调任务。

虽然简单，但提出的 SimMIM 方法用于表示学习非常有效。ViT-B 在 ImageNet1K 上预训练并在该数据集上实现了 83.8%的 top-1 微调精度，比之前的最佳方法的高出了 0.6%。

SimMIM 还被证明可以扩展到更大的模型：SwinV2-H 模型 (658M 参数）在 ImageNet-1K 分类上达到 87.1% 的 top-1 准确率，这是仅使用 ImageNet-1K 数据的方法中最好的表现。证明自监督学习可以解决由于模型容量快速增长而引起的日益严重的数据不足的问题。

在 SimMIM 的帮助下，本文使用比谷歌的 JFT-3B 数据集小约 40 倍的数据成功训练了一个具有 30 亿个参数的 SwinV2-G 模型，并在几个有代表性的基线测试中创造了新的记录：ImageNet-V2 分类上的 top-1 准确率为 84.0%，COCO 目标检测上达到 63.1/54.4 box/mask mAP，ADE20K 语义分割上达到 59.9 mIoU，Kinetics-400 动作识别上的 top-1 准确率为 86.8%。

## 二、相关工作

### 2.1 

### 2.2 

## 三、方法

### 3.1 整体架构

SimMIM 方法通过掩码图像建模来学习表示，该方法对输入图像信号的一部分进行掩码，并预测在掩码区域的原始信号。该框架由 4 个主要组件组成：

1. Masking Strategy
   给定一张输入图像，选择掩码的区域及实现所选区域的掩码。经过掩码后的图像将用作模型输入。
2. Encoder architecture
   编码器提取掩码图像上潜在的特征表示，然后用来预测掩码区域的原始信号。经过学习的编码器可用于不同的视觉任务。在本文中，主要考虑两种典型的 Vision Transformer 架构：vanilla ViT 和 Swin Transformer。
3. Prediction head
   预测头用于潜在的特征表示，表示掩码区域中的原始信号。只要输入与 Encoder 的输出一致，输出达到预测目的，Prediction head 就可以具有任意结构，哪怕是一个 Linear Layer。
4. Prediction target
   定义了要预测的原始信号的形式。可以是原始像素值，也可以是原始像素的变换。另外，定义了损失类型，包括交叉熵分类损失和 L1 或 L2 回归损失。

### 3.2 Masking Strategy

SimMIM 的 Masking Strategy 和 MAE 不同，MAE 采取的做法是直接扔掉被 mask 的 patches，但是 SimMIM 采取的做法和 BEiT，BERT 一致，即把 mask 的 patches 替换成可学习的 mask token vector，并随着网络一起训练。mask 的基本单位仍然是 Image Patches，对于 ViT 模型，masked patch size 使用 32×32；对于 Swin Transformer 模型，masked patch size 使用 4×4-32×32。

Patch 对齐的随机掩码策略：对于 Swin Transformer，考虑相同的不同分辨率的补丁大小（4×4 ~ 32×32），默认采用 32×32 的补丁大小。对于 ViT，采用 32×32 作为默认掩码补丁大小。

其他掩码策略：

1. 中心区域掩码策略，让其在图像上随机移动；
2. 块级掩码策略，利用分别为 16x16 和 32x32 的两种掩码块进行掩码。

### 3.3 Prediction head

预测头的形式和大小可以是任意的，只要其输入与编码器输出一致，其输出达到预测目标即可。一些早期的工作跟随自编码器使用一个繁琐的预测头（解码器）。本文证明了预测头可以做得非常轻，就像线性层一样轻量。此外还尝试较重的头部，如 2 层 MLP、inverse Swin-T，inverse Swin-B。

### 3.4 Prediction target

像素值在颜色空间中是连续的。一个直接的想法是通过回归来预测掩码区域的原始像素。一般来说，视觉架构通常生成下采样分辨率的特征图，例如，ViT 为 16×，其他架构为 32×。

为了预测输入图像的全分辨率下的所有像素值：

1. 将 feature map 中的每个特征向量映射回原始分辨率，并让该向量负责相应的原始像素的预测。
   例如，对于 Swin Transformer 编码器生成的 32× 下采样的 feature 3072 = 32×32×3 的 1×1 卷积（线性）层来表示 32×32 像素的 RGB 值。对原始图像分别进行{32×， 16×， 8×， 4×， 2×}下采样，考虑分辨率较低的目标。
2. 在掩码像素上使用 L1-loss。在实验中还考虑了 L2 和 smooth-L1 损失效果与之类似，默认采用 L1 损失：

$$
L = \frac{1}{\Omega(x_M)}\Vert y_M-x_M\Vert_1
$$

式中，$x,y \in\mathbb R^{3HW\times 1}$ 是输入的 RGB 值和预测值，$M$ 表示掩码像素集合，$\Omega()$ 是 element 的数量。

其他预测目标：

1. Color clustering. 在 iGPT 中，利用大量自然图像，通过 k-means 将 RGB 值分成 512 个簇。然后每个像素被分配到最近的簇中心。这种方法需要一个额外的聚类步骤来生成 9 位调色板。在实验中，作者使用了在 iGPT 中学习到的 512 簇中心。
2. Vision tokenization. 在 BEiT 中，采用离散 VAE (dVAE) 网络将图像 patch 转换为 dVAE tokens。token 可用作为分类目标。在这种方法中，需要预训练一个额外的 dVAE 网络。
3. Channel-wise bin color discretization. 将 R、G、B 通道分别进行分类，每个通道离散为相同的 bins，例如实验中使用的 8 和 256 bins。

### 3.5 Evaluation protocols

使用的评价指标是 Fine-tuning，即在模型最后添加一层线性分类器 Linear Classifier （它其实就是一个 FC 层） 完成分类，同时使用全部 label 训练目标网络 Backbone 部分的权重和分类器的权重。

同时也有 linear probing 的结果，即把目标网络 Backbone 部分的权重冻结，在模型最后添加一层线性分类器 Linear Classifier 完成分类，只训练 Linear Classifier 的参数。

## 四、实验

### 4.1 实验设置

采用 Swin-B 作为消融研究的默认骨干。为了减少实验开销，默认的输入图像大小为 192×192，并将窗口大小调整为 6 以适应改变的输入图像大小。ImageNet-1K 图像分类数据集用于预训练和微调。采用数据增强：随机调整大小裁剪，比例范围为 [0.67,1]，宽高比范围为 [3/4,4/3]，然后进行随机翻转和颜色归一化步骤。SimMIM 组件的默认选项是：一个随机掩码策略，补丁大小为 32×32，掩码率为 0.6；目标图像大小为 192×192 的线性预测头；掩码像素预测的 L1 损失。

- 预训练配置：AdamW，100 epochs，cosine learning rate scheduler，batch size=2048，base lr=8e-4，weight decay=0.05，warmup epochs=10。
- 预训练 Data Augmentation：Random resize cropping，比例范围为 [0.67,1]，宽高比范围为 [3/ 4,4 /3]，Random flipping + Color normalization。
- Fine-tuning 配置：AdamW，100 epochs，cosine learning rate scheduler，batch size=2048，base lr=5e-3，weight decay=0.05，warmup epochs=10，stochastic depth ratio=0.1。
- Fine-tuning Data Augmentation：RandAug，Mixup，Cutmix，Label smoothing，Random erasing。

### 4.2 Masking Strategy 对表征学习的影响

首先研究了不同 masking strategy 对表征学习的影响。最佳的 random masking strategy 使得 Accuracy 达到了 83.0%。此时超参数是 mask patch size=32，mask ratio=50%，即挡住 50%的原图，这个结果比 BEiT 高 0.3%。

当采用较大的掩码块 mask patch size=32 时，mask ratio 在较宽的掩码率范围 (10%-70%) 时都能够取得很不错的结果。作者认为这个实验结果产生的原因是：一个 mask 中心的像素距离边界可见像素是足够远的，因此可以强迫网络学习到一些 long-range 的关系，即使 mask 掉的像素足够多。假设一个大遮罩块的中心像素距离可见像素足够远。因此，即使使用了较低的掩码率（如 10%) 或没有掩码周围的所有补丁，它也会迫使网络学习相对较长的连接。将 mask ratio 由 0.4 增加至 0.8，在 patch size 大小为 4，8 和 16 的情况下，准确率分别提高了 0.2%，0.4%和 0.4%。使用更大的 mask ratio ，这也证明了相对较小的 patch 尺寸有利于微调性能。然而，这些较小的 patch 的总体精度不如较大的 patch(32) 的高。进一步将 patch 大小增加到 64 的观测精度下降，可能是由于预测距离太大。

上述观察和分析也可以很好地反映在一个新提出的 AvgDist 度量，该度量**测量掩码像素到最近的可见像素的平均欧氏距离**。不同掩码策略与不同掩蔽率的 AvgDist 如图 3(a) 所示。从图中可以看出，所有的 masking strategy 的 AvgDist 都随着 masking ratio 的增大而平滑增大。对于随机掩码策略，当 patch size 较小 （如 4 或 8) 时，AvgDist 相对较低，且随着掩码率的增加而增长缓慢。另一方面，当 patch size 较大时 （如 64)，很小的 mask ratio （如 10%) 仍然会产生较大的 AvgDist。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-25-23-39-20.png" alt="SimMIM-2022-04-25-23-39-20" style="zoom:50%;" /></div>

图 3(b) 绘制了微调精度和 AvgDist 度量之间的关系，它遵循山脊 (ridge) 形状。微调精度高的条目大致分布在 AvgDist 的 [10,20] 范围内，而 AvgDist 越小或越高的条目表现越差。这表明掩码图像建模中的预测距离应该适中，既不要太大，也不要太小。

在掩码预测中，AugDist 太小的话，网络可能会学习到太多的短连接，AugDist 太大的话，网络可能会很难学习。这些结果也表明，AvgDist 可能是一个很好的指标用于检测掩码建模的有效性。

实际使用的 mask ratio=0.6，patch size=32。

### 4.3 Prediction Head 对表征学习的影响

下图对比了不同结构的 Prediction head 对结果的影响。虽然通常复杂的预测头产生的损失略低，但在下游 ImageNet-1K 任务的迁移性能较低。依次尝试了 Linear layer，2 层 MLP，inverse 的 Swin-T 和 inverse 的 Swin-B 架构。发现参数量大的 Prediction head 会带来更低的 loss，但是 Top-1 的 Accuracy 反而变低了。这意味着预训练 impainting 的能力并不代表下游任务的能力。这可能是因为优异的性能很大一部分用于预测头，而不会用于下游任务。

复杂的预测头带来较高的训练成本，SimMIM 的单个线性层头在微调度量下已经显示出具有竞争力甚至是最优的迁移性能。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-26-00-01-05.png" alt="SimMIM-2022-04-26-00-01-05" style="zoom:50%;" /></div>

另外，一个有意思的发现是，之前的基于 Contrastive learning 的自监督学习工作，如 MoCo，MoCo v2，MoCo v3，SimCLR 等等，发现 Prediction head 用 2 层的比单层 MLP 好一点，原因是 Prediction head 太深的话会导致 Pre-text 的任务学习到的 latent representation 与下游任务需要的差距过大。所以 SimMIM 发现 Prediction head 就用一层 MLP 就非常好，也意味着 Contrastive learning 任务设计 Prediction head 的方法可能并不适用于 MIM 任务。

### 4.4 Prediction resolution 对表征学习的影响

下图对比了不同的 Prediction resolution 对结果的影响。大范围的分辨率 (12×12-192×192) 都能表现良好。传输性能只有在 6×6 的分辨率的低分辨率下才会下降，可能是因为 6×6 的分辨率丢弃了太多信息。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-25-23-56-12.png" alt="SimMIM-2022-04-25-23-56-12" style="zoom:50%;" /></div>

这些结果反映了下游图像分类任务所需的信息粒度。也告诉我们 MLM 任务的 pre-text 属于分类任务，但是这并不意味着 MIM 任务的 pre-text 的最优选择也是分类任务，比如 MAE 和 SimMIM 的 pre-text 属于回归任务。

### 4.5 Prediction target 对表征学习的影响

对比了不同的 Projection target 对结果的影响。使用 L1 loss，smooth L1 loss，L2 loss 的结果都差不多。

结果表明，掩码图像建模的目标不需要与掩码语言建模的分类目标一致。设计与视觉信号本身的性质保持一致的方法效果很好。

值得注意的是：SimMIM 和 MAE 的另一个很重要的不同是：SimMIM 只重建 masked patches (Prediction)，而 MAE 则重建所有的 patches (Reconstruction)。SimMIM 也做了实验：如果目标设置为重建所有的 patches，则性能略有下降。

比较了两种方法，一种是预测默认设置下的掩码区域，另一种是重建被掩码区域和未被掩码区域。掩蔽区域的预测效果明显优于重建所有图像像素效果。这表明，这两种任务在内部机制上有根本的不同，预测任务可能是一种更有前景的表征学习方法。

### 4.6 与其他方法比较

ImageNet-1k 实验结果

- 预训练实验设置：800 epochs，cosine learning rate scheduler，20 epochs linear warm-up。
- Fine-tuning 实验设置：200 epochs，layer-wise learning rate decay。

实验结果，SimMIM 超过了 DINO 和 BEiT，因为和 MAE，iBOT 是 2021 年 11 月同时期的工作，所以它们之间没有互相对比性能。

比较了 SimMIM 与使用 ViTB 进行微调和线性探测的其他方法。SimMIM 通过微调达到了 83.8% 的 Top-1 准确率，比之前的最佳方法高出了 0.6%。SimMIM 由于其简单性，保留了最高的训练效率，比 DINO、MoCo v3、ViT 和 BEiT（不包括 dVAE 训练前的时间）分别高出 2.0×、1.8×、~ 4.0×和 1.5×。

### 4.7 可伸缩性实验

采用不同模型尺寸的 Swin Transformer 进行实验，包括 Swin-B、Swin-L、SwinV2-H、SwinV2-G。为了减少实验开销，在训练前采用较小的 192×192 的图像尺寸，所有模型都使用 ImageNet-1K 数据集进行训练，但 SwinV2-G 使用更大的 ImageNet-22K-ext 数据集。在微调时，采用较大的 224×224 图像尺寸。对于 SwinV2-H，选用更大的分辨率 512×512。

SimMIM 在不同模型尺寸下的结果，并与有监督的同行进行了比较。在 SimMIM 预训练下，所有的 Swin-B、Swin-L、SwinV2-H 的准确率都显著高于有监督的对照组。另外，分辨率为 512×512 的 SwinV2-H 模型在 ImageNet-1K 上的 top-1 精度达到了 87.1%，是仅使用 ImageNet-1K 数据的方法中精度最高的方法。

### 4.8 可视化结果

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-26-00-06-03.png" alt="SimMIM-2022-04-26-00-06-03" style="zoom:50%;" /></div>

为了探究 SimMIM 模型通过预训练 masked image modeling 任务获得了一种什么样的能力，看到每一行里面的 mask 分为 Random mask，挡住主要物体的 mask，挡住全部主要物体的 mask。结果显示：

1. 如果使用 Random mask，物体的形状和纹理可以得到重建。但是，unmasked 部分因为模型没有学习这部分的重建，导致最终结果出现了很多的 artifacts。
2. 如果挡住主要物体的 mask，模型仍然能够重建出物体的部分。
3. 如果挡住全部主要物体的 mask，则模型就使用背景去填充。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-26-00-06-24.png" alt="SimMIM-2022-04-26-00-06-24" style="zoom:50%;" /></div>

对比了只重建 masked patches (Prediction)，或者重建所有的 patches (Reconstruction) 的结果。每组图片的 1 是原图，2 是 Corrupted image，3 是重建所有的 patches 的复原结果，4 是只重建 masked patches 的复原结果。显然，4 视觉效果更好，说明只重建 masked patches 的效果更好，这个结论是 MAE 没注意到的。

对比了不同大小的 mask patches 的重建结果。注意所有实验 mask ratio=0.6，结果发现当 mask patches 较小时，可以得到更好的重建结果。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SimMIM-2022-04-26-00-07-03.png" alt="SimMIM-2022-04-26-00-07-03" style="zoom:50%;" /></div>

## 五、总结

SimMIM 是继 BEiT 之后，MSRA 又提出的一个 MIM 任务上的自监督预训练 CV 模型。这个模型更像是 kaiming 的 MAE，也是直接回归预测原始像素 RGB 值，而不是像 BEiT 或者 iBOT 一样重建 tokens。和 MAE 作法一致的地方是：

1. 随机 mask image patches。
2. 直接回归预测原始像素 RGB 值。
3. Decoder (Prediction head) 是轻量的模型。

不一样的地方是：MAE 重建所有的 patches，不论是 masked 还是 unmasked，MAE 本质上属于 Reconstruction 的任务；而 SimMIM 实验证明重建所有的 patches 的效果不如只重建 masked patches 的效果，SimMIM 本质上属于 Prediction 的任务。
