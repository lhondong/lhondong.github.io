---
title: "极大似然估计与最大后验概率估计"
subtitle: "先验概率与后验概率"
layout: post
author: "L Hondong"
header-img: "img/post-bg-3.jpg"
mathjax: true
tags:
  - 数学
  - 概率论
---

# 极大似然估计与最大后验概率估计

## 先验概率与后验概率

先验概率是以全事件为背景下，A 事件发生的概率，$P(A\mid\Omega)$。

后验概率是以新事件 B 为背景下，A 事件发生的概率，$P(A\mid B)$。

全事件一般是统计获得的，所以称为先验概率，没有实验前的概率。

新事件一般是实验，如试验 $B$，此时的事件背景从全事件变成了 $B$，该事件 $B$ 可能对 $A$ 的概率有影响，那么需要对 $A$ 现在的概率进行一个修正，从 $P(A\mid\Omega)$ 变成 $P(A\mid B)$，所以称 $P(A\mid B)$ 为后验概率，也就是试验（事件 $B$ 发生）后的概率。

## 极大似然估计与最大后验概率估计

极大似然估计，认为使似然函数最大的参数 $\theta$ 即为最好的 $\theta$，此时将 $\theta$ 看作固定值，只是其值未知。（频率派）

最大后验估计，认为 $\theta$ 是一个随机变量，即 $\theta$ 具有某种概率分布，成为先验分布，求解时除了要考虑似然函数 $p(x\mid\theta)$ 外，还要考虑 $\theta$ 的先验分布 $p(\theta)$。因此认为使 $p(x\mid\theta)p(\theta)$ 取最大值的 $\theta$ 就是最好的 $\theta$。（贝叶斯派)

对于 $p(x\mid\theta)$：

- 概率函数

如果 $\theta$ 是已知确定的，$x$ 是变量，这个函数叫做概率函数 (probability function)，它描述对于不同的样本点 $x$，其出现概率是多少（表示不同 $x$ 出现的概率）。

概率函数属于已知模型和参数对时间进行预测分析。概率函数用于在已知一些参数的情况下，预测接下来的观测所得到的结果。

- 似然函数

如果 $x$ 是已知确定的，$\theta$ 是变量，这个函数叫做似然函数 (likelihood function), 它描述对于不同的模型参数\theta，出现 x 这个样本点的概率是多少（表示不同 $\theta$ 下，$x$ 出现的概率）。此时的函数也记作 $L(\theta\mid x)$ 或 $L(x;\theta)$。

似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性（可能性）。就是给定一组观测数据，对有关事物的性质的参数进行估计，即已知具体样本数据，对于模型的参数进行分析预测。

概率：参数 + 观测 → 结果

似然：观测 + 结果 → 参数

贝叶斯公式：

$$
p(\theta\mid X)=\frac{p(X\mid \theta)p(\theta)}{p(X)}
$$

$$
posterior=\frac{likehood \times prior}{evidence}
$$

**posterior：通过样本  $X$ 得到参数 $\theta$ 的概率，也就是后验概率 $p(\theta\mid X)$。**

**likehood：通过参数 $\theta$ 到样本 $X$ 的概率，似然函数 $p(X\mid \theta)$，通常就是我们的数据集的表现。**

**prior：参数 $\theta$ 的先验概率，一般是根据人的先验知识来得出的。比如人们倾向于认为抛硬币实验会符合先验分布：beta 分布。当我们选择 beta 分布的参数 $\alpha=\beta=0.5$ 时，代表人们认为抛硬币得到正反面的概率都是 0.5。**

**evidence：样本 X 发生的概率，是各种 $\theta$ 条件下发生的概率的积分（如果离散则就是求和）。**

$$
p(X)=\int p(X\mid \theta)p(\theta)d\theta
$$

## 频率学派与贝叶斯派

在说极大似然估计（Maximum Likelihood Estimate）与最大后验概率估计（Maximum A Posteriori estimation）之前，不得不说对于概率看法不同的两大派别频率学派与贝叶斯派。他们看待世界的视角不同，导致他们对于产生数据的模型参数的理解也不同。

### 1. 频率学派

他们认为世界是确定的。他们直接为事件本身建模，也就是说事件在多次重复实验中趋于一个稳定的值 $p$，那么这个值就是该事件的概率。

他们认为模型参数是个定值，希望通过类似解方程组的方式从数据中求得该未知数。这就是频率学派使用的参数估计方法：**极大似然估计（MLE）** ，这种方法往往在大数据量的情况下可以很好的还原模型的真实情况。

### 2. 贝叶斯派

他们认为世界是不确定的，因获取的信息不同而异。假设对世界先有一个预先的估计，然后通过获取的信息来不断调整之前的预估计。他们不试图对事件本身进行建模，而是从旁观者的角度来说。因此对于同一个事件，不同的人掌握的先验不同的话，那么他们所认为的事件状态也会不同。

他们认为模型参数源自某种潜在分布，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异。这就是贝叶斯派视角下用来估计参数的常用方法：**最大后验概率估计（MAP）** ，这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，相反真实的数据样例会大大占据有利地位。极端情况下，比如把先验假设去掉，或者假设先验满足均匀分布的话，那它和极大似然估计就如出一辙了。

## 极大似然估计与最大后验概率估计

我们这有一个任务，就是根据已知的一堆数据样本，来推测产生该数据的模型的参数，即**已知数据，推测模型和参数**。因此根据两大派别的不同，对于模型的参数估计方法也有两类：极大似然估计与最大后验概率估计。

### 极大似然估计（MLE）

频率学派模型参数估计的常用方法。顾名思义：似然，可以简单理解为概率、可能性，也就是说要最大化该事件发生的可能性。

含义：根据已知样本，希望通过调整模型参数来使得模型能够最大化样本情况出现的概率。

最大似然估计是一种“模型已定，参数未知”的方法，即利用已知的样本的结果，在使用某个模型的基础上，反推最有可能导致这样结果的模型参数值。

最大似然估计的思想：使得观测数据（样本）发生概率最大的参数就是最好的参数。

通俗的说就是 —— 最像估计法（最可能估计法），即概率最大的事件，最可能发生。

极大似然估计是典型的频率学派观点，它的基本思想是：待估计参数 $\theta$ 是客观存在的，只是未知而已，当 $\theta_{MLE}$ 满足 $\theta = \theta_{MLE}$ 时，该组观测样本 $(X_1,X_2,...,X_n) = (x_1, x_2,...,x_n) 更容易被观测到，我们就说  $ $\theta_{MLE}$ 是 $\theta$ 的极大似然估计值。也即，估计值 $\theta_{MLE}$ 使得事件发生的可能性最大。

$$
p(X\mid \theta)=\prod\limits_{x_1}^{x_n}p(x_i\mid \theta)\\=\sum\limits_{x_1}^{x_n}\log p(x_i\mid \theta)
$$

在这举个猜黑球的例子：假如一个盒子里面有红黑共 10 个球，每次有放回的取出，取了 10 次，结果为 7 次黑球，3 次红球。问拿出黑球的概率 $p$ 是多少？

- 我们假设 7 次黑球，3 次红球为事件 $A$，一个理所当然的想法就是既然事件 $A$ 已经发生了，那么事件 $A$ 发生的概率应该最大。所以既然事件 $A$ 的结果已定，我们就有理由相信这不是一个偶然发生的事件，这个已发生的事件肯定一定程度上反映了黑球在整体中的比例。所以我们要让模型产生这个整体事件的概率最大，我们把这十次抽取看成一个整体事件 $A$，很明显事件 $A$ 发生的概率是每个子事件概率之积。我们把 $P(A)$ 看成一个关于 $p$ 的函数，求 $P(A)$ 取最大值时的 $p$，这就是极大似然估计的思想。
- 确定似然函数
- 将似然函数转换为对数似然函数
- 求对数似然函数的最大值（求导，解似然方程）

具体公式化描述为 $P(A) = p^7 \times (1-p)^3$，接下来就是取对数转换为累加，然后通过求导令式子为 0 来求极值，求出 $p$ 的结果。

取对数有：$\ln(P(A) = \ln(p^7 \times (1-p)^3) = 7\ln(p)+3\ln(1-p)$, 令 $\ln'(P(A)) = \frac{7}{p}+\frac{3}{1-p} = 0$, 得 $p=0.7$

**极大似然估计只关注当前的样本，也就是只关注当前发生的事情，不考虑事情的先验情况。由于计算简单，而且不需要关注先验知识，因此在机器学习中的应用非常广，最常见的就是逻辑回归的求解就是用的极大似然估计。**

### 最大后验概率估计（MAP）

Maximum A Posteriori Estimation，贝叶斯派模型参数估计的常用方法。顾名思义：就是最大化在给定数据样本的情况下模型参数的后验概率。

最大后验概率分布认为 $\theta$ 是一个随机变量，即 $\theta$ 具有某种概率分布，称为先验分布，求解时除了要考虑似然函数 $P(x\mid \theta)$之外，还要考虑 $\theta$ 的先验分布 $P(\theta)$，因此其认为使 $P(x\mid \theta)P(\theta)$ 取最大值的 $\theta$ 就是最好的 $\theta$。

在最大似然估计中，由于认为 $\theta$ 是固定的，因此 $P(\theta)=1$。极大似然假设 $\theta$ 是一个定值而不是一个随机变量，并不假设它的分布情况而当作一个常量处理所以 $p(\theta)=1$ 带入 MAP 的式子消去就得到了 MLE 的极大似然函数。

依然是根据已知样本，来通过调整模型参数使得模型能够产生该数据样本的概率最大，只不过对于模型参数有了一个先验假设，即模型参数可能满足某种分布，不再一味地依赖数据样例（万一数据量少或者数据不靠谱呢）。

$$
arg \max p(\theta\mid X)=arg \max \frac{p(X\mid \theta)p(\theta)}{p(X)}=arg \max p(X\mid \theta)p(\theta)
$$

**在求最大后验概率时，可以忽略分母 $p(X)$，因为该值不影响对 $\theta$ 的估计。**

#### 最大后验概率估计的求解步骤

- 确定参数的先验分布以及似然函数
- 确定参数的后验分布函数
- 将后验分布函数转换为对数函数
- 求对数函数的最大值（求导，解方程）

在这里举个掷硬币的例子：抛一枚硬币 10 次，有 10 次正面朝上，0 次反面朝上。问正面朝上的概率 $\theta$。

- 在频率学派来看，利用极大似然估计可以得到 $\theta=10/10=1.0$。显然当缺乏数据时 MLE 可能会产生严重的偏差。
- 如果我们利用最大后验概率估计来看这件事，先验认为大概率下这个硬币是均匀的 （例如最大值取在 0.5 处的 Beta 分布），那么 $P(\theta\mid X)$ 是一个分布，最大值会介于 0.5~1 之间，而不是武断的给出 $\theta = 1$。
- 显然，随着数据量的增加，参数分布会更倾向于向数据靠拢，先验假设的影响会越来越小。

### 最大后验，最大似然的联系和区别

最大后验估计不只是关注当前样本的情况，还允许我们把先验知识加入到估计模型中，这在样本很少时候是很有用的。

最大后验，最大似然这两者的区别，其实就是对于参数\theta的理解不一样。

- 最大化后验概率的思想是该参数本身就服从某种潜在的分布，是需要考虑的。其先验概率密度函数是已知的，为 $p(\theta)$
- 而最大似然则认为该参数是一个固定的值，不是某种随机变量。

最大后验概率估计其实就是最大似然多了一个先验概率参数（待估计参数的先验分布），也可以认为最大似然估计就是把先验概率认为是一个定值。即，如果假设 $p(\theta)$ 是均匀分布，则贝叶斯方法等价于频率方法。因为直观上来讲，先验是 uniform distribution 本质上表示对事物没有任何预判， 那么最大后验，最大似然就相等了。

## 经验风险最小化与结构风险最小化

经验风险最小化与结构风险最小化是对于损失函数而言的。可以说经验风险最小化只侧重训练数据集上的损失降到最低；而结构风险最小化是在经验风险最小化的基础上约束模型的复杂度，使其在训练数据集的损失降到最低的同时，模型不至于过于复杂，相当于在损失函数上增加了正则项，防止模型出现过拟合状态。这一点也符合**奥卡姆剃刀原则：如无必要，勿增实体。**

经验风险最小化可以看作是采用了极大似然的参数评估方法，更侧重从数据中学习模型的潜在参数，而且是只看重数据样本本身。这样在数据样本缺失的情况下，很容易管中窥豹，模型发生过拟合的状态；结构风险最小化采用了最大后验概率估计的思想来推测模型参数，不仅仅是依赖数据，还依靠模型参数的先验假设。这样在数据样本不是很充分的情况下，我们可以通过模型参数的先验假设，辅助以数据样本，做到尽可能的还原真实模型分布。

### 经验风险最小化

MLE 是经验风险最小化的例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

### 结构风险最小化

MAP 是结构风险最小化的例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

结构风险最小化是在经验风险最小化的基础上增加了模型参数的先验。L1，L2 正则化就是对参数引入了拉普拉斯先验分布和高斯先验分布。
