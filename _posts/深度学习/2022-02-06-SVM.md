---
title: "SVM"
subtitle: "SVM 算法整理总结"
layout: post
author: "L Hondong"
header-img: "img/post-bg-39.jpg"
mathjax: true
tags:
  - 笔记
  - 机器学习
---

# SVM

> SVM 有三宝：间隔、对偶、核技巧。

SVM 虽然诞生只有短短的二十多年，但是自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中的表现 SVM 说是排第一估计是没有什么异议的。

SVM 是一个二元分类算法，线性分类和非线性分类都支持。经过演进，现在也可以支持多元分类，同时经过扩展，也能应用于回归问题。下面重点是 SVM 用于线性分类时模型和损失函数优化的一个总结。

## 一、线性支持向量机

### 1. 感知机

感知机的模型就是尝试找到一条直线，能够把二元数据隔离开；在三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。超平面定义为：$w^Tx^n+b=0$。

感知机模型的损失函数优化的思想是让所有误分类的点（定义为 M）到超平面的距离和最小，即最小化下式：

$$
\sum\limits_{x_i \in M} \frac{- y_i(w^Tx_i +b)}{\Vert w\Vert _2}
$$

当 $w$ 和 $b$ 成比例的增加，比如，当分子的 $w$ 和 $b$ 扩大 N 倍时，分母的 L2 范数也会扩大 N 倍，这表明分子和分母有固定的倍数关系。那么可以固定分子或者分母为 1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化损失函数。在感知机模型中采用的是保留分子，固定分母 $\Vert w\Vert_2 = 1$, 即最终感知机模型的损失函数为：

$$
\sum\limits_{x_i \in M}- y_i(w^Tx_i +b)
$$

如果不固定分母，改为固定分子，作为分类模型就是 SVM 了。

简单点讲，SVM 就是一种二类分类模型，实际上是定义在特征空间上的间隔最大的线性分类器，SVM 的学习策略就是间隔最大化。

SVM 本质是寻找一个最优的决策平面，距离两个类别最近的两个（或多个）样本，距离和最远。

### 2. 函数间隔与几何间隔

#### 函数间隔 Functional Margin

$$
\hat\gamma=y(w^Tx+b)=yf(x)
$$

最小值：$\hat\gamma=\min\hat\gamma_i$

但函数间隔有一个问题，如果成比例改变 $w$ 和 $b$，($2w^Tx+2b$)，则 $f(x)$ 变成原来的 2 倍，因此对法向量 $w$ 加约束条件：几何间隔。

#### 几何间隔 Geometrical Margin

点 $(x_1,x_2,\dots,x_n)$ 到直线 $(w^Tx^n+b)y^n=1$ 的距离：

$$
\gamma= \frac{y_i(w^Tx_i+b)}{\Vert w\Vert} =\frac{y_if(x_i)}{\Vert w\Vert}=\frac{\hat\gamma}{\Vert w\Vert}
$$

其中，$\Vert w\Vert=\sqrt{w_1^2+w_2^2+\dots+w_n^2}$

函数间隔不适合最大化间隔值，因为在超平面确定后，可以等比例缩放 $w$ 和 $b$ 的值，使 $f(x)$ 任意大。但几何间隔 $\frac{1}{\Vert w\Vert}$ 使 $\hat\gamma$ 不会随着 $w$ 和 $b$ 等比例缩放改变，只随平面的移动改变。

### 3. 支持向量

支持向量：离超平面最近的点。

在超平面 $w^Tx^n+b=0$ 上方的定义为 $y=1$，在超平面 $w^Tx^n+b=0$ 下方的定义为 $y=−1$，满足这个条件的超平面并不止一个，这些可以分类的超平面，哪个是最好的呢？或者说哪个是泛化能力最强的呢？

在感知机模型中，可以找到多个用于分类的超平面将数据分开，并且优化时希望所有的点都被准确分类。但是实际上离超平面很远的点已经被正确分类，且他们对超平面的位置没有影响。

我们最关心是那些离超平面很近的点，这些点很容易被误分类。SVM 的思想就是让离超平面比较近的点尽可能的远离超平面，最大化几何间隔，使得分类效果更好。

如下图所示，分离超平面为 $w^Tx + b =0$，如果所有的样本不光可以被超平面分开，还和超平面保持一定的函数距离（下图函数距离为 1），那么这样的分类超平面是比感知机的分类超平面优的。可以证明，这样的超平面只有一个。和超平面平行的保持一定的函数距离的这两个超平面对应的向量，定义为支持向量，如下图虚线所示。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-04-10-49-50.png" alt="SVM-2022-02-04-10-49-50" style="zoom:20%;" /></div>

支持向量到超平面的距离为 $\frac{1}{\Vert w\Vert_2}$, 两个支持向量之间的距离为 $\frac{2}{\Vert w\Vert_2}$。

### 4. SVM 模型目标函数与优化

SVM 的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：

$$
\max \gamma = \frac{y(w^Tx + b)}{\Vert w \Vert_2} 
$$

$$
s.t. \text{ } y_i(w^Tx_i + b) = \gamma_i \geq
\gamma (i =1,2,\dots,m)
$$

一般取函数间隔 $\hat{\gamma}$ 为 1，这样优化函数定义为：

$$
\max \frac{1}{\Vert w \Vert_2} 
$$

$$
s.t. \text{ } y_i(w^Tx_i + b) \geq 1 (i =1,2,\dots,m)
$$

这意味着 SVM 要在约束条件 $y_i(w^Tx_i + b) \geq 1 (i=1,2,\dots,m)$ 下，最大化 $\frac{1}{\Vert w \Vert_2}$。可以看出，这与感知机的优化方式不同，感知机是固定分母优化分子，而 SVM 是固定分子优化分母，同时加上了支持向量的限制。

由于 $\frac{1}{\Vert w \Vert_2}$ 的最大化等同于 $\frac{1}{2}\Vert w \Vert_2^2$ 的最小化。这样 SVM 的优化函数等价于：

$$
\min\frac{1}{2}\Vert w \Vert_2^2 
$$

$$
s.t. \text{ } y_i(w^Tx_i + b) \geq 1 (i =1,2,\dots,m)
$$

由于目标函数 $\frac{1}{2}\Vert w \Vert_2^2$ 是凸函数，同时约束条件不等式是仿射的，根据凸优化理论，可以通过拉格朗日函数将优化目标转化为无约束的优化函数：

$$
L(w,b,\alpha) = \frac{1}{2}\Vert w \Vert_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] 
$$

满足 $\alpha_i \geq 0$。

### 5. 拉格朗日乘子法（Lagrange Multiplier）

最优化问题可以分为以下三类：

#### 1. 无约束的优化问题

$$
\min\limits_x f(x)
$$
   
使用 Fermat 定理，即使用求取 $f(x)$ 的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。

#### 2. 有等式约束的优化问题

$$
\min\limits_x f(x)
$$

约束条件 $s.t. \text{ } h_i(x) = 0 \text{ } (i=0,1,2,\dots)$

使用拉格朗日乘子法（Lagrange Multiplier) ，即把等式约束 $h_i(x)$ 用一个系数与目标函数 $f(x)$ 写为一个式子，称为拉格朗日函数，而系数 $\mu$ 称为拉格朗日乘子。

$$
L(x, \mu)=f(x) - \mu h(x)
$$

通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。

#### 3. 有不等式约束的优化问题

$$
\min\limits_x f(x)
$$

约束条件 

$$
s.t. \text{ }  \begin{aligned}g(x_i) \leq 0 \text{ } (i=0,1,2,\dots) \\ h(x_i) = 0 \text{ } (i=0,1,2,\dots) \end{aligned} 
$$

使用 KKT 条件，把所有的等式、不等式约束与 $f(x)$ 写为一个式子，也叫拉格朗日函数，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的必要条件，这个条件称为 KKT 条件：

1. $L(x, a, b)$ 对 $x$ 求导为零
2. $h(x) = 0$
3. $a \cdot g(x) = 0$

要求 $f(x)$ 的最小值，可以构建拉格朗日函数：

$$
L(x,a,b) = f(x) + a \cdot g(x) + b \cdot h(x), a \geq 0
$$

当 $g(x)\leq 0,h(x)=0, a\geq 0$ 时，$a\cdot g(x)\leq 0$，所以 $L(x,a,b)$ 在取得最大值时，即 $a \cdot g(x)=0$ 时，就等于 $f(x)$。

所以需要求解的目标函数的最小值表达式是：

$$
\min\limits_{x} \max\limits_{a,b} L(x,a,b)
$$

其对偶表达式是：

$$
\max\limits_{a,b} \min\limits_{x} L(x,a,b)
$$

回到 SVM，由于引入了拉格朗日乘子，优化目标变成：

$$
\min\limits_{w,b} \max\limits_{\alpha_i\geq 0} L(w,b,\alpha)
$$

和最大熵模型一样的，该优化函数满足 KKT 条件，可以通过拉格朗日对偶将该优化问题转化为等价的对偶问题来求解。

$$
\max\limits_{\alpha_i \geq 0} \min\limits_{w,b} L(w,b,\alpha)
$$

首先求 $L(w,b,\alpha)$ 基于 $w$ 和 $b$ 的极小值，即 $\min\limits_{w,b} L(w,b,\alpha)$，对 $w$ 和 $b$ 分别求偏导数得到：

$$
\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i 
$$ 

$$
\frac{\partial L}{\partial b} = 0 \Rightarrow \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$

上式已经求得 $w$ 和 $\alpha$ 的关系，只要后面能够求出优化函数极大化对应的 $\alpha$，就可以求出 $w$ ，至于 $b$，由于上两式已经没有 $b$，所以最后的 $b$ 可以有多个。

接下来再求拉格朗日乘子 $\alpha$ 的极大值。将 $w$ 和 $\alpha$ 的关系带入优化函数 $L(w,b,\alpha)$ 消去 $w$。

定义：

$$
\psi(\alpha) = \min\limits_{w,b} L(w,b,\alpha)
$$

将 $w$ 替换为 $\alpha$ 的表达式以后的优化函数 $\psi(\alpha)$ 的表达式：

$$ 
\begin{aligned} \psi(\alpha) 
& = \frac{1}{2}\Vert w \Vert_2^2 -\sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \\
& = \frac{1}{2}w^Tw-\sum\limits_{i=1}^{m}\alpha_iy_iw^Tx_i -\sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\
& =\frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i -\sum\limits_{i=1}^{m}\alpha_iy_iw^Tx_i - \sum\limits_{i=1}^{m}\alpha_iy_ib +\sum\limits_{i=1}^{m}\alpha_i \\
& =\frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i w^T-\sum\limits_{i=1}^{m}\alpha_iy_ix_i - \sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\
& = -\frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i -\sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\
& = -\frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i -b\sum\limits_{i=1}^{m}\alpha_iy_i + \sum\limits_{i=1}^{m}\alpha_i \\
& =-\frac{1}{2}(\sum\limits_{i=1}^{m}\alpha_iy_ix_i)^T(\sum\limits_{i=1}^{m}\alpha_iy_ix_i)- b\sum\limits_{i=1}^{m}\alpha_iy_i + \sum\limits_{i=1}^{m}\alpha_i \\
& = -\frac{1}{2}\sum\limits_{i=1}^{m}\alpha_iy_ix_i^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i- b\sum\limits_{i=1}^{m}\alpha_iy_i + \sum\limits_{i=1}^{m}\alpha_i \\
& =-\frac{1}{2}\sum\limits_{i=1}^{m}\alpha_iy_ix_i^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i+ \sum\limits_{i=1}^{m}\alpha_i \\
& =-\frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_iy_ix_i^T\alpha_jy_jx_j +
\sum\limits_{i=1}^{m}\alpha_i \\
& = \sum\limits_{i=1}^{m}\alpha_i -
\frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j
\end{aligned}
$$

1. 对于向量的转置：$(\sum\limits_{i=1}^{m}\alpha_iy_ix_i)^T=\sum\limits_{i=1}^{m}\alpha_iy_ix_i^T$ 由于常量的转置是其本身，所有只有向量 $x_i$ 被转置。
2. 由于 $\sum\limits_{i=1}^{m}\alpha_iy_i=0$，所以 $b\sum\limits_{i=1}^{m}\alpha_iy_i=0$ 直接消掉。
3. 接下来使用了 $(a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+…$ 的乘法运算法则。

对 $w,b$ 极小化以后，优化函数 $\psi(\alpha)$ 仅仅只有 $\alpha$ 向量做参数。接下来求拉格朗日乘子 $\alpha$ 的极大值，就可以求出此时对应的 $\alpha$，进而求出 $w,b$。

$$
\max\limits_{\alpha} \sum\limits_{i=1}^{m} \alpha_i - \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j)
$$

$$
s.t. \sum\limits_{i=1}^{m}\alpha_iy_i = 0 
$$ 

$$
\alpha_i \geq 0 \text{ } (i=1,2,\dots,m) 
$$

等价为极小化问题：

$$
\min\limits_{\alpha}
\frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i
\cdot x_j) - \sum\limits_{i=1}^{m} \alpha_i 
$$ 

$$
s.t.  \sum\limits_{i=1}^{m}\alpha_iy_i = 0 
$$ 

$$
\alpha_i \geq 0  i=1,2,\dots,m 
$$

通过 SMO 算法极小化上式，得到了对应的 $\alpha$ 的极小值 $\alpha^{\ast}$。

那么根据 $w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i$，可以求出对应的 $w$ 的值

$$
w^{\ast} =\sum\limits_{i=1}^{m}\alpha_i^{\ast}y_ix_i
$$

求 $b$ 则稍微麻烦一点。注意到，对于任意支持向量 $(x_s, y_s)$，都有：

$$
y_s(w^Tx_s+b) = y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1 
$$

假设有 $S$ 个支持向量，则对应求出 $S$ 个 $b^{\ast}$，理论上这些 $b^{\ast}$ 都可以作为最终的结果，但是一般采用一种更健壮的办法，即求出所有支持向量所对应的 $b_s^{\ast}$，然后将其平均值作为最后的结果。

注意到对于严格线性可分的 SVM，$b$ 的值是有唯一解的，也就是这里求出的所有 $b^{\ast}$ 都是一样的，这里仍然这么写是为了和后面加入软间隔后的 SVM 的算法描述一致。

据 KKT 条件中的对偶互补条件 $\alpha_{i}^{\ast}\left(y_i(w^Tx_i + b) - 1\right) =0$，如果 $\alpha_i>0$ 则有 $y_i(w^Tx_i + b) =1$，即点在支持向量上，否则如果 $\alpha_i=0$ 则有 $y_i(w^Tx_i+ b) \geq 1$，即样本在支持向量上或者已经被正确分类。

### 6. 线性可分 SVM 的算法过程

线性可分 SVM 的算法过程总结：

输入是线性可分的 $m$ 个样本 ${(x_1,y_1), (x_2,y_2), \dots, (x_m,y_m)}$，其中 $x$ 为 $n$ 维特征向量。$y$ 为二元输出，值为 1 或者 -1.

输出是分离超平面的参数 $w^{\ast}$、$b^{\ast}$ 和分类决策函数。

算法过程如下：

1. 构造约束优化问题

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum\limits_{i=1}^{m} \alpha_i
$$ 

$$
s.t. \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$ 

$$
\alpha_i \geq 0 \text{ } i=1,2,\dots,m
$$

2. 用 SMO 算法求出上式最小时对应的 $\alpha$ 向量的值 $\alpha^{\ast}$ 向量。
3. 计算 $w^{\ast} = \sum\limits_{i=1}^{m}\alpha_i^{\ast}y_ix_i$
4. 找出所有的 $S$ 个支持向量，即满足 $\alpha_s > 0$ 对应的样本 $(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1$，计算出每个支持向量 $(x_x,
y_s)$ 对应的 $b_s^{\ast}$，计算出这些 $b_s^{\ast} = y_s -\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s$。所有的 $b_s^{\ast}$对应的平均值即为最终的 $b^{\ast} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{\ast}$

这样最终的分类超平面为：$w^{\ast} \cdot x + b^{\ast} = 0$，最终的分类决策函数为：$f(x) = sign(w^{\ast} \cdot x + b^{\ast})$

线性可分 SVM 的学习方法对于非线性的数据集是没有办法使用的，有时候不能线性可分的原因是线性数据集里面多了少量的异常点，由于这些异常点导致了数据集不能线性可分。使用线性 SVM 的软间隔最大化可以处理这些异常点，使数据集依然可以用线性可分的思想。

## 二、线性支持向量机的软间隔最大化模型

### 1. 线性分类 SVM 面临的问题

有时候本来数据的确是可分的，可以用线性分类 SVM 的学习方法来求解，但是却因为混入了异常点，导致不能线性可分。比如下图，本来数据是可以按下面的实线来做超平面分离，但是由于一个橙色和一个蓝色的异常点导致 SVM 没法按照线性支持向量机方法来分类。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-04-16-17-49.png" alt="SVM-2022-02-04-16-17-49" style="zoom:100%;" /></div>

另外一种情况虽然没有糟糕到不可分，但是会严重影响模型的泛化预测效果，比如下图，本来如果不考虑异常点，SVM 的超平面应该是下图中的红色线所示，但是由于有一个蓝色的异常点，导致学习到的超平面是下图中的粗虚线所示，严重影响了分类模型预测效果。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-04-16-19-22.png" alt="SVM-2022-02-04-16-19-22" style="zoom:100%;" /></div>

SVM 引入软间隔最大化的方法来解决这些问题。

### 2. 线性分类 SVM 的软间隔最大化

所谓的软间隔，是相对于硬间隔说的，一般认为线性分类 SVM 的学习方法属于硬间隔最大化。

回顾下硬间隔最大化的条件：

$$
\min \frac{1}{2}\Vert w \Vert_2^2 
$$

$$
s.t. \text{ } y_i(w^Tx_i + b) \geq 1 (i =1,2,\dots,m)
$$

对于软间隔，SVM 对训练集里面的每个样本 $(x_i,y_i)$ 引入了一个松弛变量 $\xi_i \geq 0$, 使函数间隔加上松弛变量大于等于 1：

$$
y_i(w\cdot x_i +b) + \xi_i \geq 1
$$

即：

$$
y_i(w\cdot x_i +b) \geq 1- \xi_i
$$

对比硬间隔最大化，可以看到对样本到超平面的函数距离的要求放松了，之前是一定要大于等于 1，现在只需要加上一个大于等于 0 的松弛变量能大于等于 1 就可以了。当然，松弛变量不能白加，是有成本的，每一个松弛变量 $\xi_i$ 对应了一个代价 $\xi_i$。

得到软间隔最大化的 SVM 学习条件：

$$
\min \frac{1}{2}\Vert w \Vert_2^2 +C\sum\limits_{i=1}^{m}\xi_i 
$$ 

$$
s.t. \text{ } y_i(w^Tx_i + b) \geq 1 - \xi_i (i =1,2,\dots,m)
$$

$$
\xi_i \geq 0 (i =1,2,\dots,m)
$$

这里，$C>0$ 为惩罚系数，可以理解为一般回归和分类问题正则化时候的参数。$C$ 越大，对误分类的惩罚越大；$C$ 越小，对误分类的惩罚越小。我们希望 $\frac{1}{2}\Vert w \Vert_2^2$ 尽量小，误分类的点尽可能的少。$C$ 是协调两者关系的正则化惩罚系数。在实际应用中，需要调参来选择。

这个目标函数的优化和第一章中的线性可分 SVM 的优化方式类似，下面详解线性分类 SVM 的软间隔最大化如何学习优化。

### 3. 线性分类 SVM 的软间隔最大化目标函数的优化

和线性可分 SVM 的优化方式类似，首先将软间隔最大化的约束问题用拉格朗日函数转化为无约束问题如下：

$$
L(w,b,\xi,\alpha,\mu) = \frac{1}{2}\Vert w \Vert_2^2 +C\sum\limits_{i=1}^{m}\xi_i - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] - \sum\limits_{i=1}^{m}\mu_i\xi_i 
$$

其中 $\mu_i \geq 0, \alpha_i \geq 0$，均为拉格朗日系数。

现在要优化的目标函数是：

$$
\min\limits_{w,b,\xi} \max\limits_{\alpha_i
\geq 0, \mu_i \geq 0,} L(w,b,\alpha, \xi,\mu)
$$

这个优化目标也满足 KKT 条件，也可以通过拉格朗日对偶将该优化问题转化为等价的对偶问题来求解：

$$
\max\limits_{\alpha_i \geq 0, \mu_i \geq 0,} \min\limits_{w,b,\xi} L(w,b,\alpha,
\xi,\mu)
$$

按照相同的思路，先求优化函数对于 $w, b, \xi$ 的极小值，接着再求拉格朗日乘子 $\alpha$ 和 $\mu$ 的极大值。

首先求优化函数对于 $w, b, \xi$ 的极小值，求偏导数：

$$
\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i 
$$

$$
\frac{\partial L}{\partial b} = 0 \Rightarrow \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$

$$
\frac{\partial L}{\partial \xi} = 0 \Rightarrow C- \alpha_i - \mu_i = 0 
$$

接下来利用上面的三个式子去消除 $w$ 和 $b$。

$$
\begin{aligned} 
L(w,b,\xi,\alpha,\mu) & = \frac{1}{2}\Vert w \Vert_2^2 + C\sum\limits_{i=1}^{m}\xi_i - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] - \sum\limits_{i=1}^{m}\mu_i\xi_i \\ 
&= \frac{1}{2}\Vert w \Vert_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] + \sum\limits_{i=1}^{m}\alpha_i\xi_i \\ & = \frac{1}{2}\Vert w \Vert_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \\ 
& = \frac{1}{2}w^Tw-\sum\limits_{i=1}^{m}\alpha_iy_iw^Tx_i - \sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\ 
& = \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - \sum\limits_{i=1}^{m}\alpha_iy_iw^Tx_i - \sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\ 
& = \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - \sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\ 
& = - \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - \sum\limits_{i=1}^{m}\alpha_iy_ib + \sum\limits_{i=1}^{m}\alpha_i \\
& = - \frac{1}{2}w^T \sum\limits_{i=1}^{m} \alpha_i y_i x_i -b\sum\limits_{i=1}^{m}\alpha_i y_i + \sum\limits_{i=1}^{m}\alpha_i \\ 
& = -\frac{1}{2}(\sum\limits_{i=1}^{m}\alpha_iy_ix_i)^T(\sum\limits_{i=1}^{m}\alpha_iy_ix_i) + \sum\limits_{i=1}^{m}\alpha_i \\ 
& = -\frac{1}{2}\sum\limits_{i=1}^{m}\alpha_iy_ix_i^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i + \sum\limits_{i=1}^{m}\alpha_i \\
& = - \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum\limits_{i=1}^{m}\alpha_i
\end{aligned}
$$

仔细观察，这个式子与第一章线性可分 SVM 几乎一样，唯一不一样的是约束条件。线性不可分 SVM 的优化目标：

$$
\max\limits_{\alpha} \sum\limits_{i=1}^{m}\alpha_i  - \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j
$$ 

$$
s.t. \text{ } \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$ 

$$
C- \alpha_i - \mu_i = 0
$$ 

$$
\alpha_i \geq 0 \text{ } (i =1,2,\dots,m)
$$ 

$$
\mu_i \geq 0 \text{ } (i =1,2,\dots,m)
$$

$ C- \alpha_i - \mu_i = 0, \alpha_i \geq 0, \mu_i \geq 0$ 三个式子中消去 $\mu_i$，只留下 $\alpha_i$，即 $0 \leq \alpha_i \leq C$。

同时将优化目标函数变号，求极小值：

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum\limits_{i=1}^{m}\alpha_i 
$$ 

$$
s.t.  \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$ 

$$
0 \leq \alpha_i \leq C
$$

这就是软间隔最大化时的线性可分 SVM 的优化目标形式，与第一章硬间隔最大化的线性可分 SVM 相比，仅仅多了一个约束条件 $0 \leq \alpha_i \leq C$，依然可以通过 SMO 算法，求出上式极小化时对应的 $\alpha$ 向量就可以求出 $w$ 和 $b$。

### 4. 软间隔最大化时的支持向量

在硬间隔最大化时，支持向量比较简单，满足 $y_i(w^Tx_i + b) -1=0$ 即可。根据 KKT 条件中的对偶互补条件 $\alpha_{i}^{\ast}(y_i(w^Tx_i + b) - 1) = 0$，如果 $\alpha_{i}^{\ast}>0$ 则有 $y_i(w^Tx_i + b) =1$ 即点在支持向量上，否则如果 $\alpha_{i}^{\ast}=0$ 则有 $y_i(w^Tx_i + b) \geq 1$，即样本在支持向量上或者已经被正确分类。

在软间隔最大化时，则稍微复杂一些，因为需要对每个样本 $(x_i,y_i)$ 引入松弛变量 $\xi_i$。下图表示软间隔最大化时支持向量的情况，第 $i$ 个点到对应类别支持向量的距离为 $\frac{\xi_i}{\Vert w \Vert_2}$。根据软间隔最大化时 KKT 条件中的对偶互补条件 $\alpha_{i}^{\ast}(y_i(w^Tx_i + b) - 1 + \xi_i^{\ast}) = 0$ 有：

1. 如果 $\alpha = 0$，那么 $y_i(w^Tx_i + b) - 1 \geq 0$，即样本在间隔边界上或者已经被正确分类。如图中所有远离间隔边界的点。
2. 如果 $0 < \alpha < C$，那么 $\xi_i = 0 , y_i(w^Tx_i + b) - 1 = 0$，即点在间隔边界上。
3. 如果 $\alpha = C$，说明这是一个可能比较异常的点，需要检查此时 $\xi_i$：
    1. 如果 $0 \leq \xi_i \leq 1$，那么点被正确分类，但是却在超平面和自己类别的间隔边界之间。如图中的样本 2 和 4。
    2. 如果 $\xi_i =1$, 那么点在分离超平面上，无法被正确分类。
    3. 如果 $\xi_i > 1$, 那么点在超平面的另一侧，这个点不能被正常分类，如图中的样本 1 和 3。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-06-07-34-20.png" alt="SVM-2022-02-06-07-34-20" style="zoom:50%;" /></div>

### 5. 软间隔最大化的线性可分 SVM 的算法过程

总结软间隔最大化时的线性可分 SVM 的算法过程。

输入是线性可分的 $m$ 个样本 ${(x_1,y_1), (x_2,y_2), \dots,(x_m,y_m),}$, 其中 $x$ 为 $n$ 维特征向量，$y$ 为二元输出，值为 1，或者 -1。

输出是分离超平面的参数 $w^{\ast}$ 和 $b^{\ast}$ 和分类决策函数。

算法过程如下：

1. 选择一个惩罚系数 $C>0$, 构造约束优化问题

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum\limits_{i=1}^{m}\alpha_i
$$ 

$$
s.t.  \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$ 

$$
0 \leq \alpha_i \leq C
$$

2. 用 SMO 算法求出上式最小时对应的 $\alpha$ 向量的值 $\alpha^{\ast}$ 向量。
3. 计算 $w^{\ast} = \sum\limits_{i=1}^{m}\alpha_i^{\ast}y_ix_i$。
4. 找出所有的 $S$ 个支持向量对应的样本 $(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1$，计算出每个支持向量 $(x_s,y_s)$ 对应的 $b_s^{\ast}$，计算出这些 $b_s^{\ast} = y_s - \sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s$。所有的 $b_s^{\ast}$ 对应的平均值即为最终的 $b^{\ast} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{\ast}$。

这样最终的分类超平面为：$w^{\ast} \cdot x + b^{\ast} = 0 $，最终的分类决策函数为：

$$
f(x) = sign(w^{\ast}\cdot x + b^{\ast})
$$

### 6. 合页损失函数

线性支持向量机还有另外一种解释如下：

$$
\min\limits_{w, b} [1-y_i(w \cdot x + b)]_{+} +\lambda \Vert w \Vert_2^2
$$

其中 $L(y(w \cdot x + b)) = [1-y_i(w \cdot x + b)]_{+}$ 称为合页损失函数 (Hinge loss function)，下标 + 表示为：

$$
[z]_{+}=  
\begin{cases}  
z & {z >0} \\  
0& {z\leq 0}  
\end{cases}
$$

即：

$$
\min\limits_{w, b} \left( \max(0,(1-y_i(w \cdot x + b))) +  \lambda \Vert w \Vert_2^2 \right)
$$

如果点被正确分类，且函数间隔大于 1，损失是 0，否则损失是 $1-y(w \cdot x + b)$，如下图中的绿线。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-06-07-51-46.png" alt="SVM-2022-02-06-07-51-46" style="zoom:100%;" /></div>

在图中还可以看出其他各种模型损失和函数间隔的关系：

1. 0-1 损失函数，如果正确分类，损失是 0，误分类损失 1，如图黑线，可见 0-1 损失函数是不可导的。
2. 对于感知机模型，其损失函数是 $[-y_i(w \cdot x + b)]_{+}$，这样当样本被正确分类时，损失是 0，误分类时，损失是 $-y_i(w \cdot x +b)$，如图中紫线。
3. 对于逻辑回归和最大熵模型对应的对数损失，损失函数是 $\log[1+\exp(-y(w \cdot x + b))]$，如下图红线所示。

线性可分 SVM 通过软间隔最大化，可以解决线性数据集带有异常点时的分类处理，但是现实生活中有很多数据不是线性可分的，对于线性不可分 SVM 就需要核函数来解决。

## 三、线性不可分支持向量机与核函数

在前面两章中分析了线性可分 SVM 的硬间隔最大化和软间隔最大化的算法，它们可以处理线性可分的数据，但是不能处理完全线性不可分的数据。接下来探讨 SVM 如何处理线性不可分的数据，重点讲述核函数在 SVM 中处理线性不可分数据的作用。

### 1. 回顾多项式回归

对于一个只有两个特征的 $p$ 次方多项式回归的模型：

$$
h_\theta(x_1, x_2) = \theta_0 + \theta_{1}x_1 +
\theta_{2}x_{2} + \theta_{3}x_1^{2} + \theta_{4}x_2^{2} + \theta_{5}x_{1}x_2
$$

令 $(x_0 = 1, x_1 = x_1, x_2 = x_2, x_3 =x_1^{2}, x_4 = x_2^{2}, x_5 = x_1x_2)$，这样就得到了下式：

$$
h_\theta(x_1, x_2) = \theta_0 + \theta_{1}x_1 + \theta_{2}x_{2} + \theta_{3}x_3 + \theta_{4}x_4 + \theta_{5}x_5
$$

这样从多项式回归又回到了线性回归，这是一个五元线性回归，可以用线性回归的方法来完成算法。对于每个二元样本特征 $(x_1,x_2)$，可以得到一个五元样本特征 $(1,x_1, x_2, x_{1}^2, x_{2}^2, x_{1}x_2)$，通过这个改进的五元样本特征，把不是线性回归的函数重新变回线性回归。对于二维的非线性数据，将其映射到了五维以后就变成了线性数据。

对于在低维线性不可分的数据，映射到高维以后，就变成线性可分的了。这个思想同样可以运用到 SVM 的线性不可分数据上。

### 2. 核函数的引入

对于线性不可分的低维特征数据，可以将其映射到高维，就能线性可分。现在将这个思想运用到 SVM 的算法上。回顾线性可分 SVM 的优化目标函数：

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i \cdot x_j - \sum\limits_{i=1}^{m}\alpha_i
$$ 

$$
s.t. \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$ 

$$
0 \leq \alpha_i \leq C
$$

注意到上式低维特征仅仅以内积 $x_i \cdot x_j$ 的形式出现，如果定义一个低维特征空间到高维特征空间的映射 $\phi$（比如 2 维到 5 维的映射），将所有特征映射到一个更高的维度，让数据线性可分，就可以继续按前两章的方法来优化目标函数，求出分离超平面和分类决策函数了，那么 SVM 的优化目标函数变成：

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(x_i) \cdot \phi(x_j) - \sum\limits_{i=1}^{m}\alpha_i
$$ 

$$
s.t. \sum\limits_{i=1}^{m}\alpha_iy_i = 0
$$ 

$$
0 \leq \alpha_i \leq C
$$

上式与线性可分 SVM 的优化目标函数的区别仅仅是将内积 $x_i \cdot x_j$ 替换为 $\phi(x_i) \cdot \phi(x_j)$。

接下来还有一个问题：假如是一个 2 维特征的数据，可以将其映射到 5 维来做特征的内积；如果原始空间是三维，可以映射到到 19 维空间，似乎还可以处理。但是如果低维特征是 1000 个维度，那么要将其映射到超级高的维度来计算特征的内积，此时映射成的高维维度是爆炸性增长的，这个计算量实在是太大了，根本无从计算。

这时就需要核函数了。

假设 $\phi$ 是一个从低维的输入空间 $\chi$（欧式空间的子集或者离散集合）到高维的希尔伯特空间的 $\mathcal{H}$ 映射。那么如果存在函数 $K(x,z)$，对于任意 $x, z \in \chi$，都有：

$$
K(x, z) = \phi(x) \cdot \phi(z)
$$

那么就称 $K(x, z)$ 为核函数。

**任意两个样本点在扩维后的空间的内积，如果等于这两个样本点在原来空间经过一个函数后的输出，那么这个函数就叫核函数**。

仔细观察上式可以发现，$K(x,z)$ 的计算是在低维特征空间来计算的，它避免了在高维维度空间计算内积的恐怖计算量。在遇到线性不可分的样例时，常用做法是把样例特征映射到高维空间中去（如多项式回归映射到高维线性回归），但是遇到线性不可分的样例，直接映射到高维空间，那么维度大小是会高到不能接受。此时，就需要核函数的作用了，核函数虽然也是将特征进行从低维到高维的转换，但在低维上进行计算，而将实质上的分类效果（利用了内积）表现在了高维上，这样避免了直接在高维空间中的复杂计算，真正解决了 SVM 线性不可分的问题。

关于核函数解决高维计算量过大的解释：

设两个数据， $x = (x_1,  x_2,  x_3),  y = (y_1,  y_2,  y_3)$。此时在 3D 空间已经不能对其进行线性划分了，那么可以通过一个函数将数据映射到更高维的空间，比如 9 维的话，则 $f(x) = (x_1x_1, x_1x_2, x_1x_3, x_2x_1, x_2x_2, x_2x_3, x_3x_1, x_3x_2, x_3x_3)$，由于需要计算内积，所以在新的数据在 9 维空间，需要计算  $<f(x),  f(y)>$ 的内积，需要花费时间复杂度 $O(n^2)$。

具体点，令 $x = (1, 2, 3)$，$y = (4, 5, 6)$，那么 $f(x)  = (1, 2, 3, 2, 4, 6, 3, 6, 9)$，$f(y) = (16, 20, 24, 20, 25, 36, 24, 30, 36)$。

此时：
 
$$
<f(x),  f(y)>  = 16 + 40 + 72 +40 +100 + 180 + 72 +180 +324 = 1024
$$

对于 3D 空间这两个数据，似乎还能计算，但是如果将维数扩大到一个非常大数的时候，计算起来可就不是这么一点点问题了。

使用核函数：$K(x, y) = (<x, y>)^2$，代入上式： $K(x, y) = (4 + 10 + 18)^2 = 32^2 = 1024$

即： 

$$
K(x, y) = (<x, y>)^2  = <f(x),  f(y)>
$$

但是 $K(x, y)$ 计算起来却比 $<f(x), f(y)>$ 简单的多，这意味着只要用 $K(x, y)$ 来计算，效果与 $<f(x), f(y)>$ 是一样的，但是计算效率却大幅度提高了，如  $K(x, y)$ 是 $O(n)$，而 $<f(x), f(y)>$ 是 $O(n^2)$，所以使用核函数的好处就是，可以在一个低维空间去完成一个高纬度（或者无限维度）样本内积的计算，比如上面例子中 $K(x, y)$ 的 3D 空间对比 $<f(x), f(y)>$ 的 9D 空间。

有了这个核函数，以后的高维内积都可以转换为低维的函数运算了，即只需要计算低维的内积，然后再平方，这样复杂度降低极大。

总之，核函数实现的就是先在低维空间完成内积计算，然后再映射到高维空间，避免了映射到高维空间进行内积计算的复杂计算。

### 3. 核函数的介绍

事实上，核函数的研究非常早，要比 SVM 出现早得多，将它引入 SVM 中是最近二十多年的事情。对于从低维到高维的映射，核函数不止一个，那么什么样的函数才可以当做核函数呢？

由于一般核函数都是正定核函数，这里只说明正定核函数的充分必要条件：一个函数要想成为正定核函数，必须满足里面任何点的集合形成的 Gram 矩阵是半正定的，即对于任意的 $x_i \in \chi (i=1,2,3\dots,m) $，$K(x_i,x_j)$ 对应的 Gram 矩阵 $K = \bigg[ K(x_i, x_j )\bigg]$ 是半正定矩阵，则 $K(x,z)$ 是正定核函数。

从上面的定理看，要求任意的集合都满足 Gram 矩阵半正定，所以自己去找一个核函数还是很难的，还好常用的核函数也仅仅只有那么几个。

Scikit-learn 中默认可选的几个核函数：

####  3.1 线性核函数

线性核函数（Linear Kernel）其实就是前两中的线性可分 SVM，表达式为：

$$
K(x, z) = x \cdot z 
$$

线性可分 SVM 可以和线性不可分 SVM 归为一类，区别仅仅在于线性可分 SVM 用的是线性核函数。

####  3.2 多项式核函数

多项式核函数（Polynomial Kernel）是线性不可分 SVM 常用的核函数之一，表达式为：

$$
K(x, z) = (\gamma x \cdot z + r)^d
$$

其中，$\gamma, r, d$ 都需要自己调参定义。

####  3.3 高斯核函数

高斯核函数（Gaussian Kernel），在 SVM 中也称为径向基核函数（Radial Basis Function, RBF），它是非线性分类 SVM 最主流的核函数。libsvm 默认的核函数就是它。表达式为：

$$
K(x, z) = e^{(-\gamma\Vert x-z\Vert^2)}
$$

其中，$\gamma$ 大于 0，需要自己调参定义。

####  3.4 Sigmoid 核函数

Sigmoid 核函数（Sigmoid Kernel）也是线性不可分 SVM 常用的核函数之一，表达式为：

$$
K(x, z) = \tanh(\gamma x \cdot z + r)
$$

其中，$\gamma, r$ 都需要自己调参定义。

### 4. 分类 SVM 的算法小结

引入了核函数后，SVM 算法才算是完整了。现在对分类 SVM 的算法过程做一个总结，不再区别是否线性可分。

输入是 $m$ 个样本 ${(x_1,y_1), (x_2,y_2), \dots, (x_m,y_m),}$，其中 $x$ 为 $n$ 维特征向量。$y$ 为二元输出，值为 1，或者-1.

输出是分离超平面的参数 $w^{\ast}$ 和 $b^{\ast}$ 和分类决策函数。

算法过程如下：

1. 选择适当的核函数 $K(x,z)$ 和一个惩罚系数 $C>0$，构造约束优化问题：

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j) - \sum\limits_{i=1}^{m}\alpha_i 
$$ 

$$
s.t. \sum\limits_{i=1}^{m}\alpha_iy_i = 0 
$$ 

$$
0 \leq \alpha_i \leq C
$$

2. 用 SMO 算法求出上式最小时对应的 $\alpha$ 向量的值 $\alpha^{\ast}$ 向量。
3. 得到 $ w^{\ast} = \sum\limits_{i=1}^{m}\alpha_i^{\ast} y_i\phi(x_i)$，此处可以不直接显式的计算 $w^{\ast}$。
4. 找出所有的 $S$ 个支持向量，即满足 $0 < \alpha_s$ 对应的样本 $(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{m}\alpha_iy_iK(x_i,x_s)+b) = 1$，计算出每个支持向量 $(x_s,y_s)$ 对应的 $b_s^{\ast}$，计算出这些 $b_s^{\ast} = y_s -
\sum\limits_{i=1}^{m}\alpha_iy_iK(x_i,x_s)$。所有的 $b_s^{\ast}$ 对应的平均值即为最终的 $b^{\ast} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{\ast}$。

最终的分类超平面为：$\sum\limits_{i=1}^{m}\alpha_i^{\ast}y_iK(x, x_i)+ b^{\ast} = 0$，最终的分类决策函数为：

$$
f(x) = sign(\sum\limits_{i=1}^{m}\alpha_i^{\ast}y_iK(x, x_i)+ b^{\ast})
$$

至此，分类 SVM 总结完毕，最后一点内容是 SMO 算法，关系到如何求出优化函数极小化时候的 $\alpha^{\ast}$，进而求出 $w,b$。

## 四、SMO 算法

### 1. 回顾 SVM 优化目标函数

首先回顾下 SVM 优化目标函数：

$$
\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j) - \sum\limits_{i=1}^{m}\alpha_i
$$ 

$$
s.t. \sum\limits_{i=1}^{m}\alpha_iy_i =0 
$$ 

$$
0 \leq \alpha_i \leq C
$$

解要满足的 KKT 条件的对偶互补条件为：

$$
\alpha_{i}^{\ast}(y_i(w^Tx_i + b) - 1 + \xi_i^{\ast}) = 0
$$

根据这个 KKT 条件的对偶互补条件，有：

$$
\alpha_{i}^{\ast} = 0 \Rightarrow y_i(w^{\ast} \cdot \phi(x_i) + b) \geq 1
$$ 

$$
0 <\alpha_{i}^{\ast} < C \Rightarrow y_i(w^{\ast}\cdot \phi(x_i) + b) = 1
$$ 

$$
\alpha_{i}^{\ast}= C \Rightarrow y_i(w^{\ast}\cdot \phi(x_i) + b) \leq 1
$$

由于 $w^{\ast} = \sum\limits_{j=1}^{m}\alpha_j^{\ast}y_j\phi(x_j)$，令 $g(x) = w^{\ast} \cdot \phi(x) + b =\sum\limits_{j=1}^{m}\alpha_j^{\ast}y_jK(x_i, x_j)+ b^{\ast}$，则有：

$$
\alpha_{i}^{\ast} = 0 \Rightarrow y_ig(x_i) \geq 1
$$ 

$$
0 < \alpha_{i}^{\ast} < C \Rightarrow y_ig(x_i) = 1
$$ 

$$
\alpha_{i}^{\ast}= C \Rightarrow y_ig(x_i) \leq 1
$$

### 2. SMO 算法的基本思想

上面的优化式子比较复杂，里面有 $m$ 个变量组成的向量 $\alpha$ 需要在目标函数极小化的时候求出，直接优化时很难。SMO 算法则采用了一种启发式的方法，每次只优化两个变量，将其他的变量都视为常数。由于$\sum\limits_{i=1}^{m}\alpha_iy_i = 0$。假如将 $\alpha_3, \alpha_4, \dots, \alpha_m$ 固定，那么 $\alpha_1, \alpha_2$ 之间的关系也确定了。这样 SMO 算法将一个复杂的优化算法转化为一个比较简单的两变量优化问题。

先举个例子，假设优化目标为一个二元函数：

$$
arg \min\limits_{x_1,x_2}f(x_1,x_2)=-x_1^2-3x_2^2+2x_1x_2+6
$$

先给 $(x_1,x_2)$ 一个初值，然后开始迭代。

1. 先固定 $x_1$，把 $f$ 看作 $x_2$ 的一元函数最优解，可以求导得：

$$
\frac{\partial f}{\partial x_1} = -2x_1+2x_2 =0 \rightarrow x_1=x_2
$$

2. 再固定 $x_2$，把 $f$ 看成 $x_1$ 的一元函数求最优值，得到 $x_1$ 的解析解:

$$
\frac{\partial f}{\partial x_2} = -6x_2+2x_2 =0 \rightarrow x_2=\frac{1}{3}x_1
$$

按照上面两个过程不断交替的优化 $x_1$ 和 $x_2$，直到函数收敛。

通过下面的图可以看出优化的过程，因为每次只优化一个变量，每次迭代的方向都是沿着坐标轴方向的。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-06-15-54-42.png" alt="SVM-2022-02-06-15-54-42" style="zoom:30%;" /></div>

因为每次只是做一维优化，所以每个循环中的优化过程的效率是很高的, 但是迭代的次数会比较多。

回到 SMO 算法，为了后面表示方便，定义 $K_{ij} = \phi(x_i) \cdot \phi(x_j)$，将所有的常量 $\alpha_3, \alpha_4, \dots, \alpha_m$ 等都合并成常数项 $C$，这样目标优化函数变成：

$$
\min\limits_{\alpha_1, \alpha_2} \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 +y_1y_2K_{12}\alpha_1 \alpha_2 +y_1\alpha_1\sum\limits_{i=3}^{m}y_i\alpha_iK_{i1} + y_2\alpha_2\sum\limits_{i=3}^{m}y_i\alpha_iK_{i2}-(\alpha_1 + \alpha_2)+C
$$ 

$$
s.t.\text{ } \alpha_1y_1 + \alpha_2y_2 = -\sum\limits_{i=3}^{m}\alpha_iy_i = \varsigma 
$$ 

$$
0 \leq \alpha_i \leq C (i =1,2)
$$

其中，当 $i=j$ 时，$y_iy_j = 1$。

### 3. SMO 算法目标函数的优化

为了求解上面含有这两个变量的目标优化问题，首先分析约束条件，所有的 $\alpha_1, \alpha_2$ 都要满足约束条件，然后在约束条件下求最小。

根据上面的约束条件 $\alpha_1y_1 + \alpha_2y_2 = \varsigma$，$0 \leq \alpha_i \leq C (i =1,2)$，又由于 $y_1,y_2$ 均只能取值 1 或者 -1，这样 $\alpha_1,\alpha_2$ 在 $[0,C]$ 和 $[0,C]$ 形成的盒子里面，并且两者的关系直线的斜率只能为 1 或者 -1，即 $\alpha_1,\alpha_2$ 的关系直线平行于 $[0,C]$ 和 $[0,C]$ 形成的盒子的对角线，如下图所示：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-06-19-14-56.png" alt="SVM-2022-02-06-19-14-56" style="zoom:100%;" /></div>

由于 $\alpha_1,\alpha_2$ 的关系被限制在盒子里的一条线段上，所以两变量的优化问题实际上仅仅是一个变量的优化问题，不妨假设最终是 $\alpha_2$ 的优化问题。由于 SMO 采用的是启发式的迭代法，假设上一轮迭代得到的解是 $\alpha_1^{old},\alpha_2^{old}$，沿着约束方向 $\alpha_2$ 未经剪辑的解是 $\alpha_2^{new,unc}$。本轮迭代完成后的解为 $\alpha_1^{new},\alpha_2^{new}$。

由于 $\alpha_2^{new}$ 必须满足上图中的线段约束。假设 L 和 H 分别是上图中 $\alpha_2^{new}$ 所在的线段的边界。那么显然有：

$$
L \leq \alpha_2^{new} \leq H
$$

而对于 L 和 H，也有限制条件，如果是上面左图中的情况，则：

$$
L = max(0, \alpha_2^{old}-\alpha_1^{old})H = min(C, C+\alpha_2^{old}-\alpha_1^{old})
$$

如果是上面右图中的情况，有：

$$
L = max(0, \alpha_2^{old}+\alpha_1^{old}-C) H = min(C, \alpha_2^{old}+\alpha_1^{old})
$$

假如通过求导得到的 $\alpha_2^{new,unc}$，则最终的 $\alpha_2^{new}$ 应该为：

$$
\alpha_2^{new}=  
\begin{cases}  H& { \alpha_2^{new,unc} > H} \\ \alpha_2^{new,unc}& {L \leq \alpha_2^{new,unc} \leq H} \\  L& {\alpha_2^{new,unc} < L}  
\end{cases}
$$

求解 $\alpha_2^{new,unc}$ 的方法很简单，只需要将目标函数对 $\alpha_2$ 求偏导数即可。

首先整理下目标函数，为了简化叙述，令：

$$
E_i = g(x_i)-y_i = \sum\limits_{j=1}^{m}\alpha_j^{\ast}y_jK(x_i,x_j)+ b - y_i
$$

其中 $g(x)$：

$$
g(x) = w^{\ast} \cdot \phi(x) + b =\sum\limits_{j=1}^{m}\alpha_j^{\ast}y_jK(x, x_j)+ b^{\ast}
$$

令：

$$
v_i = \sum\limits_{j=3}^{m}y_j\alpha_jK(x_i,x_j) = g(x_i) - \sum\limits_{j=1}^{2}y_j\alpha_jK(x_i,x_j) -b
$$

这样优化目标函数进一步简化为：

$$
W(\alpha_1,\alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 +y_1y_2K_{12}\alpha_1 \alpha_2 -(\alpha_1 + \alpha_2) +y_1\alpha_1v_1 + y_2\alpha_2v_2
$$

由于 $\alpha_1y_1 + \alpha_2y_2 = \varsigma$，并且 $y_i^2 = 1$，可以得到 $\alpha_1$ 用 $\alpha_2$ 表达的式子为：

$$
\alpha_1 = y_1(\varsigma - \alpha_2y_2)
$$

将上式带入目标优化函数，就可以消除 $\alpha_1$，得到仅仅包含 $\alpha_2$ 的式子：

$$
W(\alpha_2) =\frac{1}{2}K_{11}(\varsigma - \alpha_2y_2)^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_2K_{12}(\varsigma - \alpha_2y_2) \alpha_2 - (\varsigma - \alpha_2y_2)y_1 - \alpha_2 +(\varsigma - \alpha_2y_2)v_1 + y_2\alpha_2v_2
$$

接下来求导得到 $\alpha_2^{new,unc}$：

$$
\frac{\partial W}{\partial \alpha_2} = K_{11}\alpha_2 + K_{22}\alpha_2 -2K_{12}\alpha_2 - K_{11}\varsigma y_2 + K_{12}\varsigma y_2 +y_1y_2 -1 -v_1y_2 +y_2v_2 = 0
$$

整理上式有：

$$
(K_{11} +K_{22}-2K_{12}) \alpha_2 = y_2(y_2-y_1 + \varsigma K_{11} - \varsigma K_{12} + v_1 - v_2) \\ = y_2(y_2-y_1 + \varsigma K_{11} - \varsigma K_{12} + (g(x_1) - \sum\limits_{j=1}^{2}y_j\alpha_jK_{1j} -b ) -(g(x_2) - \sum\limits_{j=1}^{2}y_j\alpha_jK_{2j} -b))
$$

将 $\varsigma = \alpha_1y_1 + \alpha_2y_2$ 带入上式，有：

$$
(K_{11} +K_{22}-2K_{12})\alpha_2^{new,unc} = y_2((K_{11}+K_{22}-2K_{12})\alpha_2^{old}y_2 +y_2-y_1 +g(x_1) - g(x_2)) \\ = (K_{11} +K_{22}-2K_{12}) \alpha_2^{old} + y_2(E_1-E_2)
$$

最终得到 $\alpha_2^{new,unc}$ 的表达式：

$$
\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1-E_2)}{K_{11} +K_{22}-2K_{12})}
$$

利用上面讲到的 $\alpha_2^{new,unc}$ 和 $\alpha_2^{new}$ 的关系式，就可以得到新的 $\alpha_2^{new}$；利用 $\alpha_2^{new}$ 和 $\alpha_1^{new}$ 的线性关系，可以得到新的 $\alpha_1^{new}$。

### 4. SMO 算法两个变量的选择

SMO 算法需要选择合适的两个变量做迭代，其余的变量做常量来进行优化，那么怎么选择这两个变量呢？

#### 4.1 第一个变量的选择

SMO 算法称选择第一个变量为外层循环，这个变量需要选择在训练集中违反 KKT 条件最严重的样本点。对于每个样本点，要满足的 KKT 条件：

$$
\alpha_{i}^{\ast} = 0 \Rightarrow y_ig(x_i) \geq 1 
$$ 

$$
0 < \alpha_{i}^{\ast} < C \Rightarrow y_ig(x_i) =1
$$ 

$$
\alpha_{i}^{\ast}= C \Rightarrow y_ig(x_i) \leq 1
$$

一般来说，首先选择违反 $0 < \alpha_{i}^{\ast} < C \Rightarrow y_ig(x_i) =1$ 这个条件的点。如果这些支持向量都满足 KKT 条件，再选择违反 $\alpha_{i}^{\ast} = 0 \Rightarrow y_ig(x_i) \geq 1$ 和 $\alpha_{i}^{\ast}= C \Rightarrow y_ig(x_i) \leq 1$ 的点。

### 4.2 第二个变量的选择

SMO 算法称选择第二个变量为内层循环，假设外层循环是 $\alpha_1$，第二个变量 $\alpha_2$ 的选择标准是让 $\vert E1-E2 \vert$ 有足够大的变化。由于 $\alpha_1$ 定了的时候，$E_1$ 也确定了，所以要想 $\vert E1-E2 \vert$ 最大，只需要在 $E_1$ 为正时，选择最小的 $E_i$ 作为 $E_2$，在 $E_1$ 为负时，选择最大的 $E_i$ 作为 $E_2$，可以将所有的 $E_i$ 保存下来加快迭代。

如果内存循环找到的点不能让目标函数有足够的下降，可以采用遍历支持向量点来做 $\alpha_2$，直到目标函数有足够的下降，如果所有的支持向量做 $\alpha_2$ 都不能让目标函数有足够的下降，那么就要跳出循环，重新选择 $\alpha_1$。

### 4.3 计算阈值 b 和差值 $E_i$

在每次完成两个变量的优化之后，需要重新计算阈值 b。当 $0 < \alpha_{1}^{new} < C$时，有： 

$$
y_1 - \sum\limits_{i=1}^{m}\alpha_iy_iK_{i1} -b_1 = 0
$$

于是新的 $b_1^{new}$ 为：

$$
b_1^{new} = y_1 - \sum\limits_{i=3}^{m}\alpha_iy_iK_{i1} - \alpha_{1}^{new}y_1K_{11} - \alpha_{2}^{new}y_2K_{21}
$$

计算出 $E_1$ 为：

$$
E_1 = g(x_1) - y_1 = \sum\limits_{i=3}^{m}\alpha_iy_iK_{i1} + \alpha_{1}^{old}y_1K_{11} + \alpha_{2}^{old}y_2K_{21} + b^{old} -y_1
$$

可以看到上两式都有 $y_1 - \sum\limits_{i=3}^{m}\alpha_iy_iK_{i1}$，因此可以将 $b_1^{new}$ 用 $E_1$ 表示为：

$$
b_1^{new} = -E_1 -y_1K_{11}(\alpha_{1}^{new} - \alpha_{1}^{old}) -y_2K_{21}(\alpha_{2}^{new} - \alpha_{2}^{old}) + b^{old}
$$

同样的，如果 $0 < \alpha_{2}^{new} < C$，那么有：

$$
b_2^{new} = -E_2 -y_1K_{12}(\alpha_{1}^{new} - \alpha_{1}^{old}) -y_2K_{22}(\alpha_{2}^{new} -
\alpha_{2}^{old}) + b^{old}
$$

最终的 $b^{new}$ 为：

$$
b^{new} = \frac{b_1^{new} + b_2^{new}}{2}
$$

得到了 $b^{new}$ 需要更新 $E_i$：

$$
E_i = \sum\limits_{S}y_j\alpha_jK(x_i,x_j) + b^{new} -y_i
$$

其中，$S$ 是所有支持向量 $x_j$ 的集合。

### 5. SMO 算法总结

输入是 $m$ 个样本 ${(x_1,y_1), (x_2,y_2), \dots,(x_m,y_m),}$，其中 $x$ 为 $n$ 维特征向量。$y$ 为二元输出，值为 1，或者 -1，精度为 e；输出是近似解 $\alpha$。

1. 取初值 $\alpha^{0} = 0, k =0$
2. 按照 4.1 节的方法选择 $\alpha_1^k$，接着按照 4.2 节的方法选择 $\alpha_2^k$，求出新的$\alpha_2^{new,unc}$。

$$
\alpha_2^{new,unc} = \alpha_2^{k} + \frac{y_2(E_1-E_2)}{K_{11} +K_{22}-2K_{12})}
$$

3. 按照下式求出 $\alpha_2^{k+1}$

$$
\alpha_2^{k+1}=  
\begin{cases}  
H & { \alpha_2^{new,unc} > H} \\  
\alpha_2^{new,unc} & {L \leq \alpha_2^{new,unc} \leq H} \\  
L & {\alpha_2^{new,unc} < L}  
\end{cases}
$$

4. 利用 $\alpha_2^{k+1}$ 和 $\alpha_1^{k+1}$ 的关系求出 $\alpha_1^{k+1}$。
5. 按照 4.3 节的方法计算 $b^{k+1}$ 和 $E_i$
6. 在精度 e 范围内检查是否满足如下的终止条件：

$$
\sum\limits_{i=1}^{m} \alpha_i y_i = 0
$$ 

$$
0 \leq \alpha_i \leq C, i = 1,2\dots, m
$$ 

$$
\alpha_{i}^{k+1} = 0 \Rightarrow y_ig(x_i) \geq 1
$$ 

$$
0 <\alpha_{i}^{k+1} < C \Rightarrow y_ig(x_i) = 1
$$

$$
\alpha_{i}^{k+1}= C \Rightarrow y_ig(x_i) \leq 1
$$

7. 如果满足则结束，返回$\alpha^{k+1}$, 否则转到步骤 2。

SMO 算法至此结束。

## 五、线性支持回归

前四章在讨论 SVM 的线性分类和非线性分类，以及在分类时用到的算法，实际上 SVM 也可以用于回归模型，重点关注 SVM 分类和 SVM 回归的相同点与不同点。

### 1. SVM 回归模型的损失函数度量

回顾 SVM 分类模型中，目标函数是让 $\frac{1}{2}\Vert w \Vert_2^2$ 最小，同时让各个训练集中的点尽量远离自己类别一边的的支持向量，即 $y_i(w \cdot \phi(x_i )+ b) \geq 1$。如果加入一个松弛变量 $\xi_i \geq 0$，则目标函数是 $\frac{1}{2}\Vert w \Vert_2^2 +C\sum\limits_{i=1}^{m}\xi_i$，对应的约束条件变成：$y_i(w \cdot \phi(x_i ) + b ) \geq 1 - \xi_i$。

对于回归模型，没有类别，优化目标函数可以继续和 SVM 分类模型保持一致为 $\frac{1}{2}\Vert w \Vert_2^2$，但是约束条件不可能是让各个训练集中的点尽量远离自己类别一边的的支持向量。那么回归模型的目标是让训练集中的每个点 $(x_i,y_i)$，尽量拟合到一个线性模型 $y_i = w \cdot \phi(x_i ) +b$。一般的回归模型使用均方差作为损失函数，但是 SVM 则使用不同的损失函数。

SVM 需要定义一个常量 $\epsilon > 0$，对于某一个点 $(x_i,y_i)$，如果 $\vert y_i - w \cdot \phi(x_i)-b \vert \leq \epsilon$，则完全没有损失，如果 $\vert y_i - w \cdot \phi(x_i ) -b \vert >\epsilon$，则对应的损失为 $\vert y_i - w \cdot \phi(x_i ) -b\vert  -\epsilon$，这与均方差损失函数不同，均方差只要 $y_i - w \cdot \phi(x_i ) -b \neq 0$，那么就会有损失。

如下图所示，在蓝色条带里面的点都是没有损失的，但是外面的点的是有损失的，损失大小为红色线的长度。

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/SVM-2022-02-06-20-33-07.png" alt="SVM-2022-02-06-20-33-07" style="zoom:100%;" /></div>

SVM 回归模型的损失函数度量为：

$$ 
err(x_i,y_i) =  
\begin{cases}  
0 & {\vert y_i - w \cdot \phi(x_i ) -b\vert  \leq \epsilon} \\  
\vert y_i - w \cdot \phi(x_i ) -b\vert  - \epsilon & {\vert y_i - w \cdot \phi(x_i ) -b\vert  > \epsilon}  
\end{cases}
$$

### 2. SVM 回归模型的目标函数的原始形式

定义 SVM 回归模型的目标函数如下：

$$
\min \frac{1}{2}\Vert w \Vert_2^2
$$

$$
s.t. \text{ } \vert y_i - w \cdot \phi(x_i ) -b\vert  \leq \epsilon (i =1,2,\dots,m)
$$

与 SVM 分类模型相似，回归模型也可以对每个样本 $(x_i,y_i)$ 加入松弛变量 $\xi_i \geq 0$，由于这里用的是绝对值，实际上是两个不等式，即两边都需要松弛变量，定义为 $\xi_i^{\lor}, \xi_i^{\land}$，则 SVM 回归模型的损失函数度量在加入松弛变量之后变为：

$$
min \frac{1}{2}\Vert w \Vert_2^2 + C\sum\limits_{i=1}^{m}(\xi_i^{\lor}+ \xi_i^{\land})
$$ 

$$
s.t.  -\epsilon - \xi_i^{\lor} \leq y_i - w \cdot \phi(x_i ) -b \leq \epsilon + \xi_i^{\land}
$$ 

$$
\xi_i^{\lor} \geq 0,  \xi_i^{\land} \geq 0 (i = 1,2,\dots,m)
$$

与 SVM 分类模型相似，可以用拉格朗日函数将目标优化函数变成无约束的形式：

$$
L(w,b,\alpha^{\lor}, \alpha^{\land}, \xi_i^{\lor}, \xi_i^{\land}, \mu^{\lor}, \mu^{\land}) = \frac{1}{2}\Vert w \Vert_2^2 + C\sum\limits_{i=1}^{m}(\xi_i^{\lor}+ \xi_i^{\land}) + \sum\limits_{i=1}^{m}\alpha^{\lor}(-\epsilon - \xi_i^{\lor} -y_i + w \cdot \phi(x_i) + b) + \sum\limits_{i=1}^{m}\alpha^{\land}(y_i - w \cdot \phi(x_i) - b -\epsilon - \xi_i^{\land}) - \sum\limits_{i=1}^{m}\mu^{\lor}\xi_i^{\lor} - \sum\limits_{i=1}^{m}\mu^{\land}\xi_i^{\land}
$$

其中 $\mu^{\lor} \geq 0, \mu^{\land} \geq 0, \alpha_i^{\lor} \geq 0, \alpha_i^{\land} \geq 0$，均为拉格朗日系数。

### 3. SVM 回归模型的目标函数的对偶形式

SVM 回归模型的目标的目标是

$$
\min\limits_{w,b,\xi_i^{\lor}, \xi_i^{\land}} \max\limits_{\mu^{\lor} \geq 0, \mu^{\land} \geq 0, \alpha_i^{\lor} \geq 0, \alpha_i^{\land} \geq 0} L(w,b,\alpha^{\lor}, \alpha^{\land}, \xi_i^{\lor}, \xi_i^{\land}, \mu^{\lor}, \mu^{\land}) 
$$

与 SVM 分类模型一样，这个优化目标也满足 KKT 条件，可以通过拉格朗日对偶将优化问题转化为等价的对偶问题来求解：

$$
\max\limits_{\mu^{\lor} \geq 0, \mu^{\land} \geq 0, \alpha_i^{\lor} \geq 0, \alpha_i^{\land} \geq 0} \min\limits_{w,b,\xi_i^{\lor}, \xi_i^{\land}}L(w,b,\alpha^{\lor}, \alpha^{\land}, \xi_i^{\lor}, \xi_i^{\land}, \mu^{\lor}, \mu^{\land}) 
$$

可以先求优化函数对于 $w,b,\xi_i^{\lor}, \xi_i^{\land}$ 的极小值，接着再求拉格朗日乘子 $\alpha^{\lor}, \alpha^{\land}, \mu^{\lor}, \mu^{\land}$ 的极大值。

首先来求优化函数对于 $w,b,\xi_i^{\lor}, \xi_i^{\land}$ 的极小值，可以通过求偏导数求得：

$$
\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum\limits_{i=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor})\phi(x_i)
$$

$$
\frac{\partial L}{\partial b} = 0 \Rightarrow \sum\limits_{i=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor}) = 0
$$

$$
\frac{\partial L}{\partial \xi_i^{\lor}} = 0 \Rightarrow C-\alpha^{\lor}-\mu^{\lor} = 0
$$

$$
\frac{\partial L}{\partial \xi_i^{\land}} = 0 \Rightarrow C-\alpha^{\land}-\mu^{\land} = 0
$$

将上面 4 个式子带入 $L(w,b,\alpha^{\lor}, \alpha^{\land}, \xi_i^{\lor}, \xi_i^{\land}, \mu^{\lor}, \mu^{\land}) $ 中消去 $w,b,\xi_i^{\lor}, \xi_i^{\land}$。

看似很复杂，其实消除过程和系列第一章第二章类似，最终得到的对偶形式为：

$$
\max\limits_{\alpha^{\lor}, \alpha^{\land}} -\sum\limits_{i=1}^{m}(\epsilon- y_i)\alpha_i^{\land}+ (\epsilon+y_i)\alpha_i^{\lor}) - \frac{1}{2}\sum\limits_{i=1,j=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor})(\alpha_j^{\land} - \alpha_j^{\lor})K_{ij} 
$$ 

$$
s.t.  \sum\limits_{i=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor}) = 0
$$ 

$$
0 < \alpha_i^{\lor} < C  (i =1,2,\dots,m)
$$ 

$$
0 < \alpha_i^{\land} < C  (i =1,2,\dots,m)
$$  

对目标函数取负号，求最小值可以得到和 SVM 分类模型类似的求极小值的目标函数如下：

$$
\min\limits_{\alpha^{\lor}, \alpha^{\land}} \frac{1}{2}\sum\limits_{i=1,j=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor})(\alpha_j^{\land} - \alpha_j^{\lor})K_{ij} + \sum\limits_{i=1}^{m}(\epsilon-y_i)\alpha_i^{\land}+ (\epsilon+y_i)\alpha_i^{\lor}
$$ 

$$
s.t.  \sum\limits_{i=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor}) = 0
$$ 

$$
0 < \alpha_i^{\lor} < C  (i =1,2,\dots,m)
$$ 

$$
0 < \alpha_i^{\land} < C  (i =1,2,\dots,m)
$$

对于这个目标函数，依然可以用 SMO 算法来求出对应的 $\alpha^{\lor}, \alpha^{\land}$，进而求出回归模型系数 $w, b$。

### 4. SVM 回归模型系数的稀疏性

在 SVM 分类模型中，KKT 条件的对偶互补条件为： 

$$
\alpha_{i}^{\ast}(y_i(w \cdot \phi(x_i) + b) -1+\xi_i^{\ast}) = 0
$$

而在回归模型中，对偶互补条件类似如下：

$$
\alpha_i^{\lor}(\epsilon + \xi_i^{\lor} + y_i - w \cdot \phi(x_i ) - b ) = 0
$$

$$
\alpha_i^{\land}(\epsilon + \xi_i^{\land} - y_i + w \cdot \phi(x_i ) + b ) = 0
$$

根据松弛变量定义条件，如果 $\vert y_i - w \cdot \phi(x_i ) -b\vert  < \epsilon$，则 $\xi_i^{\lor} = 0, \xi_i^{\land}= 0$，此时 

$$
\epsilon + \xi_i^{\lor} + y_i - w \cdot \phi(x_i ) - b \neq 0
$$

$$
\epsilon + \xi_i^{\land} - y_i + w \cdot \phi(x_i ) + b \neq 0
$$ 

这样要满足对偶互补条件，只有 $\alpha_i^{\lor} = 0, \alpha_i^{\land} = 0$。

定义样本系数

$$
\beta_i =\alpha_i^{\land}-\alpha_i^{\lor} 
$$

根据 $w$ 的计算式 $w = \sum\limits_{i=1}^{m}(\alpha_i^{\land} - \alpha_i^{\lor})\phi(x_i)$，可以发现此时$\beta_i = 0, w$ 不受这些在误差范围内的点的影响，对于在边界上或者在边界外的点，$\alpha_i^{\lor} \neq 0, \alpha_i^{\land} \neq 0$，此时 $\beta_i \neq 0$。

### 5. SVM 算法小结

SVM 算法至此结束，接下来对 SVM算法做一个总结。

SVM 算法是一个很优秀的算法，在集成学习和神经网络之类的算法没有表现出优越性能前，SVM 基本占据了分类模型的统治地位。目前则是在大数据时代的大样本背景下，SVM 由于其在大样本时超级大的计算量，热度有所下降，但是仍然是一个常用的机器学习算法。

SVM 算法的主要优点有：

1. 解决高维特征的分类问题和回归问题很有效，在特征维度大于样本数时依然有很好的效果。
2. 仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。
3. 有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。
4. 样本量不是海量数据的时候，分类准确率高，泛化能力强。

SVM 算法的主要缺点有：

1. 如果特征维度远远大于样本数，则 SVM 表现一般。
2. SVM 在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。
3. 非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。
4. SVM 对缺失数据敏感。
