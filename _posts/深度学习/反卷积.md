# 反卷积与空洞卷积

反卷积也可以称为卷积转置或转置卷积，但其并非卷积操作的反向操作。由上边的介绍可以看出，卷积操作会将输入映射到一个更小的 feature map 中，那么反卷积则可以将这个小的 feature map 映射为一个大的 feature map，但是切记，这不是卷积的反向操作，也不会得到与原始输入一样的输出，但是却保留了映射中相对位置关系的信息，我们可以将其理解为上采样。

实际上，上采样（upsampling）一般包括 2 种方式：

1. Resize，如双线性插值直接缩放，类似于图像缩放（这种方法在原文中提到）
2. Deconvolution，也叫 Transposed Convolution。

传统的网络是 subsampling 的，对应的输出尺寸会降低；upsampling 的意义在于将小尺寸的高维度 feature map 恢复回去，以便做 pixelwise prediction，获得每个点的分类信息。

反卷积（转置卷积）通常用在三个方面：

1. CNN 可视化，通过反卷积将卷积得到的 feature map 还原到像素空间，来观察 feature map 对哪些 pattern 相应最大，即可视化哪些特征是卷积操作提取出来的；
2. FCN 全卷积网络中，由于要对图像进行像素级的分割，需要将图像尺寸还原到原来的大小，类似 upsampling 的操作，所以需要采用反卷积；
3. GAN 对抗式生成网络中，由于需要从输入图像到生成图像，自然需要将提取的特征图还原到和原图同样尺寸的大小，即也需要反卷积操作。

卷积和反卷积的图：

- 卷积 (convolution)：卷积核为 3x3；no padding , strides=1

<div align=center><img src="/assets/Convolution.gif" alt="卷积" style="zoom:100%;" /></div>

- 反卷积 (the transpose of conv) 可以理解为 upsample conv。卷积核为：3x3; no padding , strides=1

<div align=center><img src="/assets/Deconvolution.gif" alt="反卷积" style="zoom:100%;" /></div>

在实际计算过程中，我们要转化为矩阵的乘积的形式，一个转化为 Toeplitz matrix，一个 reshape 为列矩阵。

比如 input = [3,3]，Reshape 之后，为 A = [1,9]。B（可以理解为滤波器）= [9,4](Toeplitz matrix)，那么 A*B = C = [1,4]。Reshape C = [2，2]。所以，通过 B 卷积，我们从 shape = [3,3] 变成了 shape = [2,2]。

反过来，输入 A=[2,2]，reshape 之后为 [1,4]。B 的转置为 [4,9]，那么 A*B=C=[1,9],reshape 为 [3,3]。所以，通过 B 的转置 - "反卷积"，我们从 shape=[2,2] 得到了 shape=[3,3]

也就是输入 feature map A=[3,3] 经过了卷积滤波 B=[2,2] 输出为 [2,2] , 所以 padding=0,stride=1。反卷积则是输入 feature map A=[2,2], 经过了反卷积滤波 B=[2,2]. 输出为 [3,3].padding=0,stride=1。

#### 卷积矩阵

顾名思义就是把卷积操作写成一个矩阵的形式，通过一次矩阵乘法就可以完成整个卷积操作。卷积矩阵的构造是通过对卷积核的重排列构造的。例如对于 3x3 的卷积核：

$$
\left[\begin{array}{rrr}
1 & 4 & 1 \\
1 & 4 & 3 \\
3 & 3 & 1
\end{array}\right]
$$

可以重排列为一个 4x16 的卷积矩阵：

$$
\left[\begin{array}{rrrr}
1 & 4 & 1 & 0 & 1 & 4 & 3 & 0 & 3 & 3 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 4 & 1 & 0 & 1 & 4 & 3 & 0 & 3 & 3 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 4 & 1 & 0 & 1 & 4 & 3 & 0 & 3 & 3 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 4 & 1 & 0 & 1 & 4 & 3 & 0 & 3 & 3 & 1
\end{array}\right]
$$

先来看正常卷积。设该输入矩阵为：

$$
\left[\begin{array}{rrrr}
4 & 5 & 8 & 7 \\
1 & 8 & 8 & 8 \\
3 & 6 & 6 & 4 \\
6 & 5 & 7 & 8
\end{array}\right]
$$

以 1 为步长没有填充，那么卷积结果为：

$$
\left[\begin{array}{rr}
122 & 148 \\
126 & 134 
\end{array}\right]
$$

将输入矩阵转换为一个 1x16 的列向量，那么该列向量与卷积矩阵相乘会得到一个 1x4 的向量，再将该向量 reshape 为 2x2 的矩阵即为卷积结果。

$$
\left[\begin{array}{r}
4 & 5 & 8 & 7 & 1 & 8 & 8 & 8 & 3 & 6 & 6 & 4 & 6 & 5 & 7 & 8
\end{array}\right]
$$

与卷积矩阵相乘后得：

$$
\left[\begin{array}{r}
122 & 148 & 126 & 134 
\end{array}\right]
$$

再 reshape 则结果为：

$$
\left[\begin{array}{rr}
122 & 148 \\
126 & 134 
\end{array}\right]
$$

#### 转置卷积

那么转置卷积就是将卷积矩阵进行转置，那么就可以得到一个 16x4 的转置卷积矩阵，对于输出的 2x2 的 feature map，reshape 为 4x1，再将二者相乘即可得到一个 16x1 的转置卷积的结果，此时再 reshape 即可得到一个 4x4 的输出。

$$
\left[\begin{array}{llll}
1 & 0 & 0 & 0 \\
4 & 1 & 0 & 0 \\
1 & 4 & 0 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
4 & 1 & 4 & 1 \\
3 & 4 & 1 & 4 \\
0 & 3 & 0 & 1 \\
3 & 0 & 1 & 0 \\
3 & 3 & 4 & 1 \\
1 & 3 & 3 & 4 \\
0 & 1 & 0 & 3 \\
0 & 0 & 3 & 0 \\
0 & 0 & 3 & 3 \\
0 & 0 & 1 & 3 \\
0 & 0 & 0 & 1
\end{array}\right] \times\left[\begin{array}{c}
122 \\
148 \\
126 \\
134
\end{array}\right]=\left[\begin{array}{c}
2 \\
9 \\
6 \\
1 \\
6 \\
29 \\
30 \\
7 \\
10 \\
29 \\
33 \\
13 \\
12 \\
24 \\
16 \\
4
\end{array}\right]
$$

输出为：

$$
\left[\begin{array}{rrrr}
2 & 9 & 6 & 1 \\
6 & 29 & 30 & 7 \\
10 & 29 & 33 & 13 \\
12 & 24 & 16 & 4
\end{array}\right]
$$

这样就通过转置卷积将 2x2 的矩阵反卷为一个 4x4 的矩阵，但是从结果也可以看出反卷积的结果与原始输入信号不同。只是保留了位置信息，以及得到了想要的形状。

反卷积可以应用在生成对抗网络 (GAN) 的生成器上，可以参考 DCGAN 进行理解。

使用转置卷积时会出现棋盘格伪影：

<div align=center><img src="/assets/反卷积-2022-03-30-23-39-32.png" alt="反卷积-2022-03-30-23-39-32" style="zoom:100%;" /></div>

棋盘状伪影是由转置卷积的“不均匀重叠”引起的。这种重叠使得更多的隐喻性绘画在某些地方比其他地方更多。

在下图中，顶部的图层是输入图层，底部的图层是转置卷积后的输出图层。在转置卷积期间，具有较小尺寸的层被映射到具有较大尺寸的层。

<div align=center><img src="/assets/反卷积-2022-03-30-23-39-54.png" alt="反卷积-2022-03-30-23-39-54" style="zoom:100%;" /></div>

在示例 (a) 中，步长为 1 且 filters 大小为 2。如红色所示，输入上的第一个像素映射到输出上的第一个和第二个像素。如绿色所示，输入上的第二个像素映射到输出上的第二个和第三个像素。输出上的第二个像素从输入上的第一个和第二个像素接收信息。总的来说，输出中间部分（有效输出）的像素从输入端接收相同数量的信息，这里存在中心重叠的区域。

在示例 (b) 中 filters 大小增加到 3 时，接收最多信息的中心部分收缩。但这可能不是什么大问题，因为重叠仍然是均匀的，输出中心部分的像素从输入接收相同数量的信息。

现在，对于下面的示例，我们更改 stride = 2。

<div align=center><img src="/assets/反卷积-2022-03-30-23-41-11.png" alt="反卷积-2022-03-30-23-41-11" style="zoom:100%;" /></div>

在示例 (a) 中，filter 的大小 = 2，输出上的所有像素都从输入接收相同数量的信息。它们都从输入上的单个像素接收信息，这里没有转置卷积的重叠。

如果我们在示例 (b) 中将 filters 的大小更改为 4，则均匀重叠的区域会缩小。但是，仍然可以使用输出的中心部分作为有效输出，其中每个像素从输入接收相同数量的信息。

但是，如果我们在示例 (c) 和 (d) 中将 filters 的大小更改为 3 和 5，事情会变得很有趣。对于这两种情况，输出上的每个像素与其相邻像素相比接收不同数量的信息，人们无法在输出上找到连续且均匀重叠的区域。

**当 filters 的大小不能被步长整除时，转置卷积有不均匀的重叠**。这种“不均匀的重叠”使得某些地方的涂料比其他地方更多，从而产生棋盘效果。事实上，不均匀重叠的区域在两个维度上往往更加极端。在那里，两个模式相乘，不均匀性变成它们的平方。

在应用转置卷积的同时，可以做两件事来减少此类伪像。**首先，请确保使用的 filters 的大小能被步长整除，以避免不均匀的重叠问题**。其次，**可以使用步长=1 的转置卷积，这有助于减少棋盘效应**。但是，正如许多最新模型中所看到的那样，伪影仍可能出现。

[这篇文章](https://link.zhihu.com/?target=https%3A//distill.pub/2016/deconv-checkerboard/) 提出了一种更好的上采样方法：首先，调整图像的大小（使用最近邻插值或双线性插值），然后做一个卷积层，这样有效避免了棋盘效应。

# 空洞卷积

扩张卷积（空洞卷积）

系统能以相同的计算成本，提供更大的感受野，扩张卷积在实时分割领域特别受欢迎。 在需要更大的观察范围，且无法承受多个卷积或更大的 kennels，可以用它。

<div align=center><img src="/assets/Convolution.gif" alt="卷积" style="zoom:100%;" /></div>

标准的离散卷积计算公式如下：

$$
(F * k)(\boldsymbol{p})=\sum_{\boldsymbol{s}+\boldsymbol{t}=\boldsymbol{p}} F(\boldsymbol{s}) k(\boldsymbol{t})
$$

扩张卷积的计算公式如下：

$$
\left(F *_{l} k\right)(\boldsymbol{p})=\sum_{s+l \boldsymbol{t}=\boldsymbol{p}} F(\boldsymbol{s}) k(\boldsymbol{t})
$$

<div align=center><img src="/assets/Dilated_Convolution.gif
" alt="空洞卷积" style="zoom:100%;" /></div>

当 $l=1$ 时，扩张卷积变成标准的卷积。

<div align=center><img src="/assets/反卷积-2022-03-31-00-09-49.png" alt="反卷积-2022-03-31-00-09-49" style="zoom:100%;" /></div>

直观上，空洞卷积通过在卷积核部分之间插入空间让卷积核「膨胀」。这个增加的参数 $l$ （空洞率）表明了我们想要将卷积核放宽到多大。下图显示了当 $l=1,2,4$  时的卷积核大小（当 $l=1$ 时，空洞卷积就变成了一个标准的卷积）。

### 重新思考卷积： Rethinking Convolution

在赢得其中一届 ImageNet 比赛里 VGG 网络的文章中，最大的贡献并不是 VGG 网络本身，而是对于卷积叠加的一个巧妙观察。

> This (stack of three 3 × 3 conv layers) can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).

这里意思是 7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因。

<div align=center><img src="/assets/反卷积-2022-04-01-00-49-05.png" alt="反卷积-2022-04-01-00-49-05" style="zoom:100%;" /></div>

然而 Deep CNN 对于其他任务还有一些致命性的缺陷。较为著名的是 up-sampling 和 pooling layer 的设计。这个在 Hinton 的演讲里也一直提到过。

主要问题有：

1. Up-sampling / pooling layer (e.g. bilinear interpolation) is deterministic. (a.k.a. not learnable)
2. 内部数据结构丢失；空间层级化信息丢失。
3. 小物体信息无法重建 （假设有四个 pooling layer 则任何小于 $2^4 = 16$ pixel 的物体信息将理论上无法重建。)

在这样问题的存在下，语义分割问题一直处在瓶颈期无法再明显提高精度， 而 dilated convolution 的设计就良好的避免了这些问题。

### 空洞卷积的拯救之路：Dilated Convolution to the Rescue

这篇文章 M[ULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS ](https://arxiv.org/pdf/1511.07122.pdf) 可能是第一篇尝试用 dilated convolution 做语义分割的文章。后续图森组和 Google Brain 都对于 dilated convolution 有着更细节的讨论，推荐阅读：[Understanding Convolution for Semantic Segmentation](https://arxiv.org/abs/1702.08502)， [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)。

对于 dilated convolution，已经发现了他的优点，即内部数据结构的保留和避免使用 down-sampling 这样的特性。但是完全基于 dilated convolution 的结构如何设计则是一个新的问题。

#### 潜在问题 1：The Gridding Effect

假设我们仅仅多次叠加 dilation rate 2 的 3 x 3 kernel 的话，则会出现这个问题：

<div align=center><img src="/assets/反卷积-2022-04-01-00-53-57.png" alt="反卷积-2022-04-01-00-53-57" style="zoom:100%;" /></div>

发现 kernel 并不连续，也就是并不是所有的 pixel 都用来计算了，因此这里将信息看做 checker-board 的方式会损失信息的连续性。这对 pixel-level dense prediction 的任务来说是致命的。

#### 潜在问题 2：Long-ranged information might be not relevant.

从 dilated convolution 的设计背景来看就能推测出这样的设计是用来获取 long-ranged information。然而光采用大 dilation rate 的信息或许只对一些大物体分割有效果，而对小物体来说可能则有弊无利了。如何同时处理不同大小的物体的关系，则是设计好 dilated convolution 网络的关键。

### 通向标准化设计：Hybrid Dilated Convolution (HDC)

对于上面提到的几个问题，图森组的文章对其提出了较好的解决的方法。他们设计了一个称之为 HDC 的设计结构。

第一个特性是，叠加卷积的 dilation rate 不能有大于 1 的公约数。比如 [2, 4, 6] 则不是一个好的三层卷积，依然会出现 gridding effect。

第二个特性是，将 dilation rate 设计成**锯齿状结构**，例如 [1, 2, 5, 1, 2, 5] 循环结构。

第三个特性是，需要满足一下这个式子：

$$
M_i=\max[M_{i+1}-2r_i,M_{i+1}-2(M_{i+1}-r_i),r_i]
$$

其中 $r_i$ 是 $i$ 层的 dilation rate，而 $M_i$ 是指在 $i$ 层的最大 dilation rate，那么假设总共有 $n$ 层的话，默认 $M_n=r_n$ 。假设应用 kernel 为 $k x k$ 的话，我们的目标则是 $M_2\leq k$ ，这样我们至少可以用 dilation rate 1 即 standard convolution 的方式来覆盖掉所有洞。

一个简单的例子：dilation rate [1, 2, 5] with 3 x 3 kernel （可行的方案）。

<div align=center><img src="/assets/反卷积-2022-04-01-00-59-42.png" alt="反卷积-2022-04-01-00-59-42" style="zoom:100%;" /></div>

而这样的锯齿状本身的性质就比较好的来同时满足小物体大物体的分割要求（小 dilation rate 来关心近距离信息，大 dilation rate 来关心远距离信息）。

这样我们的卷积依然是连续的也就依然能满足 VGG 组观察的结论，大卷积是由小卷积的 regularisation 的叠加。

以下的对比实验可以明显看出，一个良好设计的 dilated convolution 网络能够有效避免 gridding effect.

<div align=center><img src="/assets/反卷积-2022-04-01-01-00-15.png" alt="反卷积-2022-04-01-01-00-15" style="zoom:100%;" /></div>
