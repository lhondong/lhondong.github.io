---
title: "循环神经网络"
subtitle: "循环神经网络前向反向传播算法"
layout: post
author: "L Hondong"
header-img: "img/post-bg-34.jpg"
mathjax: true
tags:
  - 机器学习
  - 深度学习
---

# 循环神经网络前向反向传播算法

RNN 假设样本是基于序列的，比如是序列索引 1 到序列索引 $\tau$ 。对于这其中的任意序列索引号 $t$, 它对应的输入是对应的样本序列中的 $x^{(t)}$。而模型在序列索引号 $t$ 位置的隐藏状态 $h^{(t)}$，则由 $x^{(t)}$ 和在 $t-1$ 位置的隐藏状态 $h^{(t-1)}$ 共同决定。在任意序列索引号 $t$，都有对应的模型预测输出 $o^{(t)}$。通过预测输出 $o^{(t)}$ 和训练序列真实输出 $y^{(t)}$, 以及损失函数 $L^{(t)}$，就可以用 DNN 类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出。

## RNN 模型

<div align=center><img src="https://cdn.jsdelivr.net/gh/lhondong/Assets/Images/循环神经网络前向反向传播算法-2022-01-29-17-59-46.png" alt="循环神经网络前向反向传播算法-2022-01-29-17-59-46" style="zoom:50%;" /></div>

上图中左边是 RNN 模型没有按时间展开的图，右边是按时间序列展开的图，描述了在序列索引号 $t$ 附近 RNN 的模型。其中：

1. $x^{(t)}$ 代表在序列索引号 $t$ 时训练样本的输入。同样的，$x^{(t-1)}$ 和 $x^{(t+1)}$ 代表在序列索引号 $t-1$ 和 $t+1$ 时训练样本的输入。
2. $h^{(t)}$ 代表在序列索引号 $t$ 时模型的隐藏状态。$h^{(t)}$ 由 $x^{(t)}$ 和 $h^{(t-1)}$ 共同决定。
3. $o^{(t)}$ 代表在序列索引号 $t$ 时模型的输出。$o^{(t)}$ 只由模型当前的隐藏状态 $h^{(t)}$ 决定。
4. $L^{(t)}$ 代表在序列索引号 $t$ 时模型的损失函数。
5. $y^{(t)}$ 代表在序列索引号 $t$ 时训练样本序列的真实输出。
6. $U,W,V$ 这三个矩阵是我们的模型的线性关系参数，它在整个 RNN 网络中是共享的，这点和 DNN 很不相同。也正因为是共享，体现了 RNN 的模型的“循环反馈”的思想。

## RNN 前向传播算法

对于任意一个序列索引号 $t$，隐藏状态 $h^{(t)}$ 由 $x^{(t)}$ 和 $h^{(t-1)}$ 得到：

$$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b )$$

其中 $\sigma$ 为 RNN 的激活函数，一般为 $tanh$，$b$ 为线性关系的偏置。

序列索引号 $t$ 时模型的输出 $o^{(t)}$ 的表达式比较简单：

$$o^{(t)} = Vh^{(t)} +c $$

在最终在序列索引号 $t$ 时的预测输出为：

$$\hat{y}^{(t)} = \sigma(o^{(t)})$$

通常由于 RNN 是识别类的分类模型，所以上面这个激活函数一般是 softmax。

通过损失函数 $L^{(t)}$，比如对数似然损失函数，我们可以量化模型在当前位置的损失，即 $\hat{y}^{(t)}$ 和 $y^{(t)}$ 的差距。

## RNN 反向传播算法

有了 RNN 前向传播算法的基础，很容易推导出 RNN 反向传播算法的流程了。RNN 反向传播算法的思路和 DNN 是一样的，即通过梯度下降法一轮轮的迭代，得到合适的 RNN 模型参数 $U,W,V,b,c$。

由于是基于时间反向传播，所以 RNN 的反向传播有时也叫做 BPTT(back-propagation through time)。当然 RNN 的 BPTT 和 DNN 有很大的不同点，即所有的 $U,W,V,b,c$ 在序列的各个位置是共享的，反向传播时更新的是相同的参数。

为了简化描述，损失函数为交叉熵损失函数，输出的激活函数为 softmax 函数，隐藏层的激活函数为 tanh 函数。

对于 RNN，由于在序列的每个位置都有损失函数，因此最终的损失 $L$ 为：

$$L = \sum\limits_{t=1}^{\tau}L^{(t)}$$

其中 $V,c$ 的梯度计算是比较简单的：

$$\frac{\partial L}{\partial c} = \sum\limits_{t=1}^{\tau}\frac{\partial L^{(t)}}{\partial c} = \sum\limits_{t=1}^{\tau}\hat{y}^{(t)} - y^{(t)}$$

$$\frac{\partial L}{\partial V} =\sum\limits_{t=1}^{\tau}\frac{\partial L^{(t)}}{\partial V} = \sum\limits_{t=1}^{\tau}(\hat{y}^{(t)} - y^{(t)}) (h^{(t)})^T$$

但是 $W,U,b$ 的梯度计算就比较的复杂了。从 RNN 的模型可以看出，在反向传播时，在某一序列位置 $t$ 的梯度损失由当前位置的输出对应的梯度损失和序列索引位置 $t+1$ 时的梯度损失两部分共同决定。对于 $W$ 在某一序列位置 $t$ 的梯度损失需要反向传播一步步的计算。定义序列索引 $t$ 位置的隐藏状态的梯度为：

$$\delta^{(t)} = \frac{\partial L}{\partial h^{(t)}}$$

可以像 DNN 一样从 $\delta^{(t+1)} $ 递推 $\delta^{(t)}$。

$$
\begin{aligned} \delta^{(t)} &= (\frac{\partial o^{(t)}}{\partial h^{(t)}} )^T\frac{\partial L}{\partial o^{(t)}} + (\frac{\partial h^{(t+1)}}{\partial h^{(t)}})^T\frac{\partial L}{\partial h^{(t+1)}} \\ & = V^T(\hat{y}^{(t)} - y^{(t)}) + W^Tdiag(1-(h^{(t+1)})^2)\delta^{(t+1)} 
\end{aligned}
$$

对于 $\delta^{(\tau)}$，由于它的后面没有其他的序列索引了，因此有：

$$\delta^{(\tau)} =( \frac{\partial o^{(\tau)}}{\partial h^{(\tau)}})^T\frac{\partial L}{\partial o^{(\tau)}} = V^T(\hat{y}^{(\tau)} - y^{(\tau)})$$

有了 $\delta^{(t)}$, 计算 $W,U,b$ 就容易了，这里给出 $W,U,b$ 的梯度计算表达式：

$$\frac{\partial L}{\partial W} = \sum\limits_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{(t)}(h^{(t-1)})^T$$

$$\frac{\partial L}{\partial b}= \sum\limits_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{(t)}$$

$$\frac{\partial L}{\partial U} =\sum\limits_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{(t)}(x^{(t)})^T$$

除了梯度表达式不同，RNN 的反向传播算法和 DNN 区别不大，因此这里就不再重复总结了。

RNN 虽然理论上可以很漂亮的解决序列数据的训练，但是它也像 DNN 一样有梯度消失时的问题，当序列很长的时候问题尤其严重。因此，上面的 RNN 模型一般不能直接应用。在语音识别，手写书别以及机器翻译等 NLP 领域实际应用比较广泛的是基于 RNN 模型的一个特例 LSTM。