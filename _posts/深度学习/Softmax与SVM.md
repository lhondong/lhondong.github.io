## Softmax 与多分类 SVM 的区别

针对一个数据点，SVM 和 Softmax 分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量 $f$（本节中是通过矩阵乘来实现）。不同之处在于对 $f$ 中分值的解释：SVM 分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别 2) 的分值比其他分类的分值高出至少ー个边界值。

Softmax 分类器将这些数值看做是每个分类没有归一化的**对数概率**，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM 的最终的损失值是 1.58，Softmax 的最终的损失值是 0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。

**Softmax 分类器为每个分类提供了“可能性”**：SVM 的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax 分类器则不同，它允许我们计算出对于所有分类标签的可能性。

举个例子，针对给出的图像，SVM 分类器可能给你的是一个 [12.5,0.6,-23.0]，对应分类“猫”,“狗”,“船”。而 Softmax 分类器可以计算出这三个标签的“可能性”是 [0.9,0.09,0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在“可能性”上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数入直接决定的，$\lambda$ 是你能直接控制的一个输入参数。举个例子，假设 3 个分类的原始分数是 [1,-2,0]，那么 Softmax 函数就会计算：

$[1,-2,0]\rightarrow e^{[1,-2,0]}=[2.71,0.14,1]\rightarrow[0.7,0.04,0.26]$

现在，如果正则化参数 $\lambda$ 更大，那么权重 $W$ 就会被惩罚的更多，然后权重数值就会更小。这样算出来的分数也会更小。假设小了一半：

$[0.5,-1,0]\rightarrow[1.65,0.73,1]\rightarrow[0.55,0.12,0.33]$

现在看起来，概率的分布就更加分散了。另外，随着正则化参数  $\lambda$ 不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，Softmax 分类器算出来的概率最好是看成一种对于分类正确性的自信。和 SVM 一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释。

在实际使用中，SVM 和 Softmax 经常是相似的：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于 Softmax 分类器，SVM 更加“局部目标化 (local objective)”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是 [10,-2,3] 的数据，其中第一个分类是正确的，那么ー个 SVM($\Delta=1$) 会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是 0。SVM 对于数字个体的细节是不关心的：如果分数是 [10,-100,-100] 或者 [10,9,9]，对于 SVM 来说没设么不同，只要满足超过边界值等于 1，那么损失值就等于 0。

对于 Softmax 分类器，情况则不同。对于 [10,9,9] 来说，计算出的损失值就远远高于 [10,-100,-100] 的。换句话来说， Softmax 分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM 只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是 SVM 的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。