# 决策树

决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。

决策树的生成主要分以下两步，这两步通常通过学习已经知道分类结果的样本来实现。

1. 节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成 2 个子节点（如不是二叉树的情况会分成 n 个子节点）
2. 阈值的确定：选择适当的阈值使得分类错误率最小 (Training Error)。

## ID3

由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。

熵度量了事物的不确定性，越不确定的事物，它的熵就越大。熵定义如下：

$H(X) = -\sum\limits_{i=1}^{n}p_i logp_i$

其中 n 代表 X 的 n 种不同的离散取值。而 $p_i$ 代表了 $X$ 取值为 $i$ 的概率，log 为以 2 或者 e 为底的对数。举个例子，比如$2$个可能的取值，而这两个取值各为 1/2 时 $X$ 的熵最大，此时 $X$ 具有最大的不确定性。$H(X)= -(\frac {1}{2}log\frac {1}{2} + \frac {1}{2}log\frac {1}{2}) = log2)$。如果一个值概率大于 1/2，另一个值概率小于 1/2，则不确定性减少，对应的熵也会减少。

所以当 Entropy 最大为 1 的时候，是分类效果最差的状态，当它最小为 0 的时候，是完全分类的状态。因为熵等于零是理想状态，一般实际情况下，熵介于 0 和 1 之间。

联合熵：

$$
H(X,Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i,y_i)
$$

有了联合熵，又可以得到条件熵的表达式 $H(X|Y)$，条件熵类似于条件概率，它度量了我们的 $X$ 在知道 $Y$ 以后剩下的不确定性。表达式如下：

$$
H(X|Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)
$$

$H(X)$ 度量了 $X$ 的不确定性，条件熵 $H(X|Y)$ 度量了我们在知道 $Y$ 以后 $X$ 剩下的不确定性，那么 $H(X)-H(X|Y)$ 度量了 $X$ 在知道 $Y$ 以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为 $I(X,Y)$。在决策树 ID3 算法中叫做信息增益。ID3 算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。

<div align=center><img src="/assets/XGBoost-2022-01-12-13-58-22.png" alt="XGBoost-2022-01-12-13-58-22" style="zoom:50%;" /></div>

左边的椭圆代表 $H(X)$，右边的椭圆代表 $H(Y)$，中间重合的部分就是我们的互信息或者信息增益 $I(X,Y)$，左边的椭圆去掉重合部分就是 $H(X|Y)$，右边的椭圆去掉重合部分就是 $H(Y|X)$。两个椭圆的并就是 $H(X,Y)$。

## C4.5

## CART 树 Classification And Regression Tree

重要：scikit-learn 使用了优化版的 CART 算法作为其决策树算法的实现

# XGBoost

作为 GBDT 的高效实现，XGBoost 是一个上限特别高的算法，因此在算法竞赛中比较受欢迎。简单来说，对比原算法 GBDT，XGBoost 主要从下面三个方面做了优化：

1. 一是算法本身的优化：在算法的弱学习器模型选择上，对比 GBDT 只支持决策树，还可以支持很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT 的损失函数只对误差部分做负梯度（一阶泰勒）展开，而 XGBoost 损失函数对误差部分做二阶泰勒展开，更加准确。算法本身的优化是我们后面讨论的重点。
2. 二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用 CPU 缓存进行读取加速。将各个分组保存到多个硬盘以提高 IO 速度。
3. 三是算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了 L1 和 L2 正则化项，可以防止过拟合，泛化能力更强。

在上面三方面的优化中，第一部分算法本身的优化是重点也是难点。现在我们就来看看算法本身的优化内容。

XGBoost 为什么使用 CART 树而不是用普通的决策树呢？简单讲，对于分类问题，由于 CART 树的叶子节点对应的值是一个实际的分数，而非一个确定的类别，这将有利于实现高效的优化算法。XGBoost 出名的原因一是准，二是快，之所以快，其中就有选用 CART 树的一份功劳。
