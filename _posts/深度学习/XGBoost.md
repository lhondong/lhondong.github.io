# 决策树

决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。

决策树的生成主要分以下两步，这两步通常通过学习已经知道分类结果的样本来实现。

1. 节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成 2 个子节点（如不是二叉树的情况会分成 n 个子节点）
2. 阈值的确定：选择适当的阈值使得分类错误率最小 (Training Error)。

## ID3

由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。

熵度量了事物的不确定性，越不确定的事物，它的熵就越大。熵定义如下：

$H(X) = -\sum\limits_{i=1}^{n}p_i logp_i$

其中 n 代表 X 的 n 种不同的离散取值。而 $p_i$ 代表了 $X$ 取值为 $i$ 的概率，log 为以 2 或者 e 为底的对数。举个例子，比如$2$个可能的取值，而这两个取值各为 1/2 时 $X$ 的熵最大，此时 $X$ 具有最大的不确定性。$H(X)= -(\frac {1}{2}log\frac {1}{2} + \frac {1}{2}log\frac {1}{2}) = log2)$。如果一个值概率大于 1/2，另一个值概率小于 1/2，则不确定性减少，对应的熵也会减少。

所以当 Entropy 最大为 1 的时候，是分类效果最差的状态，当它最小为 0 的时候，是完全分类的状态。因为熵等于零是理想状态，一般实际情况下，熵介于 0 和 1 之间。

联合熵：

$$
H(X,Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i,y_i)
$$

有了联合熵，又可以得到条件熵的表达式 $H(X|Y)$，条件熵类似于条件概率，它度量了我们的 $X$ 在知道 $Y$ 以后剩下的不确定性。表达式如下：

$$
H(X|Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)
$$

$H(X)$ 度量了 $X$ 的不确定性，条件熵 $H(X|Y)$ 度量了我们在知道 $Y$ 以后 $X$ 剩下的不确定性，那么 $H(X)-H(X|Y)$ 度量了 $X$ 在知道 $Y$ 以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为 $I(X,Y)$。在决策树 ID3 算法中叫做信息增益。ID3 算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。

<div align=center><img src="/assets/互信息-2022-01-12-13-58-22.png" alt="互信息-2022-01-12-13-58-22" style="zoom:50%;" /></div>

左边的椭圆代表 $H(X)$，右边的椭圆代表 $H(Y)$，中间重合的部分就是我们的互信息或者信息增益 $I(X,Y)$，左边的椭圆去掉重合部分就是 $H(X|Y)$，右边的椭圆去掉重合部分就是 $H(Y|X)$。两个椭圆的并就是 $H(X,Y)$。

## C4.5

## CART 树 Classification And Regression Tree

重要：scikit-learn 使用了优化版的 CART 算法作为其决策树算法的实现

# XGBoost

作为 GBDT 的高效实现，XGBoost 是一个上限特别高的算法，因此在算法竞赛中比较受欢迎。简单来说，对比原算法 GBDT，XGBoost 主要从下面三个方面做了优化：

1. 一是算法本身的优化：在算法的弱学习器模型选择上，对比 GBDT 只支持决策树，还可以支持很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT 的损失函数只对误差部分做负梯度（一阶泰勒）展开，而 XGBoost 损失函数对误差部分做二阶泰勒展开，更加准确。算法本身的优化是我们后面讨论的重点。
2. 二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用 CPU 缓存进行读取加速。将各个分组保存到多个硬盘以提高 IO 速度。
3. 三是算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了 L1 和 L2 正则化项，可以防止过拟合，泛化能力更强。

在上面三方面的优化中，第一部分算法本身的优化是重点也是难点。现在我们就来看看算法本身的优化内容。

XGBoost 为什么使用 CART 树而不是用普通的决策树呢？简单讲，对于分类问题，由于 CART 树的叶子节点对应的值是一个实际的分数，而非一个确定的类别，这将有利于实现高效的优化算法。XGBoost 出名的原因一是准，二是快，之所以快，其中就有选用 CART 树的一份功劳。

## Bagging 和 Boosting 模型集成

### Bagging

从原始样本集中抽取训练集。每轮从原始样本集中使用 Bootstraping 的方法抽取 n 个训练样本。共进行 k 轮抽取，得到 k 个训练集。每次使用一个训练集得到一个模型，k 个训练集共得到 k 个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

对分类问题：将上步得到的 k 个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

### Boosting

#### 1. 样本选择上：

- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

#### 2. 样例权重：

- Bagging：使用均匀取样，每个样例的权重相等
- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

#### 3. 预测函数：

- Bagging：所有预测函数的权重相等。
- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

#### 4.	并行计算：

- Bagging：各个预测函数可以并行生成
- Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

#### 5. 总结

这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

下面是将决策树与这些算法框架进行结合所得到的新的算法：


### GBDT

GBDT 使用的决策树是 CART 回归树，无论是处理回归问题还是二分类以及多分类，GBDT 使用的决策树通通都是都是 CART 回归树。为什么不用 CART 分类树呢？因为 GBDT 每次迭代要拟合的是梯度值，是连续值所以要用回归树。
 
对于 GBDT 分类模型一个是用指数损失函数。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。
