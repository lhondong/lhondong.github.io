---
title: "Finetune 方法汇总"
subtitle: "Pretrain-Finetune"
layout: post
author: "L Hondong"
header-img: "img/post-bg-43.jpg"
mathjax: ture
tags:
  - 深度学习
  - 预训练模型
---

# Finetune 方法汇总

迁移学习广泛地应用于 NLP、CV 等各种领域，通过在源域数据上学习知识，再迁移到下游其他目标任务上，提升目标任务上的效果。其中，Pretrain-Finetune（预训练+精调）模式是最为常见的一种迁移学习方法。例如 NLP 中的预训练 Bert 模型，通过在下游任务上 Finetune 即可取得比直接使用下游数据任务从零训练的效果要好得多。

虽然在预训练模型上 Finetune 能取得非常好的效果，但是如何 Finetune 对最终效果好坏有非常大的影响。例如，在 Finetune 时需要考虑迁移预训练网络的哪些层，哪些层需要冻结，哪些层需要随着模型一起 Finetune。实际应用时，也往往需要尝试不同的迁移方法和 Finetune 策略来达到最优效果。目前学术界也有很多创新性较强的花式 Finetune 方法研究。

**本文介绍了来自 2018 年以来 ICML、CVPR 等顶会的 7 篇论文，总结了 Finetune 的四种类型**，通过更科学的 Finetune 方式，提升迁移学习效果。

## 一、使用 Pretrain 模型做约束

在 Finetune 阶段，如果可用于 Finetune 的目标任务数据量较少时，很有可能出现过拟合现象，严重影响模型效果；或者在 Finetune 过程中出现知识遗忘问题（catastrophic memory），把 Pretrain 阶段学到的有用知识遗忘，丢了西瓜捡了芝麻。为了解决这种问题，提出利用 Pretrain 模型作为约束，指导 Finetune 的过程，让 Finetune 得到的模型更加鲁棒。

具体包括：

- 直接使用 Pretrain 模型的参数作为约束
- 使用 Pretrain 模型的中间层表示作为约束
- 使用 Pretrain 模型对不同特征注意力强度作为约束

为了防止模型在 Finetune 阶段过拟合目标任务数据，或忘记了 Pretrain 阶段学到的有意义知识，**[ICML 2018] Explicit inductive bias for transfer learning with convolutional networks **这篇文章介绍了一种使用 Pretrain 模型参数约束 Finetune 过程的方法：通过添加 Pretrain 模型参数和 Finetune 模型参数之间的某种正则化损失，让 Finetune 后的模型参数和最开始的 Pretrain 模型参数更加相似。文章中尝试了多种正则化方法，通过最终的实验发现，一个简单的 L2 正则效果最好，即对于 Pretrain 模型和 Finetune 模型的对应层的参数计算 L2 距离，作为 Finetune 过程中损失函数的一部分，公式如下（$w$ 为 Finetune 参数，$w^0$ 为 Pretrain 参数）：

$$
\Omega(w)=\frac{\alpha}{2}\Vert w-w^0\Vert_2^2
$$

通过 L2 正则化的方法拉近 Pretrain 模型和 Target 模型参数也存在一定问题，如何设定正则化的强度直接决定了迁移效果，正则化太弱仍然会导致过拟合和信息遗忘，迁移强度太强会导致 Finetune 的模型在 Target 任务上不是最优解。百度的文章 **[ICLR 2019] DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NET-WORKS** 提出，通过约束网络的 behavior，即 feature map，而非模型参数，来实现约束目标。具体的，约束项可以表示为如下形式：

$$
\Omega^{\prime}\left(\omega, \omega^{*}, \mathbf{x}_{i}, y_{i}, z\right)=\sum_{j=1}^{N}\left(\mathrm{~W}_{j}\left(z, \omega^{*}, \mathbf{x}_{i}, y_{i}\right) \cdot \Vert \text{FM}_{j}\left(z, \omega, \mathbf{x}_{i}\right)-\text{FM}_{j}\left(z, \omega^{*}, \mathbf{x}_{i}\right)\right) \Vert_{2}^{2}
$$

其中，$W_j$ 表示第 $j$ 个卷积层的约束强度，FM 表示第 $i$ 个样本经过参数 $w$ 提取的 feaure map。$W_j$ 的计算方法为，使用 Pretrain 的模型 Freeze 住底层 Feature Extractor 参数，Finetune 后面 Discriminator 参数，通过衡量去掉每个 channel 后效果的损失，得到这个 channel 的迁移强度。如果去掉 Pretrain 模型某个 channel 后效果下降特别明显，说明 Pretrain 得到的这个 channel 的信息对 Target 任务是很有效的，这个时候要增大这种 channel 参数的迁移强度。

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-06-59.png" alt="Finetune方法汇总-2022-03-29-14-06-59" style="zoom:50%;" /></div>

采用 Pretrain 模型对 Finetune 模型进行约束需要引入额外的正则化 Loss，可以被称为 Transfer Loss。由于 Transfer Loss 和 Target Task Loss 的优化目标不同，如何平衡两个 Loss 的关系决定了迁移效果。为了统一这两种 Loss，**[AAAI 2021] Robust Knowledge Transfer via Hybrid Forward on the Teacher-Student Model** 提出了一种混合前向网络。当一个样本输入时，会通过三种不同的路径得到三种 Loss，

- Loss1 和 Loss2 通过交替进入 Student 网络（Target Task 模型）某层和 Teachder 网络（Pretrain 模型）某层，最终再通过 Target Task Head 得到；
- Loss3 只进入 Student 网络通过 Target Task Head 得到。Loss1 和 Loss2 代表了 Student 网络和 Teachder 网络 Transfer Loss，Loss3 代表了 Target Task 的优化 Loss。

与之前方法相比，该方法的三个 Loss 都是以优化 Target Task 为目标的，因此可以通过直接对比来判断目前的 Transfer 强度是否太强。文中设计了一种简单的平衡 Transfer Loss 和 Target Loss 的方法，让两个任务的 Loss 同等重要，随着训练过程动态计算 Loss1 和 Loss2 的权重。

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-09-45.png" alt="Finetune方法汇总-2022-03-29-14-09-45" style="zoom:100%;" /></div>

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-10-01.png" alt="Finetune方法汇总-2022-03-29-14-10-01" style="zoom:100%;" /></div>

## 二、选择性地对 Pretrain 模型迁移

Pretrain 模型中的参数不一定都是对下游任务有帮助的，因此一些研究提出，对 Pretrain 的模型进行有选择性的迁移，重点迁移那些对下游任务帮助大的信息。**[ICML 2019] Learning What and Where to Transfer** 中提出一种基于 Meta-learning 的迁移学习方法。这篇文章的核心思路建立在 **[ICLR 2015] FITNETS: HINTS FOR THIN DEEP NETS** 一文提出的迁移方法之上，让 Target 模型通过一个回归任务拟合 Pretrain 模型中间层的表示，该优化目标可以表示为：

$$
\mathcal{L}_{H T}\left(\mathbf{W}_{\text {Guided}}, \mathbf{W}_{\mathbf{r}}\right)=\frac{1}{2}\left\Vert u_{h}\left(\mathbf{x} ; \mathbf{W}_{\text{Hint}}\right)-r\left(v_{g}\left(\mathbf{x} ; \mathbf{W}_{\text {Guided}}\right) ; \mathbf{W}_{\mathbf{r}}\right)\right\Vert^{2}
$$

在 FitNet 中 $W_{\text{Guided}}$ 和 $W_{\text{Hint}}$ 分别表示 Target 模型的 Pretrain 模型某一对应层的参数。

**[ICML 2019] Learning What and Where to Transfer** 对该方法进行了扩展。在 What to transfer 阶段，对每一个 channel 的迁移做一个权重学习，每个 channel 的权重是通过一个单独的网络输入图片在 Source 模型的输出计算得到的（T 代表 Target 模型，S 代表 Source 模型，与 FitNet 中的 Guided 和 Hint 相对应）：

$$
\mathcal{L}_{\mathrm{wfm}}^{m, n}\left(\theta \mid x, w^{m, n}\right) =\frac{1}{H W} \sum_{c} w_{c}^{m, n} \sum_{i, j}\left(r_{\theta}\left(T_{\theta}^{n}(x)\right)_{c, i, j}-S^{m}(x)_{c, i, j}\right)^{2}
$$

在 Where to transfer 阶段，主要决定 Source 模型到 Target 模型的迁移层 pair，即 Source 模型的第 $i$ 层参数要迁移到 Target 模型的哪一层。类似 What to transfer，通过一个单独的网络学习 $(i,j)$ 这组 Source 模型到 Target 模型 pair 对的迁移强度：

$$
\mathcal{L}_{\mathrm{wfm}}(\theta\vert x, \phi)=\sum_{(m, n) \in \mathcal{C}} \lambda^{m, n} \mathcal{L}_{\mathrm{wfm}}^{m, n}\left(\theta \vert x, w^{m, n}\right)
$$

最终的 Loss 由上面两个阶段的 Loss，以及任务本身的 Loss 共同组成。在训练阶段，文章采用了 Meta-learning 的方法，内循环阶段更新总体 Loss，外循环阶段更新三个 Loss 的总和。

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-10-30.png" alt="Finetune方法汇总-2022-03-29-14-10-30" style="zoom:100%;" /></div>

## 三、在 Finetune 阶段调整网络结构

之前介绍的迁移学习方法，大多数都是通过 Finetune 对 Pretrain 模型的参数进行调整。然而，下游的 Target 任务可能需要和 Source 任务采用不同的模型结构来更好的进行学习。因此，**[AAAI 2021] TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning** 提出了一种在 Finetune 阶段动态剪枝的方法，实现 Finetune 阶段不仅能够调整模型参数，还能调整模型网络结构。

该方法分为 Target-aware Pruning 和 Importance-aware Finetuning 两个阶段。

- 在 Target-aware Pruning 阶段，对于网络中每一层的每一个 filter，都对应一个可学习的权重，把 Pretrain 模型的参数 Freeze 住，使用 Target 任务的数据和优化目标进行训练，得到每组参数最终对应的权重，训练过程可以表示为：

$$
\alpha^{*}=\arg \min _{\alpha} \mathcal{L}\left(D_{t} ; W_{f}^{s} \odot \alpha\right)
$$

这个重要性权重会使用泰勒变换，融合全局各层的打分结果得到全局的打分，最后将打分较低的网络参数剪枝掉。

- 在 Importance-aware Finetuning，会结合第一阶段得到的参数打分进行 Finetune，通过将每组参数的打分结果乘到参数上，得到参数的转换结果进行前向传播。最终两个过程交替进行，直到得到最终模型。

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-10-55.png" alt="Finetune方法汇总-2022-03-29-14-10-55" style="zoom:100%;" /></div>

## 四、学习每组参数 Transfer 的方式

在利用 Pretrain 模型进行迁移学习时，往往需要决定哪些网络的参数要 Freeze，哪些网络参数跟随 Target 任务 Finetune。例如，在 CV 领域，一些研究表明底层网络能够提取出更一般的图像规律，而接近分类层的参数对于不同 Task 差异很大，因此为了不将 Pretrain 阶段学到的知识破坏，将底层参数 Freeze，只 Finetune 上层参数，会最大限度保留 Pretrain 阶段在大量数据学到的知识，提升迁移学习效果。然而，不同任务需要 Freeze 的参数存在差异，人工调试不同的 Transfer 方式（哪些层 Freeze、哪些层 Finetune）效率很低。同时，一般的 Finetune 假设每个 Target 样本都应该使用相同的 Finetune 方式，这也是不合理的。例如，和 source domain 更相似的样本，从 source domain 迁移更多知识更有帮助。因此，学术界出现一些相关工作，自动化学习每层以及每个样本的迁移策略。

在 **[CVPR 2019] SpotTune: Transfer Learning through Adaptive Fine-tuning** 这篇文章中，提出了对于每个样本学习一个个性化的迁移方式。对于每个样本，经过网络的每层可以选择是使用 Pretrain 模型的原始参数，还是使用 Pretrain 模型初始化后 Finetune 的参数。模型通过一个 Policy Network，输入每个样本的特征（一般使用 Pretrain 模型对样本的表征向量），输出模型每组参数的迁移方式（使用 Pretrain 模型原始参数，或使用 Pretrain 模型初始化后 Finetune 的参数）。这个过程中需要对两种迁移方式进行采样，而采样运算不可导。为了让该运算可导，本文使用了 Gumbel-Max Trick 生成采样结果。SpotTune 的模型结构如下图所示：

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-11-07.png" alt="Finetune方法汇总-2022-03-29-14-11-07" style="zoom:100%;" /></div>

SpotTune 实现了每个样本个性化的 Finetune 策略学习，但是只能做到 layer 维度。

**[AAAI 2020] AdaFilter: Adaptive Filter Fine-tuning for Deep Transfer Learning** 提出了能够在 filter 维度实现每个样本的 Finetune 策略学习。与 SpotTune 类似，Finetune 策略仍然为使用 Pretrain 模型的原始参数，或使用 Pretrain 模型初始化后 Finetune 的参数两种。与 SpotTune 不同的是，AdaFilter 使用 RNN 学习每层各个 channel 的 Finetune 策略，每层的 Finetune 策略选择依赖于上一层输出的表示（SpotTune 则是根据样本同时产出所有层的 Finetune 策略）。

<div align=center><img src="/assets/Finetune方法汇总-2022-03-29-14-11-18.png" alt="Finetune方法汇总-2022-03-29-14-11-18" style="zoom:100%;" /></div>

## 五、总结

本文介绍了 7 篇顶会论文中对 Finetune 阶段进行的改进，包括 Finetune 过程使用 Pretrain 模型做约束、选择性地对 Pretrain 模型进行迁移、在 Finetune 阶段调整网络结构以及学习每组参数 Transfer 的方式 4 种类型。Pretrain-Finetune 的核心问题在于，**如何考虑到 Target Task 的样本特性，将 Pretrain 的知识合理迁移到 Target Task 的模型上**。其中每个样本个性化地进行 Finetune 策略学习，可能是后续可以继续深入研究的方向。

## Reference

1. [ICLR 2019] DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NET-WORKS
2. [AAAI 2021] Robust Knowledge Transfer via Hybrid Forward on the Teacher-Student Model
3. [ICML 2019] Learning What and Where to Transfer 
4. [ICLR 2015] FITNETS: HINTS FOR THIN DEEP NETS
5. [AAAI 2021] TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning
6. [CVPR 2019] SpotTune: Transfer Learning through Adaptive Fine-tuning
7. [AAAI 2020] AdaFilter: Adaptive Filter Fine-tuning for Deep Transfer Learning