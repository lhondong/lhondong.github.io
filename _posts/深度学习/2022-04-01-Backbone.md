---
title: "什么是 Backbone head neck？"
subtitle: "Backbone、head、neck 等深度学习中的术语解释"
layout: post
author: "L Hondong"
header-img: "img/post-bg-25.jpg"
mathjax: true
tags:
  - 笔记
---

## Backbone

翻译为主干网络的意思，这个主干网络大多时候指的是提取特征的网络，其作用就是提取图片中的信息，共后面的网络使用。这些网络经常使用的是 Resnet、VGG 等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为 Backbone 的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的 Backbone 模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。

## head

head 是获取网络输出内容的网络，head 利用之前提取的特征，做出预测。

## neck

是放在 Backbone 和 head 之间的，是为了更好的利用 Backbone 提取的特征。

## Bottleneck

瓶颈的意思，通常指的是网络输入的数据维度和输出的维度不同，输出的维度比输入的小了许多，就像脖子一样，变细了。经常设置的参数 bottle_num=256，指的是网络输出的数据的维度是 256 ，可是输入进来的可能是 1024 维度的。

## GAP

在设计的网络中经常能够看到 gap 这个层，指的就是 Global Average Pool 全局平均池化，就是将某个通道的特征取平均值，经常使用 AdaptativeAvgpoold(1), 在 pytorch 中，这个代表自适应性全局平均池化，说人话就是将某个通道的特征取平均值。

```python
self.gap = nn.AdaptiveAvgPool2d(1)
```

## Embedding

深度学习方法都是使用线性和非线性变换对复杂的数据进行自动特征抽取，并将特征表示为“向量”（vector），这一过程一般也称为“嵌入”（embedding）

## pretext task 和 downstream task

用于预训练的任务被称为前置/代理任务 (pretext task)，用于微调的任务被称为下游任务 (downstream task)

## temperature parameters

在论文中经常能看到这个温度参数的身影，比如经常看到下面这样的式子：

$$
p(i\vert x_{t,i})=\frac{e^{(\mathcal K[i]^Tf(x_{t.i})/\beta)}}{\sum\limits^{N_t}_{j=1}e^{(\mathcal K[i]^Tf(x_{t.i})/\beta)}}
$$

里面的 $\beta$ 就是 temperature parameter，他在运算的时候可以起到平滑 softmax 输出结果的作用，举例子如下：

```python
import torch
x = torch.tensor([1.0,2.0,3.0])
y = torch.softmax(x,0)
print(y)
 
x1 = x / 2  # beta 为 2
y = torch.softmax(x1,0)
print(y)
 
x2 = x/0.5  # beta 为 0.5
y = torch.softmax(x2,0)
print(y)

输出结果如下：

tensor([0.0900, 0.2447, 0.6652])
tensor([0.1863, 0.3072, 0.5065])
tensor([0.0159, 0.1173, 0.8668])
```

当 $\beta>1$ 的时候，可以将输出结果变得平滑，当 $\beta<1$ 的时候，可以让输出结果变得差异更大一下，更尖锐一些。如果 $\beta$ 比较大，则分类的 cross entropy 损失会很大，可以在不同的迭代次数里，使用不同的 $\beta$ 数值，有点类似于学习率的效果。

## 热身 Warm up

Warm up 指的是用一个小的学习率先训练几个 epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。

## End to end

在论文中经常能遇到 End to end 这样的描述，其实就是给了一个输入，我们就给出一个输出，不管其中的过程多么复杂，但只要给了一个输入，机会对应一个输出。比如分类问题，你输入了一张图片，网络有特征提取，全链接分类，概率计算什么的，但是跳出算法问题，单从结果来看，就是给了一张输入，输出了一个预测结果。End-To-End 的方案，即输入一张图，输出最终想要的结果，算法细节和学习过程全部丢给了神经网络。

## domain adaptation 和 domain generalization 域适应和域泛化

域适应中，常见的设置是源域 $\mathcal D_S$ 完全已知，目标域 $\mathcal D_T$ 有或无标签。域适应方法试着将源域知识迁移到目标域。第二种场景可以视为 domain generalization 域泛化。这种更常见因为将模型应用到完全未知的领域，正因为没有见过，所以没有任何模型更新和微调。这种泛化问题就是一种开集问题，由于所需预测类别较多，所以比较头疼 。
