---
title: "变分自编码器"
subtitle: "VAE"
layout: post
author: "L Hondong"
header-img: "img/post-bg-49.jpg"
mathjax: true
tags:
  - 笔记
---

# 变分自编码器

## 一、VAE 的设计思路

Auto-encoder 中，引入神经网络强大的拟合能力，使得编码（Code）的维度能够比原始图像（X）的维度低非常多。在一个手写数字图像的生成模型中，Deep Auto-Encoder 能够把一个 784 维的向量（28*28 图像）压缩到只有 30 维，并且解码回的图像具备清楚的辨认度。

但是这并没有达到我们真正想要构造的生成模型的标准，因为，对于一个生成模型而言，解码器部分应该是单独能够提取出来的，并且对于在规定维度下任意采样的一个编码，都应该能通过解码器产生一张清晰且真实的图片。

首先来分析一下现有模型无法达到这一标准的原因。

<div align=center><img src="/assets/VAE-2022-04-05-19-46-55.png" alt="VAE-2022-04-05-19-46-55" style="zoom:50%;" /></div>

如上图所示，假设有两张训练图片，一张是全月图，一张是半月图，经过训练我们的自编码器模型已经能无损地还原这两张图片。接下来，我们在 code 空间上，两张图片的编码点中间处取一点，然后将这一点交给解码器，我们希望新的生成图片是一张清晰的图片（类似 3/4 全月的样子）。但是，实际的结果是，生成图片是模糊且无法辨认的乱码图。一个比较合理的解释是，因为编码和解码的过程使用了深度神经网络，这是一个非线性的变换过程，所以在 code 空间上点与点之间的迁移是非常没有规律的。

如何解决这个问题呢？我们可以引入噪声，使得图片的编码区域得到扩大，从而掩盖掉失真的空白编码点。

<div align=center><img src="/assets/VAE-2022-04-05-19-47-42.png" alt="VAE-2022-04-05-19-47-42" style="zoom:50%;" /></div>

如上图所示，现在在给两张图片编码的时候加上一点噪音，使得每张图片的编码点出现在绿色箭头所示范围内，于是在训练模型的时候，绿色箭头范围内的点都有可能被采样到，这样解码器在训练时会把绿色范围内的点都尽可能还原成和原图相似的图片。然后我们可以关注之前那个失真点，现在它处于全月图和半月图编码的交界上，于是解码器希望它既要尽量相似于全月图，又要尽量相似于半月图，于是它的还原结果就是两种图的折中（3/4 全月图）。

由此我们发现，给编码器增添一些噪音，可以有效覆盖失真区域。不过这还并不充分，因为在上图的距离训练区域很远的黄色点处，它依然不会被覆盖到，仍是个失真点。为了解决这个问题，我们可以试图把噪音无限拉长，使得对于每一个样本，它的编码会覆盖整个编码空间，不过我们得保证，在原编码附近编码的概率最高，离原编码点越远，编码概率越低。在这种情况下，图像的编码就由原先离散的编码点变成了一条连续的编码分布曲线，如下图所示。

<div align=center><img src="/assets/VAE-2022-04-05-19-48-43.png" alt="VAE-2022-04-05-19-48-43" style="zoom:100%;" /></div>

<div align=center><img src="/assets/VAE-2022-04-05-19-30-37.png" alt="VAE-2022-04-05-19-30-37" style="zoom:50%;" /></div>

在 Auto-encoder 中，编码器是直接产生一个编码的，但是在 VAE 中，为了给编码添加合适的噪声，编码器会输出两个编码，一个是原有编码 $(m_1,m_2,m_3)$，另外一个是控制噪声干扰程度的编码 $(\sigma_1,\sigma_2,\sigma_3)$，第二个编码其实很好理解，就是为随机噪声码 $(e_1,e_2,e_2)$ 分配权重，然后加上 $\exp(\sigma_i)$ 的目的是为了保证这个分配的权重是个正值，最后将原编码与噪声编码相加，就得到了 VAE 在 code 层的输出结果 $(c_1,c_2,c_2)$。其它网络架构都与 Deep Auto-encoder 无异。

损失函数方面，除了必要的重构损失外，VAE 还增添了一个损失函数（见上图第二个 Minimize 内容），这同样是必要的部分，因为如果不加的话，整个模型就会出现问题：为了保证生成图片的质量越高，编码器肯定希望噪声对自身生成图片的干扰越小，于是分配给噪声的权重越小，这样只需要将 $(\sigma_1,\sigma_2,\sigma_3)$ 赋为接近负无穷大的值就好了。所以，第二个损失函数就有限制编码器走这样极端路径的作用，这也从直观上就能看出来，$\exp(\sigma_i)-(1+\sigma_i)$ 在 $\sigma_i=0$ 处取得最小值，于是 $(\sigma_1,\sigma_2,\sigma_3)$ 就会避免被赋值为负无穷大。

上述我们只是粗略地理解了 VAE 的构造机理，但是还有一些更深的原理需要挖掘，例如第二个损失函数为何选用这样的表达式，以及 VAE 是否真的能实现我们的预期设想，即“图片能够编码成易于表示的形态，并且这一形态能够尽可能无损地解码回原真实图像”，是否有相应的理论依据。

下面从理论上深入地分析一下 VAE 的构造依据以及作用原理。

## 二、VAE 的作用原理

对于生成模型而言，主流的理论模型可以分为隐马尔可夫模型 HMM、朴素贝叶斯模型 NB 和高斯混合模型 GMM，而 VAE 的理论基础就是高斯混合模型。

什么是高斯混合模型呢？就是说，任何一个数据的分布，都可以看作是若干高斯分布的叠加。

<div align=center><img src="/assets/VAE-2022-04-05-19-37-43.png" alt="VAE-2022-04-05-19-37-43" style="zoom:100%;" /></div>

如图所示，如果 $P(x)$ 代表一种分布的话，存在一种拆分方法能让它表示成图中若干浅蓝色曲线对应的高斯分布的叠加。有意思的是，这种拆分方法已经证明出，当拆分的数量达到 512 时，其叠加的分布相对于原始分布而言，误差是非常非常小的了。

于是我们可以利用这一理论模型去考虑如何给数据进行编码。一种最直接的思路是，直接用每一组高斯分布的参数作为一个编码值实现编码。

<div align=center><img src="/assets/VAE-2022-04-05-19-39-05.png" alt="VAE-2022-04-05-19-39-05" style="zoom:50%;" /></div>

如上图所示，$m$ 代表着编码维度上的编号，譬如实现一个 512 维的编码，$m$ 的取值范围就是 $(1,2,3……512)$。$m$ 会服从于一个概率分布 $P(m)$（多项式分布）。现在编码的对应关系是，每采样一个 $m$，其对应到一个小的高斯分布 $\mathcal N(\mu^m,\Sigma^m)$，$P(x)$ 就可以等价为所有的这些高斯分布的叠加，即：

$$
P(x)=\sum\limits_m P(m)P(x\vert m)
$$

其中，$m\sim P(m),x\vert m\sim \mathcal N(\mu^m,\Sigma^m)$。

上述的这种编码方式是非常简单粗暴的，它对应的是我们之前提到的离散的、有大量失真区域的编码方式。于是我们需要对目前的编码方式进行改进，使得它成为连续有效的编码。

<div align=center><img src="/assets/VAE-2022-04-05-19-52-51.png" alt="VAE-2022-04-05-19-52-51" style="zoom:50%;" /></div>

现在我们的编码换成一个连续变量 $z$ ，我们规定 $z$ 服从正态分布 $\mathcal N(0,1)$（实际上并不一定要选用 $\mathcal N(0,1)$，其他的连续分布都是可行的）。每对于一个采样 $z$，会有两个函数 $\mu$ 和 $\sigma$，分别决定 $z$ 对应到的高斯分布的均值和方差，然后在积分域上所有的高斯分布的累加就成为了原始分布 $P(X)$，即：

$$
P(x)=\int\limits_z P(z)P(x\vert z) dz
$$

其中，$z \sim \mathcal N(0,1),x\vert z \sim \mathcal N\big(\mu(z),\sigma(z)\big)$。

接下来就可以求解这个式子。由于 $P(z)$ 是已知的，$P(x\vert z)$ 未知，而 $x\vert z \sim \mathcal N\big(\mu(z),\sigma(z)\big)$，于是我们真正需要求解的，是 $\mu$ 和 $\sigma$ 两个函数的表达式。又因为 $P(x)$ 通常非常复杂，导致和难以计算，我们需要引入两个神经网络来帮助我们求解。

第一个神经网络叫做 Decoder，它求解的是 $\mu(z)$ 和 $\sigma(z)$ 两个函数，这等价于求解 $P(x\vert z)$。

<div align=center><img src="/assets/VAE-2022-04-05-20-59-05.png" alt="VAE-2022-04-05-20-59-05" style="zoom:50%;" /></div>

第二个神经网络叫做 Encoder，它求解的结果是 $q(z\vert x)$，$q$ 可以代表任何分布。

<div align=center><img src="/assets/VAE-2022-04-05-20-59-18.png" alt="VAE-2022-04-05-20-59-18" style="zoom:50%;" /></div>

值得注意的是，这儿引入第二个神经网路 Encoder 的目的是，辅助第一个 Decoder 求解，这也是整个 VAE 理论中最精妙的部分，下面我会详细地解释其中的奥妙。

我们先回到最开始要求解的目标式：

$$
P(x)=\int\limits_z P(z)P(x\vert z) dz
$$

我们希望 $P(x)$ 越大越好，这等价于求解：

$$
\max L = \sum_x \log P(x)
$$

注意到：

$$
\begin{aligned}
\log P(x)&=\int_{z} q(z \vert x) \log P(x) dz \quad(\mathrm{q}(z \vert x) \text { 可以是任何分布） } \\
&=\int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{P(z \vert x)}\right) dz \\
&=\int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{q(z \vert x)} \frac{q(z \vert x)}{P(z \vert x)}\right) dz \\
&=\int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{q(z \vert x)}\right) dz+\int_{z} q(z \vert x) \log \left(\frac{q(z \vert x)}{P(z \vert x)}\right) dz \\
&=\int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{q(z \vert x)}\right) dz+ \mathbb{KL}\big(q(z \vert x) \Vert P(z \vert x)\big) 
\end{aligned}
$$

上式的第二项是一个大于等于 0 的值，于是我们就找到了一个 $\log P(x)$ 的下界：

$$
\log P(x) \geq \int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{q(z \vert x)}\right) dz
$$

我们把这个下界记作 

$$
L_b =\int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{q(z \vert x)}\right) dz
$$

于是原式化为：

$$
\log P(x) = L_b + \mathbb{KL}\big(q(z \vert x) \Vert P(z \vert x)\big) 
$$

接下来，VAE 思维的巧妙设计就体现出来了。原本，我们需要求 $P(x\vert z)$ 使得最大，现在引入了一个 $q(z\vert x)$，变成了同时求 $P(x\vert z)$ 和 $q(z\vert x)$ 使得 $\log P(x)$ 最大。

不妨观察一下 $\log P(x)$ 和 $L_b$ 的关系：

<div align=center><img src="/assets/VAE-2022-04-05-21-54-29.png" alt="VAE-2022-04-05-21-54-29" style="zoom:50%;" /></div>

一个有趣的现象是，当我们固定住 $P(x\vert z)$ 时，因为 $\log P(x)$ 只与 $P(x\vert z)$ 有关，所以 $\log P(x)$ 的值是会不变的，此时我们去调节 $q(z\vert x)$，使得 $L_b$ 越来越高，同时 KL 散度越来越小，当我们调节到 $q(z\vert x)$ 与 $P(z\vert x)$ 完全一致时，KL 散度就消失为 0，$L_b$ 与 $\log P(x)$ 完全一致。由此可以得出，不论 $\log P(x)$ 的值如何，我们总能够通过调节使得 $L_b$ 等于 $\log P(x)$，又因为 $L_b$ 是 $\log P(x)$ 的下界，所以求解 $\max \log P(x)$ 等价为求解 $\max L_b$。

这个现象从宏观上来看也是很有意思，调节 $P(x\vert z)$ 就是在调节 Decoder，调节 $q(z\vert x)$ 就是在调节 Encoder。于是，VAE 的训练逻辑就变成了，Decoder 每前进一步，Encoder 就调节成与其一致的样子，并且站在那拿“枪”顶住 Decoder，这样 Decoder 在下次训练的时候就只能前进，不能退步了。

上述便是 VAE 的巧妙设计之处。再回到我们之前的步骤上，现在需求解 $\max L_b$。

注意到：

$$
\begin{aligned}
L_b&=\int_{z} q(z \vert x) \log \left(\frac{P(z, x)}{q(z \vert x)}\right) dz \\
&=\int_{z} q(z \vert x) \log \left(\frac{P(x\vert z)P(z)}{q(z \vert x)}\right) dz \\
&=\int_{z} q(z \vert x) \log P(x\vert z) dz + \int_{z} q(z \vert x) \log \left(\frac{P(z)}{q(z \vert x)}\right) dz \\
&=\int_{z} q(z \vert x) \log P(x\vert z) dz - \mathbb{KL}\big(q(z \vert x) \Vert P(z)\big) 
\end{aligned}
$$

所以，求解 $\max L_b$，等价于求解 $\mathbb{KL}\big(q(z \vert x) \Vert P(z)\big)$ 的最小值和 $\int_{z} q(z \vert x) \log P(x\vert z) dz$ 的最大值。

我们先来求第一项，其实 $-\mathbb{KL}\big(q(z \vert x) \Vert P(z)\big)$ 的展开式刚好等于：

$$
\sum_{i=1}^{J}\left(\exp \left(\sigma_{i}\right)-\left(1+\sigma_{i}\right)+\left(m_{i}\right)^{2}\right)
$$

于是，第一项式子就是第二节 VAE 模型架构中第二个损失函数的由来。

接下来求第二项，注意到：

$$
\max \int_{z} q(z \vert x) \log P(x \vert z) dz=\max E_{q(z \vert x)}[\log P(x \vert z)]
$$

上述的这个期望，也就是表明在给定 $q(z \vert x)$（编码器输出）的情况下 $P(x \vert z)$（解码器输出）的值尽可能高，这其实就是一个类似于 Auto-Encoder 的损失函数（方差忽略不计的话）。

<div align=center><img src="/assets/VAE-2022-04-05-22-35-33.png" alt="VAE-2022-04-05-22-35-33" style="zoom:50%;" /></div>

因此，第二项式子就是第二节 VAE 模型架构中第一个损失函数的由来。

## 三、VAE 的直观理解

自编码器的缺点在于，不同类别的样本映射到特征空间后是不连续的。

<div align=center><img src="/assets/VAE-2022-04-05-23-25-22.png" alt="VAE-2022-04-05-23-25-22" style="zoom:50%;" /></div>

例如上图是在 MNIST 数据集上训练得到，可以看到不同类别的数字图像在二维空间中被明显地分开。它的好处仅表现在去区分和训练数据相似的数据。

但生成模型的目的是从潜在空间（latent space）中随机采样，或是在输入图像的基础上生成一些变体（要求潜在空间的连续性），从上图明显可以看出，潜在空间中还有大量没有被覆盖到的区域——如果在这些区域上采样，解码器该如何判断？

我们希望不同的类别彼此尽可能靠近同时又能区分开。 这就是 VAE 做的事情。

### 3.1 概率解释

<div align=center><img src="/assets/VAE-2022-04-05-23-26-03.png" alt="VAE-2022-04-05-23-26-03" style="zoom:50%;" /></div>

Encoder network 的输出的确是均值和方差，多维决定了这是多个随机变量，这才有了后面的多维高斯分布。 比如下图中均值和方差分别是 30 维，即 30 个随机变量，每组均值和方差都决定了一个高斯分布。从这些分布中采样即可得到 $z$。

<div align=center><img src="/assets/VAE-2022-04-05-23-27-37.png" alt="VAE-2022-04-05-23-27-37" style="zoom:50%;" /></div>

从分布中采样的好处是，即使对于同一个输入 $x$，Encoder 输出的 $z$ 也可能不同。因为这是按概率在均值 $\mu$ 和方差 $\sigma$ 确定的一片区域内采样得到的。其意义是使 Decoder 将潜在空间的这片区域对应到同一个类别，而不再是只将潜在空间的一个点对应某一类别。

<div align=center><img src="/assets/VAE-2022-04-05-23-30-05.png" alt="VAE-2022-04-05-23-30-05" style="zoom:50%;" /></div>

### 3.2 损失函数

回到我们的目标——使不同类别的输入在潜在空间中既能**彼此靠近**又能**互相区分**。

<div align=center><img src="/assets/VAE-2022-04-05-23-30-40.png" alt="VAE-2022-04-05-23-30-40" style="zoom:50%;" /></div>

靠近：在没有约束的情况下，为了便于分类，Encoder 输出的 $\mu$ 和 $\sigma$ 会很分散。其后果就是不同随机变量的分布在潜在空间中“各自为王”。VAE 的做法是让这些随机变量的分布都向标准正态分布靠近，偏离则会受到惩罚，其本质就是正则化。度量分布之间的距离选用的是 KL 散度。

但仅仅考虑靠近在一起是下图这样的效果：

<div align=center><img src="/assets/VAE-2022-04-05-23-31-33.png" alt="VAE-2022-04-05-23-31-33" style="zoom:50%;" /></div>

区分：靠 Decoder 来完成。

<div align=center><img src="/assets/VAE-2022-04-05-23-32-06.png" alt="VAE-2022-04-05-23-32-06" style="zoom:50%;" /></div>

最终的损失函数为：

$$
\min L_i(\theta,\phi) = - \mathbb E_{q_\theta(z \vert x)}[\log p_\phi(x_i \vert z)] + \mathbb{KL}\big(q_\theta(z \vert x_i) \Vert p(z)\big) 
$$

第一项就是 Decoder 网络的优化目标，是为了将不同类别在局部上区分；第二项是正则项，为了在全局上保持连续性。

### 3.3 生成过程

生成数据时只用到了 Decoder 网络，即 $z$ 不再是从 $q_\phi(z\vert x)$ 中采样，而是直接从标准正态分布中采样。因为 $q_\phi(z\vert x)$ 只是为了训练 Decoder 而引入的，目的是把样本数据集在潜在空间中聚拢，聚拢的目标是标准正态分布。经过训练后，Decoder 已经可以从这样的空间中区分不同类别的数据。生成时直接从标准正态分布采样 $z$ 即可。

## 四、VAE 的问题

VAE 最大的问题在于它从来没有真正的学习认识 image，因此，产生的照片中如果有差异一个 pixel，不同的位置对人来说也许意义不同，但对它而言都是一样的，因为它不懂。

<div align=center><img src="/assets/VAE-2022-04-05-22-39-13.png" alt="VAE-2022-04-05-22-39-13" style="zoom:50%;" /></div>

上图 7 为例，左右两图与 7 皆差异一个 pixel，但是左图的 pixel 是接续在后面，而右图的 pixel 是在本体之外的一个点，但都是差异一个 pixel，对 VAE 而言是相同的。

也因为这个缺点，延伸出后面的 GAN。

## 五、VAE 的深入理解

### 5.1 分布变换

通常我们会拿 VAE 跟 GAN 比较，的确，它们两个的目标基本是一致的——希望构建一个从隐变量 $Z$ 生成目标数据 $X$ 的模型，但是实现上有所不同。

更准确地讲，它们是假设了服从某些常见的分布（比如正态分布或均匀分布），然后希望训练一个模型 $X=g(Z)$，这个模型能够将原来的概率分布映射到训练集的概率分布，也就是说，它们的目的都是进行分布之间的变换。

<div align=center><img src="/assets/VAE-2022-04-02-22-07-41.png" alt="VAE-2022-04-02-22-07-41" style="zoom:50%;" /></div>

生成模型的难题就是判断生成分布与真实分布的相似度，因为我们只知道两者的采样结果，不知道它们的分布表达式。

那现在假设服从标准的正态分布，那么我就可以从中采样得到若干个 $(Z1,Z2,…,Zn)$，然后对它做变换得到 $\hat{X}_1=g(Z_1),\hat{X}_2=g(Z_2),…,\hat{X}_n=g(Z_n)$，我们怎么判断这个通过 $f$ 构造出来的数据集，它的分布跟我们目标的数据集分布是不是一样的呢？

KL 散度不可以衡量这个区别，因为 KL 散度是根据两个概率分布的表达式来算它们的相似度的，然而目前我们并不知道它们的概率分布的表达式。

我们只有一批从构造的分布采样而来的数据 $(\hat{X}_1,\hat{X}_2,…,\hat{X}_n)$，还有一批从真实的分布采样而来的数据 $(X_1,X_2,…,X_n)$（也就是我们希望生成的训练集）。只有样本本身，没有分布表达式，当然也就没有方法算 KL 散度。

AN 的思路很直接粗犷：既然没有合适的度量，那我干脆把这个度量也用神经网络训练出来吧。

就这样，WGAN 就诞生了，详细过程请参考 [互怼的艺术：从零直达 WGAN-GP](https://spaces.ac.cn/archives/4439)。而 VAE 则使用了一个精致迂回的技巧。

首先我们有一批数据样本 $(X_1,…,X_n)$，其整体用 $X$ 来描述，我们本想根据 $(X_1,…,X_n)$ 得到 $X$ 的分布 $p(X)$，如果能得到的话，那直接根据 $p(X)$ 来采样，就可以得到所有可能的 $X$ 了（包括 $(X_1,…,X_n)$ 以外的），这是一个终极理想的生成模型了。

当然，这个理想很难实现，于是我们将分布改一改：

$$
p(X)=\sum\limits_Z p(X\vert Z)p(Z)\tag{1}
$$

这里我们就不区分求和还是求积分了，意思对了就行。此时 $p(X\vert Z)$ 就描述了一个由 $Z$ 来生成 $X$ 的模型，而我们假设 $Z$ 服从标准正态分布，也就是 $p(Z)=\mathcal N(0,I)$。如果这个理想能实现，那么我们就可以先从标准正态分布中采样一个 $Z$，然后根据 $Z$ 来算一个 $X$，也是一个很棒的生成模型。

接下来就是结合自编码器来实现重构，保证有效信息没有丢失，再加上一系列的推导，最后把模型实现。框架的示意图如下：

<div align=center><img src="/assets/VAE-2022-04-02-23-40-23.png" alt="VAE-2022-04-02-23-40-23" style="zoom:50%;" /></div>

对于该图，我们其实完全不清楚：究竟经过重新采样出来的 $Z_k$，是不是还对应着原来的 $X_k$，所以我们如果直接最小化 $\mathcal D(\hat{X}_k,X_k)^2$（这里 $\mathcal D$ 代表某种距离函数）是很不科学的，而事实上你看代码也会发现根本不是这样实现的。

其实，在整个 VAE 模型中，我们并没有去使用 $p(Z)$（隐变量空间的分布）是正态分布的假设，**我们用的是假设 $p(Z\vert X)$（后验分布）是正态分布**！

具体来说，给定一个真实样本 $X_k$，我们假设存在一个专属于 $X_k$ 的分布 $p(Z\vert X_k)$（后验分布），并进一步假设这个分布是（独立的、多元的）正态分布。为什么要强调“专属”呢？因为我们后面要训练一个生成器 $X=g(Z)$，希望能够把从分布 $p(Z\vert X_k)$ 采样出来的一个 $Z_k$ 还原为 $X_k$。如果假设 $p(Z)$ 是正态分布，然后从 $p(Z)$ 中采样一个 $Z$，那么我们怎么知道这个 $Z$ 对应于哪个真实的 $X$ 呢？现在 $p(Z\vert X_k)$ 专属于 $X_k$，我们有理由说从这个分布采样出来的 $Z$ 应该要还原到 $X_k$ 中去。事实上，在论文《Auto-Encoding Variational Bayes》的应用部分，也特别强调了这一点：

> In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:
> 
> $$\log q_{\phi}(\boldsymbol{z}\vert \boldsymbol{x}^{(i)}) = \log \mathcal{N}(\boldsymbol{z} ;\boldsymbol{\mu}^{(i)},\boldsymbol{\sigma}^{2(i)}\boldsymbol{I})\tag{9}$$

论文中的式 $(9)$ 是实现整个模型的关键，不知道为什么很多教程在介绍 VAE 时都没有把它凸显出来。尽管论文也提到 $p(Z)$ 是标准正态分布，然而那其实并不重要。

回到本文，这时候每一个 $X_k$ 都配上了一个专属的正态分布，才方便后面的生成器做还原。但这样有多少个 $X$ 就有多少个正态分布了。我们知道正态分布有两组参数：均值 $\mu$ 和方差 $\sigma^2$（多元的话，它们都是向量），那我怎么找出专属于 $X_k$ 的正态分布 $p(Z\vert X_k)$ 的均值和方差呢？好像并没有什么直接的思路，那就用神经网络来拟合出来！这就是神经网络时代的哲学：难算的我们都用神经网络来拟合，在 WGAN 那里我们已经体验过一次了，现在再次体验到了。

于是我们构建两个神经网络 $\mu_k = f_1(X_k),\log \sigma_k^2 = f_2(X_k)$ 来算它们了。我们选择拟合 $\log \sigma_k^2$ 而不是直接拟合 $\sigma_k^2$，是因为 $\sigma_k^2$ 总是非负的，需要加激活函数处理，而拟合 $\log \sigma_k^2$ 不需要加激活函数，因为它可正可负。到这里，我能知道专属于 $X_k$ 的均值和方差了，也就知道它的正态分布长什么样了，然后从这个专属分布中采样一个 $Z_k$ 出来，然后经过一个生成器得到 $\hat{X}_k=g(Z_k)$，现在我们可以放心地最小化 $\mathcal{D}(\hat{X}_k,X_k)^2$，因为 $Z_k$ 是从专属 $X_k$ 的分布中采样出来的，这个生成器应该要把开始的 $X_k$ 还原回来。于是可以画出 VAE 的示意图：

<div align=center><img src="/assets/VAE-2022-04-03-21-21-08.png" alt="VAE-2022-04-03-21-21-08" style="zoom:50%;" /></div>

事实上，VAE 是为每个样本构造专属的正态分布，然后采样来重构。

### 5.2 分布标准化

让我们来思考一下，根据上图的训练过程，最终会得到什么结果。

首先，我们希望重构 $X$，也就是最小化 $\mathcal{D}(\hat{X}_k,X_k)^2$，但是这个重构过程受到噪声的影响，因为 $Z_k$ 是通过重新采样过的，不是直接由 encoder 算出来的。显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）是通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为 0。而方差为 0 的话，也就没有随机性了，所以不管怎么采样其实都只是得到确定的结果（也就是均值），只拟合一个当然比拟合多个要容易，而均值是通过另外一个神经网络算出来的。

说白了，模型会慢慢退化成普通的 AutoEncoder，噪声不再起作用。这样不就白费力气了吗？说好的生成模型呢

其实** VAE 还让所有的 $p(Z\vert X)$ 都向标准正态分布看齐**，这样就防止了噪声为零，同时保证了模型具有生成能力。怎么理解“保证了生成能力”呢？如果所有的 $p(Z\vert X)$ 都很接近标准正态分布 $\mathcal{N}(0,I)$，那么根据定义

$$
p(Z)=\sum_X p(Z\vert X)p(X)=\sum_X \mathcal{N}(0,I)p(X)=\mathcal{N}(0,I) \sum_X p(X) = \mathcal{N}(0,I)\tag{2}
$$

这样我们就能达到我们的先验假设：$p(Z)$ 是标准正态分布。然后我们就可以放心地从 $\mathcal{N}(0,I)$ 中采样来生成图像了。

<div align=center><img src="/assets/VAE-2022-04-03-21-28-22.png" alt="VAE-2022-04-03-21-28-22" style="zoom:50%;" /></div>

为了使模型具有生成能力，VAE 要求每个 $p(Z\vert X)$ 都向正态分布看齐。

那怎么让所有的 $p(Z\vert X)$ 都向 $\mathcal{N}(0,I)$ 看齐呢？如果没有外部知识的话，其实最直接的方法应该是在重构误差的基础上中加入额外的 loss：

$$
\mathcal{L}_{\mu}=\Vert f_1(X_k)\Vert^2\quad \text{and}\quad \mathcal{L}_{\sigma^2}=\Vert f_2(X_k)\Vert^2\tag{3}
$$

因为它们分别代表了均值 $\mu_k$ 和方差的对数 $\log\sigma_k^2$，达到 $\mathcal{N}(0,I)$ 就是希望二者尽量接近于 0 了。不过，这又会面临着这两个损失的比例要怎么选取的问题，选取得不好，生成的图像会比较模糊。所以，原论文直接算了一般（各分量独立的）正态分布与标准正态分布的 KL 散度 $KL\big(N(\mu,\sigma^2)\big\Vert N(0,I)\big)$ 作为这个额外的 loss，计算结果为：

$$
\mathcal{L}_{\mu,\sigma^2}=\frac{1}{2} \sum_{i=1}^d \big(\mu_{(i)}^2 + \sigma_{(i)}^2 - \log \sigma_{(i)}^2 - 1\big)\tag{4}
$$

这里的 $d$ 是隐变量 $Z$ 的维度，而 $\mu_{(i)}$ 和 $\sigma_{(i)}^2$ 分别代表一般正态分布的均值向量和方差向量的第 $i$ 个分量。直接用这个式子做补充 loss，就不用考虑均值损失和方差损失的相对比例问题了。显然，这个 loss 也可以分两部分理解：

$$
\begin{aligned}
&\mathcal{L}_{\mu,\sigma^2}=\mathcal{L}_{\mu} + \mathcal{L}_{\sigma^2}\\
&\mathcal{L}_{\mu}=\frac{1}{2} \sum_{i=1}^d \mu_{(i)}^2=\frac{1}{2}\Vert f_1(X)\Vert^2\\
&\mathcal{L}_{\sigma^2}=\frac{1}{2} \sum_{i=1}^d\big(\sigma_{(i)}^2 - \log \sigma_{(i)}^2 - 1\big)
\end{aligned}\tag{5}
$$

由于我们考虑的是各分量独立的多元正态分布，因此只需要推导一元正态分布的情形即可，根据定义我们可以写出：

$$
\begin{aligned}
&KL\big(N(\mu,\sigma^2)\big\Vert N(0,1)\big)\\
=&\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \left(\log \frac{e^{-(x-\mu)^2/2\sigma^2}/\sqrt{2\pi\sigma^2}}{e^{-x^2/2}/\sqrt{2\pi}}\right)dx\\
=&\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \log \left\{\frac{1}{\sqrt{\sigma^2}}\exp\left\{\frac{1}{2}\big[x^2-(x-\mu)^2/\sigma^2\big]\right\} \right\}dx\\
=&\frac{1}{2}\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \big[-\log \sigma^2+x^2-(x-\mu)^2/\sigma^2 \big] dx
\end{aligned}
$$

整个结果分为三项积分，第一项实际上就是 $-\log \sigma^2$ 乘以概率密度的积分（也就是 1），所以结果是 $-\log \sigma^2$；第二项实际是正态分布的二阶矩，熟悉正态分布的同学应该都清楚正态分布的二阶矩为 $\mu^2+\sigma^2$；而根据定义，第三项实际上就是“-方差除以方差=-1”。所以总结果就是

$$
KL\big(N(\mu,\sigma^2)\big\Vert N(0,1)\big)=\frac{1}{2}\big(-\log \sigma^2+\mu^2+\sigma^2-1\big)
$$

### 5.3 重参数技巧

<div align=center><img src="/assets/VAE-2022-04-03-21-39-59.png" alt="VAE-2022-04-03-21-39-59" style="zoom:50%;" /></div>

最后是实现模型的一个技巧，英文名是 reparameterization trick，我这里叫它做重参数吧。其实很简单，就是我们要从 $p(Z\vert X_k)$ 中采样一个 $Z_k$ 出来，尽管我们知道了$p(Z\vert X_k)$是正态分布，但是均值方差都是靠模型算出来的，我们要靠这个过程反过来优化均值方差的模型，但是“采样”这个操作是不可导的，而采样的结果是可导的。我们利用：

$$
\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)dz = \frac{1}{\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^2\right]d\left(\frac{z-\mu}{\sigma}\right)\tag{6}
$$

这说明 $(z-\mu)/\sigma=\varepsilon$ 是服从均值为 0、方差为 1 的标准正态分布的，要同时把 $dz$ 考虑进去，是因为乘上 $dz$ 才算是概率，去掉 $dz$ 是概率密度而不是概率。

这时候我们得到：从 $\mathcal{N}(\mu,\sigma^2)$ 中采样一个 $Z$，相当于从 $\mathcal{N}(0,I)$ 中采样一个 $\varepsilon$，然后让 $Z=\mu + \varepsilon \sigma$。

于是，我们将从 $\mathcal{N}(\mu,\sigma^2)$ 采样变成了从 $\mathcal{N}(0,I)$ 中采样，然后通过参数变换得到从 $\mathcal{N}(\mu,\sigma^2)$ 中采样的结果。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型可训练了。具体怎么实现，大家把上述文字对照着代码看一下，一下子就明白了。

## 六、VAE 的本质

VAE 虽然也称是 AE（AutoEncoder）的一种，但它的做法（或者说它对网络的诠释）是别具一格的。在 VAE 中，它的 Encoder 有两个，一个用来计算均值，一个用来计算方差，这已经让人意外了：Encoder 不是用来 Encode 的，是用来算均值和方差的，这真是大新闻了，还有均值和方差不都是统计量吗，怎么是用神经网络来算的？

事实上，我觉得 VAE 从让普通人望而生畏的变分和贝叶斯理论出发，最后落地到一个具体的模型中，虽然走了比较长的一段路，但最终的模型其实是很接地气的：**它本质上就是在我们常规的自编码器的基础上，对 encoder 的结果（在 VAE 中对应着计算均值的网络）加上了“高斯噪声”，使得结果 decoder 能够对噪声有鲁棒性；而那个额外的 KL loss（目的是让均值为 0，方差为 1），事实上就是相当于对 encoder 的一个正则项，希望 encoder 出来的东西均有零均值**。

那另外一个 encoder（对应着计算方差的网络）的作用呢？它是用来**动态调节噪声的强度**的。直觉上来想，当 decoder 还没有训练好时（重构误差远大于 KL loss），就会适当降低噪声（KL loss 增加），使得拟合起来容易一些（重构误差开始下降）；反之，如果 decoder 训练得还不错时（重构误差小于 KL loss），这时候噪声就会增加（KL loss 减少），使得拟合更加困难了（重构误差又开始增加），这时候 decoder 就要想办法提高它的生成能力了。

<div align=center><img src="/assets/VAE-2022-04-03-21-45-12.png" alt="VAE-2022-04-03-21-45-12" style="zoom:70%;" /></div>

说白了，重构的过程是希望没噪声的，而 KL loss 则希望有高斯噪声的，两者是对立的。所以，VAE 跟 GAN 一样，内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的。

从这个角度看，VAE 的思想似乎还高明一些，因为在 GAN 中，造假者在进化时，鉴别者是安然不动的，反之亦然。当然，这只是一个侧面，不能说明 VAE 就比 GAN 好。GAN 真正高明的地方是：它连度量都直接训练出来了，而且这个度量往往比我们人工想的要好（然而 GAN 本身也有各种问题，这就不展开了）。

从这个讨论中，我们也可以看出，当然，每个 $p(Z\vert X)$ 是不可能完全精确等于标准正态分布，否则 $p(Z\vert X)$ 就相当于跟 $X$ 无关了，重构效果将会极差。最终的结果就会是，$p(Z\vert X)$ 保留了一定的 $X$ 信息，重构效果也还可以，并且 $(2)$ 近似成立，所以同时保留着生成能力。

### 6.1 正态分布

对于 $p(Z\vert X)$ 的分布，我们可能会有疑惑：是不是必须选择正态分布？可以选择均匀分布吗？估计不大可行，这还是因为 KL 散度的计算公式：

$$
KL\big(p(x)\big\Vert q(x)\big) = \int p(x) \ln \frac{p(x)}{q(x)}dx\tag{7}
$$

要是在某个区域中 $p(x)\neq 0$ 而 $q(x)=0$ 的话，那么 KL 散度就无穷大了。对于正态分布来说，所有点的概率密度都是非负的，因此不存在这个问题。但对于均匀分布来说，只要两个分布不一致，那么就必然存在 $p(x)\neq 0$ 而 $q(x)=0$ 的区间，因此 KL 散度会无穷大。当然，写代码时我们会防止这种除零错误，但依然避免不了 KL loss 占比很大，因此模型会迅速降低 KL loss，也就是后验分布 $p(Z\vert X)$ 迅速趋于先验分布 $p(Z)$，而噪声和重构无法起到对抗作用。这又回到我们开始说的，无法区分哪个 $Z$ 对应哪个  $X$ 了。

当然，非得要用均匀分布也不是不可能，就是算好两个均匀分布的 KL 散度，然后做好除零错误处理，加大重构 loss 的权重，等等～但这样就显得太丑陋了。

### 6.2 变分在哪里

还有一个有意思（但不大重要）的问题是：VAE 叫做“变分自编码器”，它跟变分法有什么联系？在 VAE 的论文和相关解读中，好像也没看到变分法的存在呀？

其实如果已经承认了 KL 散度的话，那 VAE 好像真的跟变分没多大关系了。因为理论上对于 KL 散度 $(7)$ 我们要证明：

> 固定概率分布 $p(x)$（或 $q(x)$）的情况下，对于任意的概率分布 $q(x)$（或 $p(x)$），都有 $KL\big(p(x)\big\Vert q(x)\big)\geq 0$，而且只有当 $p(x)=q(x)$ 时才等于零。

因为 $KL\big(p(x)\big\Vert q(x)\big)$ 实际上是一个泛函，要对泛函求极值就要用到变分法，当然，这里的变分法只是普通微积分的平行推广，还没涉及到真正复杂的变分法。而 VAE 的变分下界，是直接基于 KL 散度就得到的。所以直接承认了 KL 散度的话，就没有变分的什么事了。

一句话，VAE 的名字中“变分”，是因为它的推导过程用到了 KL 散度及其性质。

## 七、条件 VAE

最后，因为目前的 VAE 是无监督训练的，因此很自然想到：如果有标签数据，那么能不能把标签信息加进去辅助生成样本呢？这个问题的意图，往往是希望能够实现控制某个变量来实现生成某一类图像。当然，这是肯定可以的，我们把这种情况叫做 Conditional VAE，或者叫 CVAE。（相应地，在 GAN 中我们也有个 CGAN。）

但是，CVAE 不是一个特定的模型，而是一类模型，总之就是把标签信息融入到 VAE 中的方式有很多，目的也不一样。这里基于前面的讨论，给出一种非常简单的 VAE。

<div align=center><img src="/assets/VAE-2022-04-03-21-53-30.png" alt="VAE-2022-04-03-21-53-30" style="zoom:70%;" /></div>

在前面的讨论中，我们希望 $X$ 经过编码后，$Z$ 的分布都具有零均值和单位方差，这个“希望”是通过加入了 KL loss 来实现的。如果现在多了类别信息 $Y$，**我们可以希望同一个类的样本都有一个专属的均值 $\mu^Y$（方差不变，还是单位方差），这个$\mu^Y$让模型自己训练**。这样的话，有多少个类就有多少个正态分布，而在生成的时候，我们就可以通过控制均值来控制生成图像的类别。事实上，这样可能也是在 VAE 的基础上加入最少的代码来实现 CVAE 的方案了，因为这个“新希望”也只需通过修改 KL loss 实现：

$$
\mathcal{L}_{\mu,\sigma^2}=\frac{1}{2} \sum_{i=1}^d\big[\big(\mu_{(i)}-\mu^Y_{(i)}\big)^2 + \sigma_{(i)}^2 - \log \sigma_{(i)}^2 - 1\big]\tag{8}
$$

下图显示这个简单的 CVAE 是有一定的效果的，不过因为 encoder 和 decoder 都比较简单（纯 MLP），所以控制生成的效果不尽完美。最近还出来了 CVAE 与 GAN 结合的工作 [CVAE-GAN](https://arxiv.org/abs/1703.10155)，模型套路千变万化啊。

<div align=center><img src="/assets/VAE-2022-04-03-21-54-58.png" alt="VAE-2022-04-03-21-54-58" style="zoom:50%;" /></div>

## 总结

总的来说，VAE 的思路还是很漂亮的。倒不是说它提供了一个多么好的生成模型（因为事实上它生成的图像并不算好，偏模糊），而是它提供了一个将概率图跟深度学习结合起来的一个非常棒的案例，这个案例有诸多值得思考回味的地方。
