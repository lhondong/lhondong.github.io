---
title: "神经网络"
subtitle: "深度学习-神经网络"
layout: post
author: "L Hondong"
header-img: "img/post-bg-19.jpg"
mathjax: true
tags:
  - 机器学习
---

# 神经网络

无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。

结构化数据即数据的基本数据库。例如在房价预测中，有一个数据库，专门的几列数据展示卧室的大小和数量；或者在预测用户是否会点击广告时，会有关于用户的信息，比如年龄以及关于广告的一些信息，然后对预测分类标注，这就是结构化数据，每个特征（比如房屋大小卧室数量，或者是一个用户的年龄）都有一个很好的定义。

相反非结构化数据是指比如音频，原始音频或者你想要识别的图像或文本中的内容。这里的特征可能是图像中的像素值或文本中的单个单词。随着数据量的加大，深度模型的效果更好。

Scale drives deep learning progress: Data, Computation, Algorithms.

## 模型表示 Model Representation

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/神经网络-2022-01-12-20-14-23.png" alt="神经网络-2022-01-12-20-14-23" style="zoom:50%;" /></div>

神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个 3 层的神经网络，第一层输入层（**Input Layer**），最后一层为输出层（**Output Layer**），中间一层为隐藏层（**Hidden Layers**）。

我们为每一层都增加一个偏差单位（**bias unit**）：$x_0$, $a_0$

$a_{i}^{(j)}$ 代表第 $j$ 层的第 $i$ 个激活单元。$\Theta^{(j)}$ 代表从第 $j$ 层映射到第 $j+1$ 层时的权重的矩阵，例如 $\Theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 $j+1$ 层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵（加一个偏置项）。例如：上图所示的神经网络中 $\Theta^{(1)}$ 的尺寸为 3*4。

对于上图所示的模型，激活单元和输出分别表达为：

$$
\begin{aligned}
a_{1}^{(2)} &=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3}\right) \\
a_{2}^{(2)} &=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)} x_{3}\right) \\
a_{3}^{(2)} &=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right) \\
h_{\Theta}(x) &=g\left(\Theta_{10}^{(2)} a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)} a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)
\end{aligned}
$$

上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。可以知道：每一个 $a$ 都是由上一层所有的 $x$ 和每一个 $x$ 所对应 $\theta$ 的决定的，把这样从左到右的算法称为前向传播算法 forward propagation。

### 向量化表示

$x=\left[ \begin{matrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{matrix} \right]$ $z^{(2)} = \left[ \begin{matrix} z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{matrix} \right]$

$z^{(2)} =\Theta^{(1)}x$
$a^{(2)} = g(z^{(2)})$

$$
a^{(2)} = \left[ \begin{matrix} a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \end{matrix} \right] = g \left(\left[ \begin{matrix} \Theta_{10}^{(1)}&\Theta_{11}^{(1)}&\Theta_{12}^{(1)}&\Theta_{13}^{(1)} \\ \Theta_{20}^{(1)}&\Theta_{21}^{(1)}&\Theta_{22}^{(1)}&\Theta_{23}^{(1)} \\ \Theta_{30}^{(1)}&\Theta_{31}^{(1)}&\Theta_{32}^{(1)}&\Theta_{33}^{(1)} \\ \end{matrix} \right] \times\left[ \begin{matrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{matrix} \right] \right) \\ = g \left(\left[ \begin{matrix} \Theta_{10}^{(1)}x_0&\Theta_{11}^{(1)}x_1&\Theta_{12}^{(1)}x_2&\Theta_{13}^{(1)}x_3 \\ \Theta_{20}^{(1)}x_0&\Theta_{21}^{(1)}x_1&\Theta_{22}^{(1)}x_2&\Theta_{23}^{(1)} x_3\\ \Theta_{30}^{(1)}x_0&\Theta_{31}^{(1)}x_1&\Theta_{32}^{(1)}x_2&\Theta_{33}^{(1)}x_3 \\ \end{matrix} \right]  \right)
$$

令 $z^{(2)}=\Theta^{(1)}x$，则 $a^{(2)}=g(z^{(2)})$ ，计算后添加 $a_{0}^{\left( 2 \right)}=1$。 计算输出的值为：

$$
\begin{aligned}
h_\Theta(x) &= a^{(3)} = g(z^{(3)}) \\
= &g \left(\left[ \begin{matrix} \Theta_{10}^{(2)}&\Theta_{11}^{(2)}&\Theta_{12}^{(2)}&\Theta_{13}^{(2)} \end{matrix} \right] \times \left[ \begin{matrix} a_0^{(2)} \\ a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \end{matrix} \right] \right) \\ = &g (\left[ \begin{matrix} \Theta_{10}^{(2)}a_0^{(2)}&\Theta_{11}^{(2)}a_1^{(2)}&\Theta_{12}^{(2)}a_2^{(2)}&\Theta_{13}^{(2)}a_3^{(2)} \end{matrix} \right])
\end{aligned}
$$

这个过程就是前向传播 forward propagation，从输入单元的激活项开始，前向传播给隐藏层，计算隐藏层的激活项，然后继续前向传播，计算输出层的激活项。

这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，需要让同一个实例的特征都在同一列里。即：

$$
\begin{gathered}
Z^{(2)}=\Theta^{(1)}\times {X}\\
A^{(2)}=g(Z^{(2)})
\end{gathered}
$$

其实神经网络就像是 logistic regression，只不过把 logistic regression 中的输入向量 $\left[ x_1\sim {x_3} \right]$ 变成了中间层的 $\left[ a_1^{(2)}\sim a_3^{(2)} \right]$，可以把 $a_0, a_1, a_2, a_3$ 看成更为高级的特征值，也就是 $x_0, x_1, x_2, x_3$ 的进化体，并且它们是由 $x$ 与 $\Theta$ 决定的，因为是梯度下降的，所以 $a$ 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$ 次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。

### 神经网络多分类  

输入向量 $x$ 有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现 $[a\text{ }b\text{ }c\text{ }d]^{T}$，且 $a,b,c,d$ 中仅有一个为 1，表示当前类。下面是该神经网络的可能结构示例：

<div align=center><img src="https://lhondong-pic.oss-cn-shenzhen.aliyuncs.com/img/assets/神经网络-2022-01-12-21-20-38.png" alt="神经网络-2022-01-12-21-20-38" style="zoom:50%;" /></div>

Training set: $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(m)},y^{(m)})$

$$
y^{(i)} \text{ one of } \left[ \begin{matrix}1\\0\\0\\0 \end{matrix} \right],\left[ \begin{matrix}0\\1\\0\\0 \end{matrix} \right],\left[ \begin{matrix}0\\0\\1\\0 \end{matrix} \right],\left[ \begin{matrix}0\\0\\0\\1 \end{matrix} \right]
$$

## 神经网络的学习

回顾逻辑回归问题中代价函数为：

$$
J(\theta)=-\frac{1}{m}\left[\sum\limits{i=1}^{m}{y}^{(i)}\log{h_\theta({x}^{(i)})}+\left(1-{y}^{(i)}\right)log\left(1-h_\theta\left({x}^{(i)}\right)\right)\right]+\frac{\lambda}{2m} \sum\limits{j=1}^{n}{\theta_j}^{2}
$$

在逻辑回归中，只有一个输出变量，又称标量（scalar），也只有一个因变量 $y$，但是在神经网络中，可以有很多输出变量，$h_\theta(x)$ 是一个维度为 $K$ 的向量，并且训练集中的因变量也是同样维度的一个向量，因此代价函数会比逻辑回归更加复杂一些：

$$
h_\theta\left(x\right)\in \mathbb{R}^{K}
$$

$$
{\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{ output}
$$

$$
J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{K} {y_k}^{(i)} \log {(h_\Theta(x^{(i)}))}_k + \left( 1 - y_k^{(i)} \right) \log \left( 1- {\left( h_\Theta \left( x^{(i)} \right) \right)}_k \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left( \Theta_{ji}^{(l)} \right)^2
$$

这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 $K$ 个预测，利用循环对每一行特征都预测 $K$ 个不同结果，然后再利用循环在 $K$ 个预测中选择可能性最高的一个，将其与 $y$ 中的实际数据进行比较。

正则化的那一项只是排除了每一层 $\theta_0$ 后，每一层的 $\theta$ 矩阵的和。最里层的循环 $j$ 循环所有的行（由 $s_{l+1}$  层的激活单元数决定），循环 $i$ 则循环所有的列，由该层（$s_l$ 层）的激活单元数所决定。即：$h_\theta(x)$ 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行 regularization 的 bias 项处理所有参数的平方和。

### 反向传播算法

**Cost function**:

$$
J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = {\frac{1}{m}}\sum\limits_{i=1}^mL(\hat{y}, y)
$$

Loss function 和之前做 logistic 回归完全一样。

训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以预测值：$\hat{y}^{(i)},(i=1,2,…,m)$

记：

$$
dW^{[1]} = \frac{\partial J}{\partial W^{[1]}},db^{[1]} = \frac{\partial J}{\partial b^{[1]}}，dW^{[2]} = \frac{\partial J}{\partial W^{[2]}},db^{[2]} = \frac{\partial J}{\partial b^{[2]}}
$$

梯度下降：

$$
\begin{gathered}
W^{[1]}\implies{W^{[1]} - adW^{[1]}},b^{[1]}\implies{b^{[1]} -adb^{[1]}} \\
W^{[2]}\implies{W^{[2]} - \alpha{\rm d}W^{[2]}},b^{[2]}\implies{b^{[2]} - \alpha{\rm d}b^{[2]}}
\end{gathered}
$$

正向传播方程如下：

$$
\begin{gathered}
Z^{[1]}=W^{[1]} x+b^{[1]} \\
A^{[1]}=\sigma\left(Z^{[1]}\right) \\
Z^{[2]}=W^{[2]} A^{[1]}+b^{[2]} \\
A^{[2]}=g^{[2]}\left(Z^{[2]}\right)=\sigma\left(Z^{[2]}\right)
\end{gathered}
$$

对于 Logistic 回归，有：

$$
\frac{dL(a,y)}{da}=\frac{y}{a}+\frac{1-y}{1-a}
$$

$$
\frac{\partial L}{\partial z}=\frac{dL}{da}\frac{da}{dz}=(\frac{y}{a}+\frac{1-y}{1-a})(a(1-a)) =a-y
$$

反向传播方程如下：

向量化实现（$Y$ 是 $1×m$ 的矩阵）：

$$
\begin{gathered}
d Z^{[2]}=A^{[2]}-Y, Y=\left[\begin{array}{llll}
y^{[1]} & y^{[2]} & \cdots & y^{[m]}
\end{array}\right] \\
d W^{[2]}=\frac{1}{m} d Z^{[2]} A^{[1] T} \\
d b^{[2]}=\frac{1}{m} n p . s u m\left(d Z^{[2]}, \text { axis }=1, \text { keepdims }=\text { True }\right) \\
d Z^{[1]}=\underbrace{W^{[2] T} d Z^{[2]}}_{(n[1], m)} * \underbrace{g^{[1]}{(1]}^{\prime}\left(Z^{[1]}\right)}_{\left(n^{[1]}, m\right)} \\
d W^{[1]}=\frac{1}{m} d Z^{[1]} X^{T} \\
\underbrace{d b^{[1]}}_{\left(n^{[1]}, 1\right)}=\frac{1}{m} n p . s u m\left(d Z^{[1]}, a x i s=1, \text { keepdims }=\text { True }\right)
\end{gathered}
$$

由于 $J=\sum\limits_{i=1}^mL()$，因此

$$
\frac{\partial J}{\partial Z}={\frac{1}{m}}dZ^{[2]}
$$

这里 `np.sum` 是 python 的 numpy 命令，`axis=1` 表示水平相加求和，`keepdims` 是防止 python 输出那些错误的秩数 $(n,)$，加上这个确保阵矩阵 $db^{[2]}$ 这个向量输出的维度为 $(n^{[1]},1)$ 这样标准的形式。 还有一种防止 python 输出奇怪的秩数，需要显式地调用 `reshape` 把 `np.sum` 输出结果写成矩阵形式。

因为 $W^{[2]T}dZ^{[2]}$ 和 ${g^{[1]}}^{'}(Z^{[1]})$ 这两个都为 $(n^{[1]},m)$ 矩阵，所以这里是进行逐个元素乘积。

## 总结

反向传播：

1. $dz^{[l]}=da^{[l]}\times{g^{[l]}}'( z^{[l]})$
2. $dw^{[l]}=dz^{[l]} \cdot a^{[l-1]}$
3. $db^{[l]}=dz^{[l]}$
4. $da^{[l-1]}=w^{[l]^T}\cdot dz^{[l]}$
5. $dz^{[l]}=w^{[l+1]^T} dz^{[l+1]} \times \text{ } g^{[l]'} (z^{[l]})$

式子 5 由式子 4 带入式子 1 得到，前四个式子就可实现反向函数。

向量化实现：

6. $dZ^{[l]}=dA^{[l]}\times g^{[l]'}(Z^{[l]})$
7. $dW^{[l]}=\frac{1}{m}\text{}dZ^{[l]}\cdot A^{[l-1]T}$
8. $db^{[l]}=\frac{1}{m}\text{ }np.sum(dz^{[l]},axis=1,keepdims=True)$
9. $dA^{[l-1]}=W^{[l]T}.dZ^{[l]}$

### 随机初始化

任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非 0 的相同的数，结果也是一样的。

**打破对称性**

通常初始参数为 $[-\epsilon,+\epsilon]$ 之间的随机值。

### 训练神经网络

1. 随机初始化参数
2. 利用正向传播方法计算所有的 $h_{\theta}(x)$
3. 编写计算代价函数 $J$ 的代码
4. 利用反向传播方法计算所有偏导数
5. 利用数值检验方法检验这些偏导数
6. 使用优化算法来最小化代价函数
