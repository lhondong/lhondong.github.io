---
title: "视频压缩模型汇总"
subtitle: "整理汇总视频压缩模型"
layout: post
author: "L Hondong"
header-img: "img/post-bg-13.jpg"
tags:
  - 视频压缩
---

# 视频压缩模型汇总

|模型|作者|摘要|备注|开源|
|---|---|---|---|----|
|[NeurIPS 2019] Deep Generative Video Compression|Dartmouth College, Disney Research|在神经视频压缩还处于起步阶段的时候，使用深度生成模型进行图像压缩已经比经典的编解码器获得了很大的性能提升。在这里，我们提出了一种端到端的深度生成式建模方法来压缩时间序列，重点是视频。我们的方法建立在序列数据的变分自动编码器（VAE）模型的基础上，并将其与最近的神经图像压缩工作相结合。该方法共同学习将原始序列变换成低维表示，并根据序列VAE的预测对该表示进行离散化和熵编码。对具有不同复杂度和多样性的公共数据集的小视频的率失真评估表明，当对通用视频内容进行训练时，我们的模型得到了极有竞争力的结果，当针对特定内容训练模型时，可以获得极高的压缩性能。|VAE，离散化+熵编码||
|[ICCV 2019] Learned Video Compression|WaveOne, Inc.|针对低延时模式，提出了一种端到端学习的视频编码新算法。在这种设置下，我们的方法在几乎整个比特率范围内都优于所有现有的视频编解码器。在不同分辨率的标准视频压缩测试集上对其进行评估，并在低延迟模式下对所有主流商业编解码器进行了基准测试。在标清视频上，HEVC/H.265、AVC/H.264和VP9编码通常比我们的算法大60%。在高清1080p视频中，H.265和VP9编码最多大20%，H.264最多大35%。此外，我们的方法不会受到块效应和像素化的影响，因此生成的视频在视觉上更高清。<br>Contributions：一是提出一种新的视频压缩体系结构，(1)推广运动估计以在简单的平移之后执行任何学习的补偿，(2)不严格依赖于先前传输的参考帧，而是保持由模型学习的任意信息的状态，(3)能够联合压缩所有传输的信号(例如光流和残差)。其次，提出了一种基于机器学习的空间码率控制框架，一种为每一帧分配跨空间可变比特率的机制。这是视频编码的一个关键组成部分，据我们所知，它还没有在机器学习环境中开发出来。|1. motion estimation <br> 2. 不依赖参考帧，模型学习 <br> 3. 联合压缩，光流残差等 <br> 空间码率可控||
|[ICCV 2019] Video Compression With Rate-Distortion Autoencoders|高通AI研究院|本文提出了一种用于有损视频压缩的深度生成模型。该模型由一个离散潜在空间的三维自编码器和一个用于熵编码的自回归先验组成。对三维自编码器和自回归先验进行联合训练以最小化率失真损失，这与变分自动编码器中的ELBO算法相关。我们的方法优于基于运动补偿或插值的最先进的学习视频压缩网络。我们系统地评估各种设计选择，例如使用基于帧或时空的自动编码器，以及选择不同的自回归先验的类型。此外还给出了基本方法的三个扩展，以证明其优于经典的压缩方法。首先，我们引入语义压缩，对模型进行训练以将更多比特分配给感兴趣的对象。其次，我们研究了自适应压缩，将模型适配到变化性有限的场景，例如来自自动驾驶汽车的视频，以在该场景上实现更优的压缩。最后，我们提出了多模态压缩，其中展示了模型在非标准成像传感器(如四摄像机)捕获的多模态联合压缩中的有效性。|3D AutoEncoder+ autoregressive prior||
|[ICCV 2019] Neural Inter-Frame Compression for Video Coding|ETH Zurich|提出了一种帧间压缩的神经视频编码方法，可以无缝地适配到不同的现有神经图像编解码器。端到端解决方案通过像素空间中基于光流的运动补偿进行时间预测。 <br> 关键在于，通过将所需信息编码成可以直接解码为运动和混合系数的潜在表示，从而提高解码效率和重建质量。为解决剩余的预测误差，需要原始图像和插值帧之间的残差信息，我们直接在潜在空间而不是在像素空间计算残差，因为这样允许对关键帧和中间帧重用相同的图像压缩网络。|optical flow based motion compensation in pixel space||
|[CVPR 2019] DVC: An End-to-end Deep Video Compression Framework|上交+悉尼大学|利用传统视频压缩方法的经典结构和神经网络强大的非线性表示能力，提出了第一种端到端视频压缩深度模型，该模型联合优化了视频压缩的各个组成部分。具体地说，利用基于学习的光流估计来获取运动信息并重构当前帧。然后采用两种自动编码方式的神经网络对相应的运动信息和残差信息进行压缩。所有模块通过一个损失函数共同学习，其中它们通过考虑减少压缩比特数和提高解码视频质量之间的权衡来相互协作。实验结果表明，该方法在峰值信噪比方面优于目前广泛使用的视频编码标准H.264，在MS-SSIM方面与最新标准H.265相当。|Deep Video Compression，光流获取运动信息，重构当前帧。|开源[代码](https://github.com/GuoLusjtu/DVC)|
|[CVPR 2020] M-LVC: Multiple Frames Prediction for Learned Video Compression|中科院空间信息处理与应用系统技术重点实验室|提出了一种适用于低延迟场景的端到端学习视频压缩方案。以前的方法仅限于使用前一帧作为参考，而我们的方法引入了以前的多帧作为参考的用法。在我们的方案中，在当前帧和前一帧之间计算运动矢量（MV）场。利用多个参考帧和相关的多个MV场，我们设计的网络可以对当前帧产生更准确的预测，产生更少的残差。多参考帧也有助于产生MV预测，从而降低MV场的编码成本。我们使用两个深度自动编码器分别压缩残差和MV。为了补偿自动编码器的压缩误差，我们进一步利用多参考帧设计了MV细化网络和残差细化网络。我们的方案中的所有模块都通过一个单一的率失真损失函数进行联合优化。我们采用一步一步的训练策略来优化整个方案。实验结果表明，该方法在低延迟模式下优于现有的学习视频压缩方法。模型在峰值信噪比（PSNR）和MS-SSIM方面都优于H.265。|Multiple Frames Prediction for Learned Video Compression + 多参考帧 + Motion Vector运动矢量场||
|[CVPR 2019] Learning Image and Video Compression through Spatial-Temporal Energy Compaction|早稻田大学|本文提出了一种基于卷积编码的图像压缩体系结构，并通过在编解码端增加一个内插环路，将图像压缩推广到视频压缩。基本思想是在学习图像和视频压缩中实现时空能量压缩。因此我们在损失函数中添加基于空间能量压缩的损失，以实现更高的图像压缩性能。此外，基于时间能量分布，我们在一个插值循环中选择帧数，以适应视频内容的运动特性。实验结果表明，我们提出的图像压缩方法优于最新的MS-SSIM质量度量的图像压缩标准，并且在高比特率下提供了比最新的学习压缩方法更高的性能，这得益于我们的空间能量压缩方法。同时，我们提出的基于时间能量压缩的视频压缩方法可以显著地优于MPEG-4，与常用的H.264相当。我们的图像和视频压缩都可以产生比传统标准更令人愉悦的视觉效果。在低延迟模式下，该方法的性能优于现有的学习视频压缩方法。在PSNR和MS-SSIM上都比H.265有更好的效果。|Spatial-Temporal Energy Compaction + 时空能量压缩||
|[CVPR 2019] Video Generation from Single Semantic Label Map|华为诺亚方舟实验室|本文提出了一种新的基于单个语义标签映射的视频生成方法，在生成过程中兼顾了灵活性和质量。与典型的端到端方法单步建模场景内容和动态不同，我们提出将这一困难的任务分解为两个子问题。由于目前的图像生成方法在细节方面优于视频生成，因此我们仅通过生成第一帧来合成大小相同的高质量内容，然后根据场景的语义对场其行动画处理，以获得时间上连贯的视频，总体效果良好。我们使用cVAE来预测光流，作为一个中间步骤来生成一个以初始单帧为条件的视频序列。在光流预测模块中集成了语义标签映射，对图像到视频的生成过程进行了较大的改进。在Cityscapes数据集上的大量实验表明，我们的方法优于所有同类方法。|视频生成，single semantic label map，cVAE预测光流。| 开源[代码](https://github.com/junting/seg2vid)|
|[CVPR 2020] Learning for Video Compression with Hierarchical Quality and Recurrent Enhancement|ETH Zurich|本文提出了一种具有三个等级的质量层和递归增强网络的分层学习视频压缩方法——HLVC。通过最高质量的图像压缩方法来压缩第一层中的帧，以这些帧为参考，我们提出了双向深度压缩（BDDC）网络来对第二层进行高质量的压缩。然后，采用单一运动深度压缩(SMDC)网络对质量最低的第三层帧进行压缩，该网络采用单个运动映射来估计多帧的运动，从而减少了运动信息的比特数。 <br> 在深度解码器中，我们开发了加权递归质量增强(WRQE)网络，它以压缩帧和码流作为输入。在WRQE的递归单元中，利用质量特征对存储和更新信号进行加权，合理利用多帧信息进行增强。HLVC方法分层质量有利于编码效率，因为高质量的信息有助于分别在编码器和解码器端压缩和增强低质量的帧。实验结果表明，该方法在PSNR和MS-SSIM方面均优于x265的“低延时P(LDP)非常快”压缩方式。|Hierarchical Learned Video Compression|开源[代码](https://github.com/RenYang-home/HLVC)|
|[CVPR 2020] Scale-space flow for end-to-end optimized video compression|Google Research, Perception Team|尽管端到端优化的图像压缩深度网络方面取得了相当大的进展，视频编码仍然是一项具有挑战性的任务。最近提出的学习视频压缩方法使用光流和双线性扭曲进行运动补偿，并显示出与H.264和HEVC等手工设计的编解码器相比具有竞争力的率失真性能。然而，这些基于学习的方法依赖于复杂的体系结构和训练方法，包括使用预先训练的光流网络、子网络的顺序训练、自适应码率控制以及在训练期间将中间重建文件缓冲到磁盘。在本文中，我们证明了广义扭曲算子能够更好地处理常见的失效情况，例如disocclusions和fast motion，通过大大简化的模型和训练过程，可以提供很好的压缩结果。具体来说，我们提出了尺度空间流，一个直观的概括光流，增加了一个尺度参数，使网络更好地模拟不确定性。实验表明，使用尺度空间流进行运动补偿的低延迟视频压缩模型（无B帧）在使用更简单的程序进行训练并且没有任何预先训练的光流网络的情况下，可以优于类似的最新学习视频压缩模型。|Scale-space flow||
|[ECCV 2020] Improving Deep Video Compression by Resolution-adaptive Flow Coding|北航，北理，悉尼大学|在基于学习的视频压缩方法中，通过开发新的运动矢量（MV）编码器来压缩像素级的光流图是一个重要的问题。在这项工作中，我们提出了一个新的框架，称为分辨率自适应流编码（RaFC）来有效地压缩全局和局部的光流图，其中MV编码器的输入光流图和输出运动特征都使用多分辨率表示而不是单分辨率表示。为了全局处理复杂或简单的运动模式，我们的帧级方案RaFC-frame自动确定每个视频帧的最佳流图分辨率。为了在局部处理不同类型的运动模式，我们的块级方案RaFC block还可以为每个局部运动特征块选择最佳分辨率。此外，对RaFC帧和RaFC块都应用了率失真准则，并选择了最佳的运动编码模式进行有效的流编码。在HEVC、VTL、UVG和MCL-JCV四个基准数据集上进行的综合实验清楚地证明了我们将RaFC帧和RaFC块结合起来进行视频压缩的整体RaFC框架的有效性。|分辨率自适应流编码<br>RaFC-frame，RaFC-block||
|[ICCV 2019] Non-Local ConvLSTM for Video Compression Artifact Reduction|复旦，bilibili|视频压缩伪影消除的目的是从低质量的压缩视频中恢复出高质量的视频，大多数现有方法使用单个相邻帧或一对相邻帧（在目标帧之前和/或之后）。此外，由于总体高质量的帧可能包含低质量的补丁，且高质量的补丁可能存在于低质量整体的帧中，因此当前方法关注附近峰值质量帧（pqf）可能会遗漏低质量帧中的高质量细节。为了弥补这些不足，本文提出了一种新的端到端深度神经网络，称为非局部ConvLSTM（简称NL-ConvLSTM），它利用了多个连续帧。在NL-ConvLSTM中引入了一种近似的非局部策略来捕获全局运动模式并跟踪视频序列中的时空相关性。这种近似策略使得非局部模块以一种快速、低空间开销的方式工作。该方法利用目标帧的前后两帧产生残差，从残差中重构出更高质量的帧。在两个数据集上的实验表明，NL-ConvLSTM的性能优于现有的方法。|视频压缩伪影消除，NL-ConvLSTM，非局部卷积LSTM，目标帧的前后两帧产生残差，重构高质量帧||
|[ICASSP 2020] Adversarial Video Compression Guided by Soft Edge Detection|The University of Texas at Austin|提出了一种基于条件生成对抗网络的视频压缩框架。两个编码器：一个部署标准视频编解码器，另一个通过下采样管道、新设计的软边缘检测器和新的无损压缩方案生成低层映射。对于解码，我们使用标准的视频解码器以及基于神经网络的解码器，基于神经网络的解码器使用条件GAN训练。新的“深度”视频压缩方法要求多个视频预先训练生成网络来进行交互。与之前的工作不同，我们的方案在一对非常有限的关键帧上训练生成解码器，这些关键帧取自单个视频和相应的低级映射。经过训练的解码器依赖于低级映射的指导产生重建帧，而不需要任何内插。在131个不同的视频集上的实验表明，我们提出的基于GAN的压缩引擎在非常低的比特率下重建质量比目前流行的标准编解码器(如H.264或HEVC)高很多。|cGAN，两个编码器，两个解码器，||
|[NeurIPS 2020] Hierarchical Patch VAE-GAN Generating Diverse Videos from a Single Sample|Tel Aviv University|从一个单一的视频样本生成全新的不同的视频。最近提出了一种新的基于hierarchical patch-GAN的方法来生成不同的图像，在训练时只给出一个样本。在视频领域中，这些方法无法生成不同的样本，并且经常会崩溃为生成类似于训练视频的样本。我们介绍了一种新的基于patch的变分自动编码器（VAE），它允许更大的多样性生成。利用该工具，构造了一种新的分层视频生成方案：在粗尺度下，采用我们的patch-VAE，保证了样本的高度多样性。随后，在更精细的尺度上，patch-GAN渲染精细的细节，产生高质量的视频。实验表明，该方法在图像域和更具挑战性的视频域都能产生不同的样本。|图像：hierarchical patch-GAN<br>视频：patch-VAE生成，patch-GAN渲染细节|开源[代码](https://github.com/shirgur/hp-vae-gan)|