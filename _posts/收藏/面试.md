# 面试

## 商汤

起源是之前在知乎看到王岩博士的[招聘](https://zhuanlan.zhihu.com/p/269974223)，（现在好像不在商汤了，去了清华智能产业研究院）当时正在实习，方向就是招聘中提到的方向，就留意了一下记了下来，实习结了之后回实验室干完活开始找实习，第一反应就想到了这个招聘，投了简历马上就接了HR电话，开始三轮面试。

### 一面

由于方向及其match，基本全程在聊项目。

之前做过的图像压缩项目，基本是基于HiFiC的调参，就着HiFiC以及GAN聊了很久，包括质量评价指标（基于失真的 vs. 基于感知的，有参考的 vs. 无参考的）。L1正则化为什么更能提升感知质量。

然后问了GAN的相关东西，包括论文用的cGAN，除此之外GAN得各种变体。问了最近有没有关注图像压缩的最新进展，实话实说项目结掉之后就没太关注了。

然后是第二个项目，一点关于图像/视频超分的知识，主要也是集中在项目的详细介绍，然后是关于超分的最新进展，包括最新的SoTA BasicSR和盲超分。后面有开放式地问了一下我关于超分方向或者整个图像视频方法的未来发展看法，主要就精度和实时性之间的trade-off谈了一下（ps. 忘了说可解释性）。

最后是视频压缩的项目，之前在实验室开展新课题的时候恶补了一下Transformer，结果问了Swin Transformer忘的一干二净，就记得个滑动窗口，就边嗯啊边想边说，后来面试官给补了一课。接下来关于视频压缩的东西就是常规提问，没有聊到H.264/H.265很奇怪。

目前关于视频压缩考虑套MAE的模型，就展开了MAE的讨论，因为当时课题还在探索阶段，面试官问为什么要用ViT编码光流，我大概说了一下从图像效果不好到考虑视频结合残差可能会好一点，结果面试官的意思是为什么关注光流而不是空域，我确实是看之前的框架都是编码光流就没往这方面想，后来一琢磨好想完全是可行的，然后追着面试官讨论了一下。中间穿插着问我视频压缩的baseline，包括DVC、M-LVC、FVC、DCVC，说到最后两个面试官眼睛一亮，问我复现了没有，我实话实说DVCV没给training代码，没复现出来，然后面试官说组里现在也在复现这篇工作，但是差几个点对不上原文，说最近比较忙有几周没跟进进展了 ：）

至此其实问的就差不多了，发现我的项目实习经历和组里特别match，就说写一下代码吧。然后话锋一转我看你博客最近更新了不少剑指offer刷题笔记，那常规的我们这里就不用了，来写一下MAE吧（我：？？？）。然后主要关注一小点MAE得掩码过程，写个逻辑不用运行（我内心：您叫我十几分钟写个能运行的MAE也太看的起我了吧）

其实简单来说主要关注的MAE的掩码过程，我记得原文中的掩码是将图片划分为patch之后打乱顺序，然后直接去掉后75%。先用torch.randperm对为每个patch生成随机位置，打乱顺序后取patch序列第二个维度的后75%，即patches[:, 0:0.75*W]，最后输入encoder中。ViT模型可以直接调用timm库，就没有详细写了。

写代码的过程非常痛苦，写的慢慢吞吞笨笨的，切片第二个维度也写错了，我总是从Github上扒下来源码改一改开始调参，真正从头到尾搭一个框架实在是少之又少。事实是不光LeetCode要好好刷，PyTorch也要熟能生巧。

最后面试官问我有啥要问的，我其实主要关心实习更关注写论文还是搞落地，面试官特别正经的说，首先是以落地为基础的，但是也不是简单的拿来用，肯定是在这个过程中要有自己的创新，真是听君一席话，如听一席话。

一面除了代码写的比较便秘之外，感觉非常顺畅，跟面试官聊的很开心，甚至还从中学到了好些东西，对现在手头的项目也开阔了新的思路。

### 二面

当晚HR通知一面通过，约第二天下午二面。正巧第二天下午组会，于是推到后天。

二面时候也不是很紧张，面试官是13级清本学长，我一口一个老师叫着本来挺开心，后来他说是清本之后马上改口叫学长。

开始从图像压缩入手，问做复现源码之后量化的过程中会掉精度，讲了一下TensorRT中FP16与INT8的种种，然后聊到Rate-Distortion-Perception Tradeoff，好巧不巧当时写开题报告的时候正巧读过这篇论文，Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff，展开讨论了一下，我大概说了一下印象中的理解之后，面试官抓着SSIM作为感知质量是一个误解对我一顿教育。

不得不说清本的大哥就是不一样，从头到尾全是基础性的十分偏向数学的硬核提问，搞得我面试完缓了半小时。

首先是 Ordinary Least Square，题目是有 $X \in R^{n, d}, y \in R^{n, 1}, w^* = \arg \min \Vert X \cdot w - y\Vert ^2, loss = \Vert X \cdot w - y\Vert^2$，面试官上来就在题目写 LaTex，我直接看懵了，后来确认了半天知道是叫我写loss对 $w$ 求导。这个没啥好说的，直接 $w=(X^TX)^{-1}X^Ty$。

第二个问题是岭回归 Ridge regression，还是直接手敲 LaTeX 且没有编译器，如果 $(X^TX) d > n$ 不可逆，那么需要在原来的 loss 函数上增加一项 $w^{*} = \arg \min \Vert X w - y\Vert^2 + \lambda \Vert w\Vert^2$，其中 $\lambda > 0$，此时如何求解 $w^*$ ？增加 $\lambda\Vert w\Vert^2$ 后证明 $(X^TX)$ 可逆？（提示：通过半正定矩阵特征值 $\geq0$，加上大于 0 的 $\lambda$ 之后特征值一定大于 0，因此可逆，**回去好好复习一下 $Q^TAQ$**） 

接下来是马尔可夫复杂度问题。先来讨论自回归问题 Autoregressive：对于 $X = (x_1, .... x_t), x_i \in (0, 1, ... k -1),  P(X) = \sum p(x_t \vert x<t)$，求计算 $P(X)$ 的复杂度？（提示：对于 $(a, b, c)\in  (0, 1), p(a, b, c)$ 的复杂度为 $2^3$，对于 $(a, b)\in (0, 1, 2), p(a|b)$ 的复杂度为 $3^2$。那么对于 $P(X)$ joint，其 table size 为 $ K^T + K^{t-1}+ ... +K$，复杂度为 $O(K^T)$）。

而对于一个马尔可夫过程，Markovian：$P(X) = \sum p(x_t \vert x_{t-1}), P(X)$ joint, table size 为 $K^2 + K^2 .... K^2$，复杂度是 $O(TK^2)$。n-gram markvian 复杂度 $O(T(K^{(n+1)}))$。

最后是求马尔可夫矩阵的稳态，对于马尔可夫矩阵 $T$，有 $\pi[t] = T \cdot \pi [t - 1]$，问马尔可夫矩阵什么情况下有稳态，何时达到稳态 $T \cdot\pi[0] = \pi[0]$。（提示：由于达到稳态的方程正好是特征向量的定义，其特征值为 1，即当矩阵存在特征值为 1 的特征向量是可以达到稳态，稳态即为特征值 1 对应的特征向量）

最后的代码题也是非常的数学，在不使用内置函数sqrt() 的前提下求一个数 $x$ 的根号，给定精度不小于 $\epsilon$。我一开始想到了二分法，写完了之后面试官提示要注意二分的上下界，如果 $x>1$，那么 $x$ 开根号应该变小，则二分的上下界应该定为 $[1,x]$，如果  $x<1$，那么 $x$ 开根号反而会变大，则二分的上下界应该定为 $[x,1]$。

然后面试官提示了牛顿迭代法求根号，其本质是求 $f(x) = x^2 - a$ 的零点，不断用 $(x, f(x))$ 的切线来逼近方程  $x^2-a=0$ 的根。

```python
double sqr(a: double, eps: double):
	x = a, y = 0.0
	while fabs(x - y) > eps:
		y = x
		x = 0.5*(x + a / x)
	return x
```

二面其实整体答得并不好，很多数学问题太基础，在面试官的提示下连蒙带猜的基本答出来了。只能说二面通过实在侥幸。

### 三面

三面前有个小插曲，面试官给我发的面试连接上面写着小鱼易连叫我下载应用，下面又是牛客的面试邀请。然后准备好了小鱼易连找了一圈没有发现会议号，点牛客的链接进去居然是百度地图？马上开始面试了我给HR打电话还被拒接了，然后短信回复正在忙等下回复，我还有三分钟开始面试了啊喂！然后我加了他微信说了情况，他赶忙给我重新发了面试链接。有一说一，这HR真是有够马虎的了。

因为深圳疫情严重，应该是在居家办公，面试官一进会议室感觉就是在大厨房的餐桌里。当时是下午两点钟，感觉面试官像是刚刚睡醒，还有点迷离，感觉给我面试十分不情愿。

先是过了一遍项目，从头到尾都是他问一句，我回答一些，然后没有就这个问题深入讨论，甚至没有给我一个反馈我究竟答得对不对，然后直接下一个问题，面试官的话实在少的可怜。

整理面试问题：

1. 图像压缩针对HiFiC做了哪些改进？量化是否会掉精度？怎么解决？
2. Transformer了解哪些？ViT实际上并不是很好的架构，除此之外还了解哪些Transformer结构？
3. 关于视频编解码H264/H265了解哪些？说一下流程？
4. 后续处理有什么？DCT说一下？熵编码说一下？（这里犯了一个大错，H264里面用的熵编码实际上都是算术编码，而我之前对这里了解不深乱说了哈夫曼，实在是愚蠢）
5. 视频压缩最新算法？Basiline选的什么？SoTA知道什么？
6. Transformer做图像视频编码最新进展有没有了解？（这里我知道有Transformer做图像压缩的论文，但是我没有细看，直接回答我没看感觉态度也不是很合适）

最后做了两道编程题，吐槽一下面试官实在够敷衍，就给一个最简单的描述，连条件也不给。

第一道是均值滤波器，给定 $H\times W$ 矩阵，求 $K\times K$ 卷积核的均值滤波器。首先padding 2/K 补零，然后两个循环 i，j，用numpy 函数 np.mean(input[i:i+K/2, j:j+K/2]) 实现均值滤波。写完了之后面试官说有问题，然后我想了半天没想明白问题在哪，问面试官他也想了好一会，跟我说应该能跑通，这时才突然想起来循环的上下界设定有问题，会越界，改了一下。

第二道题是求两个矩阵的交并比，先求相交部分面积，然后用两个矩阵面积相加减去相交的部分即为并的面积。写完了面试官说要判断是否相交，然后又改了一下。

总体来说商汤面试还算不错，前两面比较不紧张，而且面完了感觉学了好多东西，相比之下第三面实在有点为了面试而面试的意思，有点像是面试官为了面试 KPI 任务，不情不愿的给我面了一下，没有深入沟通也没有任何反馈。

从我的角度来说，第一次面试还是反映出了很多问题，很多基础的东西比如H264以及数学等还需要深入学习了解，不能应为熵编码这种自认为无关紧要的东西就不去深入掌握了。继续加油吧！

