---
title: "FP32 FP16 和 INT8"
subtitle: ""
layout: post
author: "L Hondong"
header-img: "img/post-bg-5.jpg"
tags:
---

# FP32 FP16 和 INT8

FP32（Full Precise Float 32，单精度）占用 4 个字节，共 32 位，其中 1 位为符号位，8 位指数位，23 位尾数位。

FP16（float，半精度）占用 2 个字节，共 16 位，其中 1 位为符号位，5 位指数位，十位有效数字位。与 FP32 相比，FP16 的访存消耗仅为 1/2，也因此 FP16 是更适合在移动终端侧进行 AI 计算的数据格式。

INT8，八位整型占用 1 个字节，INT8 是一种定点计算方式，代表整数运算，一般是由浮点运算量化而来。在二进制中一个 “0” 或者 “1” 为 1bit，INT8 则意味着用 8bit 来表示一个数字。因此，虽然 INT8 比 FP16 精度低，但是数据量小、能耗低，计算速度相对更快，更符合端侧运算的特点。

|  |Dynamic Range|Minimum Positive Value|
|----|----|----|
|FP32|$-3.4\times10^{38}$ ~ $+3.4\times10^{38}$|$1.4\times10^{-45}$|
|FP16|$-65504$ ~ $+65504$|$5.96\times10^{-8}$|
|INT8|$-128$ ～ $+127$|$1$|

## 比较

低精度技术 (high speed reduced precision)。在 training 阶段，梯度的更新往往是很微小的，需要相对较高的精度，一般要用到 FP32 以上。在 inference 的时候，精度要求没有那么高，一般 FP16（半精度）就可以，甚至可以用 INT8（8 位整型），精度影响不会很大。同时低精度的模型占用空间更小了，有利于部署在嵌入式模型里面。

利用 FP16 代替 FP32
优点：
1. TensorRT 的 FP16 与 FP32 相比能有接近一倍的速度提升，前提是 GPU 支持 FP16（如最新的 2070,2080,2080ti 等）
2. 减少显存。

缺点：
1. 会造成溢出